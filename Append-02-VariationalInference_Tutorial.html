
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>附录 B： 变分法推断 &#8212; Python贝叶斯分析(中文)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="附录 C： 高斯过程" href="Append-03-GaussianProcessTutorial_01.html" />
    <link rel="prev" title="附录 A： MCMC 推断" href="Append-01-MCMC_Tutorial.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Python贝叶斯分析(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   封面
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  书籍正文
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter01-ThinkingProbabilistically.html">
   第 1 章 概率思维
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter02-ProgrammingProbabilistically.html">
   第 2 章 概率编程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter03-ModellingwithLinearRegression.html">
   第 3 章 线性回归模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter04-GeneralizedLinearRegression.html">
   第 4 章 广义线性回归模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter05-ModelComparison.html">
   第 5 章 模型比较与模型平均
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter06-MixtureModels.html">
   第 6 章 混合模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter07-GaussianProcesses.html">
   第 7 章 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter08-InterefenceEngine.html">
   第 8 章 推断引擎
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter09-WheretoGoNext.html">
   第 9 章 下一步去哪儿？
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  文献阅读
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Append-01-MCMC_Tutorial.html">
   附录 A： MCMC 推断
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   附录 B： 变分法推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-03-GaussianProcessTutorial_01.html">
   附录 C： 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-04-BayesianNN_Tutorial.html">
   附录 D：贝叶斯神经网络
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-05-BayesianDeepLearning_Tutorial.html">
   附录 E：贝叶斯深度学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-06-BayesianDeepLearningPymc3.html">
   附录 F：贝叶斯深度学习编程初步
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-07-ModelAveraging.html">
   附录 G：模型平均
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-08-ModelEnsembling.html">
   附录 H：模型集成
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-09-BayesianOptimization.html">
   附录 J：贝叶斯优化
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-10-WorkFlow.html">
   附录 K：贝叶斯工作流程
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/Append-02-VariationalInference_Tutorial.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Append-02-VariationalInference_Tutorial.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/BayesianAnalysiswithPython2nd"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/BayesianAnalysiswithPython2nd/issues/new?title=Issue%20on%20page%20%2FAppend-02-VariationalInference_Tutorial.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/xishansnow/BayesianAnalysiswithPython2nd/master?urlpath=lab/tree/Append-02-VariationalInference_Tutorial.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        <a class="jupyterhub-button" href="https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/xishansnow/BayesianAnalysiswithPython2nd&urlpath=lab/tree/BayesianAnalysiswithPython2nd/Append-02-VariationalInference_Tutorial.md&branch=master"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   1 问题提出
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   2 变分推断
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     2.1 核心思想
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     2.2 进一步加深理解
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       2.2.1 变分推断基于贝叶斯模型
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id6">
       2.2.2 问题本质是基于观测数据推断隐变量的分布
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id7">
       2.2.3 变分推断的关键是构造变分分布
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id8">
       2.2.4 变分推断是一个优化问题
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id9">
       2.2.5 变分推断的形象化解释
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     2.3 概率图表示
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id11">
       2.3.1 贝叶斯概率框架
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id12">
       2.3.2 利用概率图表示和理解
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id13">
       2.3.3 更一般的形式
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     2.4 目标函数的选择
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id15">
   3 经典方法 — 平均场、指数族与坐标上升算法
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     3.1 平均场近似的基本原理
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id17">
     3.2 指数族分布的适用性
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     3.3 坐标上升优化算法
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id19">
   4 提升可扩展性 — 随机变分推断
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id20">
     4.1 如何适应大数据 ？
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id21">
     4.2 参数梯度与自然梯度
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svi-sgd">
     4.3 随机变分推断（ SVI ）：自然梯度与 SGD 的结合
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#svi">
     4.4 随机变分推断（ SVI ）中的一些技巧
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id22">
       4.4.1 自适应学习率
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id23">
       4.4.2 方差减少策略
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id24">
   5 提升通用性 — 黑盒变分推断
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bbvi">
     5.1 为何要做黑盒变分推断（BBVI）？
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id25">
     5.2 使用评分梯度的 BBVI
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id26">
     5.3 使用重参数化梯度的 BBVI
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id27">
     5.4 方差减少策略
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id28">
   6 提升准确性 —  新的目标函数和结构化变分近似
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id29">
     6.1 平均长变分的起源和局限性
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id30">
     6.2 采用新的散度做变分推断
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id31">
     6.3 结构化变分推断
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id32">
     6.4 其他非标准的变分推断方法
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id33">
   7 摊销式变分推断与深度学习
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#amortized-variational-families">
     7.1 摊销变分推断（Amortized Variational Families）
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#vae">
     7.2 变分自编码器 （VAE）
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id34">
     7.3 更灵活的 VAE
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id35">
       7.3.1 标准化流 VAE
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id36">
       7.3.2 重要性加权 VAE
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id37">
     7.4  结构化 VAE
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id38">
       7.4.1 结构化 VAE
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id39">
     7.5 解决僵尸单元的问题
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id40">
       7.5.1 有损 VAE
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id41">
   8 讨论
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id42">
     8.1 变分推断理论方面
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id43">
     8.2 变分推断和深度学习
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id44">
     8.3 变分推断和策略梯度
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id45">
     8.4 自动变分推断
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id46">
   9 总结
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="b">
<h1>附录 B： 变分法推断<a class="headerlink" href="#b" title="Permalink to this headline">¶</a></h1>
<p>【原文】Zhang Cheng， Butepage Judith，Kjellstrom Hedvig，Mandt Stephan； Advances in Variational Inference，2018</p>
<style>p{text-indent:2em;2}</style>
<div class="section" id="id1">
<h2>1 问题提出<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>概率模型中的推断通常难以处理，根据之前的介绍，通过对随机变量进行采样的方法，能够为推断问题（例如，边缘似然推断）提供近似解。大多数基于采样的推断算法是属于马尔可夫链蒙特卡罗 (MCMC) 方法。</p>
<p>不幸的是，上述基于采样的方法有几个重要缺点。</p>
<ul class="simple">
<li><p>尽管随机方法可以保证找到全局最优解，但前提是时间充足，而这在实践中通常是受限的</p></li>
<li><p>目前尚没有好的方法能够判断采样结果与真实解到底有多接近，收敛状况仍然需要人工判断</p></li>
<li><p>为了找到一个效率足够高的解决方案，MCMC 方法需要选择合适的采样技术（例如，Metropolis-Hastings 、 HMC 、 NUTS 等），而选择本身就是一门艺术。</p></li>
</ul>
<p>为此，人们一直在寻找在保证适当精度条件下，比 MCMC 效率更高的后验推断方法，这就是变分推断方法。</p>
</div>
<div class="section" id="id2">
<h2>2 变分推断<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3>2.1 核心思想<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>根据贝叶斯概率框架，我们希望根据已有数据，来推断参数或隐变量的分布 <span class="math notranslate nohighlight">\(p\)</span> ，进而能够做后续的预测任务；但是，当 <span class="math notranslate nohighlight">\(p\)</span> 不容易表达，无法得到封闭形式解时，可以考虑寻找一个容易表达和求解的分布 <span class="math notranslate nohighlight">\(q\)</span> 来近似 <span class="math notranslate nohighlight">\(p\)</span> ，当 <span class="math notranslate nohighlight">\(q\)</span> 和 <span class="math notranslate nohighlight">\(p\)</span> 之间差距足够小时， <span class="math notranslate nohighlight">\(q\)</span> 就可以代替 <span class="math notranslate nohighlight">\(p\)</span> 作为输出结果，并执行后续任务。 当 <span class="math notranslate nohighlight">\(q\)</span> 由若干变分参数 <span class="math notranslate nohighlight">\(\nu\)</span> 定义时，该问题就变成了一个寻找 <span class="math notranslate nohighlight">\(\nu\)</span> 的最优化问题，优化目标是最小化 <span class="math notranslate nohighlight">\(q\)</span> 和 <span class="math notranslate nohighlight">\(p\)</span> 两个概率分布之间的差距。其中， <span class="math notranslate nohighlight">\(q\)</span> 被成为变分分布， <span class="math notranslate nohighlight">\(\nu\)</span> 被成为变分参数。</p>
<blockquote>
<div><p>变分推断的精髓是将 “求分布” 的推断问题，变成了 “缩小差距” 的优化问题。</p>
</div></blockquote>
<p>以下图为例进行说明，黄色分布为目标分布 <span class="math notranslate nohighlight">\(p\)</span> ，很难求解，但它看上去有些像高斯，我们可以尝试从高斯分布族 <span class="math notranslate nohighlight">\(\mathcal{Q}\)</span> 中（此处以一个红色高斯和一个绿色高斯为例，其变分参数为均值 <span class="math notranslate nohighlight">\(\mu\)</span> 和 方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span> ），寻找和 <span class="math notranslate nohighlight">\(p\)</span> 最像的（可以用“重叠面积比率最大” 或 “差异最小” 来度量 ） 那个 <span class="math notranslate nohighlight">\(q\)</span> ， 作为 <span class="math notranslate nohighlight">\(p\)</span> 的近似分布。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108085400-e452.webp" /></p>
<blockquote>
<div><p>图 1：单变量的真实分布与近似分布。 此处简化了假设，更为复杂的分布应当具有更多变量，且涉及多维空间中的多峰分布。</p>
</div></blockquote>
<p>变分方法与采样方法的主要区别是：</p>
<ul class="simple">
<li><p>变分方法几乎永远找不到全局最优解</p></li>
<li><p>可以知道自身收敛情况，甚至可以对其准确性有限制</p></li>
<li><p>更适合于随机梯度优化、多处理器并行处理以及使用 GPU 加速</p></li>
</ul>
<p>虽然采样方法的历史非常悠久，但随着大数据的发展，变分方法一直在稳步普及，并成为目前使用更为广泛的推断技术。</p>
</div>
<div class="section" id="id4">
<h3>2.2 进一步加深理解<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>变分推断的核心要点包括以下四个：</p>
<ul class="simple">
<li><p><strong>模型</strong>：变分推断需要设计一个模型  <span class="math notranslate nohighlight">\(p(\mathbf{z, x})\)</span> ，其中 <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> 为观测数据， <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 为模型参数或/和隐变量（后面统一用隐变量来表示，但需要理解模型参数和隐变量有时具有不同的含义）。模型来自专家个人的业务知识和归纳偏好，通常可以用概率图模型表达，而数据则来自实际观测。</p></li>
<li><p><strong>目的</strong>：基于观测数据 <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> 确定模型中隐变量 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 的值，但各种不确定性因素导致隐变量无法得到一个确切值，通常只能给出其概率分布 <span class="math notranslate nohighlight">\(p(\mathbf{z \mid x})\)</span> ，而最大的问题在于这个分布可能很复杂，无法直接给出封闭形式的解。</p></li>
<li><p><strong>原理</strong>：变分推断将人为构造一个由参数 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span> 索引（或定义）的概率分布 <span class="math notranslate nohighlight">\(q(\mathbf{z};\boldsymbol{\nu})\)</span> ，并通过一些方法或技巧来优化 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span>，使 <span class="math notranslate nohighlight">\(q(\mathbf{z};\boldsymbol{\nu})\)</span> 能够近似并且代替复杂的真实分布  <span class="math notranslate nohighlight">\(p(\mathbf{z \mid x})\)</span> 。</p></li>
<li><p><strong>途径</strong>：通过一些优化算法，调整 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span> 以缩小 <span class="math notranslate nohighlight">\(q(\mathbf{z};\boldsymbol{\nu})\)</span> 和   <span class="math notranslate nohighlight">\(p(\mathbf{z \mid x})\)</span> 之间的差异直至收敛。</p></li>
</ul>
<div class="section" id="id5">
<h4>2.2.1 变分推断基于贝叶斯模型<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<p>简单说，专家利用其领域知识和归纳偏好，给出一个模型假设 <span class="math notranslate nohighlight">\(p(\mathbf{z, x})\)</span> ，其中包括隐变量  <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>  和观测变量 <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> ，还有相互之间可能存在依赖关系。变分推断是基于该模型实施的。</p>
<blockquote>
<div><p>注： 此处隐变量和观测变量采用黑体符号，表示其为向量形式，即代表了不止一个随机变量。</p>
</div></blockquote>
<p>为了理解隐变量和观测变量之间的关系，一种比较易于理解的表达方式是建立 “观测变量的生成过程”  。可以认为，观测变量是从某个已知的、由隐变量组成的结构中生成出来的。</p>
<p>以高斯混合模型为例（见下图）。图中为我们观测到一个数据集，它实际上是从由 5 个相互独立的高斯组合而成的一个混合分布中采样的结果。如果从单个数据点出发，考虑其生成过程，可以分为两步：首先从 5 个类别组成的离散型分布中抽取一个类别样本（比如粉红色类别），然后从类别对应的高斯分布中抽取一个样本生成相应数据点。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108085412-fdef.webp" /></p>
<blockquote>
<div><p>图 2 ： 5 个组份构成的高斯混合模型示意图</p>
</div></blockquote>
<p>在上例中，可以发现隐变量可能有多个：</p>
<ul class="simple">
<li><p>5 个高斯分布的均值参数 <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> 和方差参数 <span class="math notranslate nohighlight">\(\boldsymbol{\sigma^2}\)</span> （均为长度为 5 的向量）</p></li>
<li><p>数据点所属类别 <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> （独热向量形式）</p></li>
</ul>
<p>上述变量 <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>、<span class="math notranslate nohighlight">\(\boldsymbol{\sigma}^2\)</span> 和 <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> 一起构成了隐变量 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> ，而且 <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>、<span class="math notranslate nohighlight">\(\boldsymbol{\sigma}^2\)</span>  和 <span class="math notranslate nohighlight">\(\mathbf{c}\)</span> 之间可能存在一定的依赖关系。</p>
</div>
<div class="section" id="id6">
<h4>2.2.2 问题本质是基于观测数据推断隐变量的分布<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>后验概率 <span class="math notranslate nohighlight">\(p(\mathbf{z|x})\)</span> 的物理含义是：基于现有观测数据 <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> ，推断隐变量 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 的分布。</p>
<p>对于上面的高斯混合模型来说，变分推断的目的就是求得隐变量 <span class="math notranslate nohighlight">\(\mathbf{z} = \{\boldsymbol{\mu},\boldsymbol{\sigma^2},  \mathbf{c} \}\)</span>  的后验分布  <span class="math notranslate nohighlight">\(p(\mathbf{z} \mid \mathbf{x})\)</span> 。根据贝叶斯公式，<span class="math notranslate nohighlight">\(p(\mathbf{z} \mid \mathbf{x}) = p(\mathbf{z,x}) / p(\mathbf{x})\)</span> 。 根据专家提供的生成过程，能够写出联合分布 <span class="math notranslate nohighlight">\(p(\mathbf{z,x})\)</span> 的表达式，但边缘似然 <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> 是一个很难处理的分母项。当 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 为连续型时，边缘似然 <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> 需要求 <span class="math notranslate nohighlight">\(p(\mathbf{z,x})\)</span> 关于 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 所在空间的积分（即边缘化）；当 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 为离散型时，需要对所有可能的 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 求和；而积分（和求和）的计算复杂性随着样本数量的增加会呈指数增长。</p>
</div>
<div class="section" id="id7">
<h4>2.2.3 变分推断的关键是构造变分分布<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h4>
<p>变分推断需要构造变分分布 <span class="math notranslate nohighlight">\(q(\mathbf{z}; \boldsymbol{\nu})\)</span>，并通过优化调整 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span>，使 <span class="math notranslate nohighlight">\(q(\mathbf{z}; \boldsymbol{\nu})\)</span> 更接近真实后验 <span class="math notranslate nohighlight">\(p(\mathbf{z \mid x})\)</span> 。</p>
<p>在变分分布 <span class="math notranslate nohighlight">\(q(\mathbf{z}; \boldsymbol{\nu})\)</span> 中， <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 为隐变量，<span class="math notranslate nohighlight">\( \boldsymbol{\nu}\)</span> 是控制 <span class="math notranslate nohighlight">\(q\)</span> 形态的参数（例如：假设 <span class="math notranslate nohighlight">\(q\)</span> 为高斯分布，则 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span> 为均值和方差 ， 而对于泊松分布，则 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span> 为二值概率）。因此构造变分分布 <span class="math notranslate nohighlight">\(q\)</span> 分为两步：</p>
<ul class="simple">
<li><p>首先是概率分布类型的选择。通常依据 <span class="math notranslate nohighlight">\(p\)</span> 的形态，由专家给出，例如在上述高斯混合模型中，假设目标分布 <span class="math notranslate nohighlight">\(p\)</span> 服从多元高斯分布，则构造 <span class="math notranslate nohighlight">\(q\)</span> 时通常依然会考虑高斯分布。</p></li>
<li><p>其次是概率分布参数的确定。该确定过程是一个逐步优化迭代的过程，通常经过渐进调整 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span> 的值 ，使 <span class="math notranslate nohighlight">\(q\)</span> 逐渐逼近 <span class="math notranslate nohighlight">\(p\)</span> 。</p></li>
</ul>
</div>
<div class="section" id="id8">
<h4>2.2.4 变分推断是一个优化问题<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h4>
<p>变分推断是一个优化问题，其最直观的优化目标是最小化 <span class="math notranslate nohighlight">\(KL\)</span> 散度，但实际可操作的优化目标是最大化证据下界 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 。</p>
<p>直观地理解，变分推断的优化目标很明确，就是最小化 <span class="math notranslate nohighlight">\(q\)</span> 和 <span class="math notranslate nohighlight">\(p\)</span> 之间的差距，而信息论为我们提供了一个量化两个分布之间差距的工具 <span class="math notranslate nohighlight">\(KL\)</span> 散度。但不幸的是，<span class="math notranslate nohighlight">\(KL\)</span> 散度的计算并不简单，其计算表达式中同样存在难以处理的边缘似然积分项（边缘似然也称证据，在一般化的数学形式中也被成为配分函数）。为此，有人提出了可操作的优化目标 — 证据下界 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> （见 2.4 节）。</p>
<p>在证据下界 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 的计算表达式中，只包括联合分布  <span class="math notranslate nohighlight">\(p(\mathbf{z, x})\)</span>  和变分分布 <span class="math notranslate nohighlight">\(q(\mathbf{z}; \boldsymbol{\nu})\)</span> ，摆脱了难以处理的边缘似然积分项。并且在给定观测数据后，最大化 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 等价于最小化 <span class="math notranslate nohighlight">\(KL\)</span> 。 也就是说，<code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 的最大化过程结束时，获得的输出 <span class="math notranslate nohighlight">\(q(\mathbf{z}; \boldsymbol{\nu^\star})\)</span>就是我们寻求的最终变分分布。</p>
</div>
<div class="section" id="id9">
<h4>2.2.5 变分推断的形象化解释<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
<p>下图为变分推断的示意图。图中大圈表示了一个分布族，由参数 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span> 索引。我们也可以理解为，大圈是分布 <span class="math notranslate nohighlight">\(q\)</span> 的参数 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span> 的取值空间，圈中每个点均表示不同 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span> 对应的分布 <span class="math notranslate nohighlight">\(q\)</span> 。例如，在高斯混合模型的例子中， <span class="math notranslate nohighlight">\(q\)</span> 属于高斯分布族，则不同的均值和方差，代表了不同的分布 <span class="math notranslate nohighlight">\(q\)</span> 。从 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}^{init}\)</span> 到 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}^*\)</span> 的路径表示变分推断的优化迭代过程，通过优化更新 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}\)</span> 值来缩小 <span class="math notranslate nohighlight">\(q\)</span> 与 <span class="math notranslate nohighlight">\(p\)</span> 之间的差距，而这个差距通常采用 <span class="math notranslate nohighlight">\(KL\)</span> 散度来衡量。路径终点 <span class="math notranslate nohighlight">\(\boldsymbol{\nu}^*\)</span> 对应的分布 <span class="math notranslate nohighlight">\(p\)</span> 与真实后验 <span class="math notranslate nohighlight">\(p(z \mid x)\)</span> 之间应当是某种距离测度上的最小值。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108085528-fa88.webp" /></p>
<blockquote>
<div><p>图 3 ：变分推断示意图。图中隐变量为 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span>，包含局部变量和全局变量。<span class="math notranslate nohighlight">\(\nu\)</span> 对应了所有变分参数。</p>
</div></blockquote>
</div>
</div>
<div class="section" id="id10">
<h3>2.3 概率图表示<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id11">
<h4>2.3.1 贝叶斯概率框架<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h4>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108085443-66c1.webp" /></p>
<blockquote>
<div><p>图 4 ：贝叶斯概率框架示意图</p>
</div></blockquote>
<p>上图是贝叶斯统计问题的一般求解流程图。领域专家拥有知识可以用来建模，并且存在需要解答的问题。他们依据拥有的知识，给出带有归纳偏好的合理假设，构建出数据的生成过程模型 (Generative  Processing Model)。模型中主要包括隐变量、观测变量以及它们之间的依赖关系。</p>
<p>利用该模型，我们希望通过对数据的处理，挖掘出有价值的模式，然后实现各式各样的应用。</p>
<p>还是以高斯混合模型为例，可以将其生成过程形式化为：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
 \boldsymbol{\mu}_{k} &amp; \sim \mathcal{N}\left(0, \sigma^{2}\right), &amp; &amp; k=1, \ldots, K, &amp;（先验）\\
\mathbf{c}_{i} &amp; \sim \text { Categorical }(1 / K, \ldots, 1 / K), &amp; &amp; i=1, \ldots, n, &amp;（先验）\\
x_{i} \mid \mathbf{c}_{i}, \boldsymbol{\mu} &amp; \sim \mathscr{N}\left(\mathbf{c}_{i}^{\top}  \boldsymbol{\mu}, 1\right) &amp; &amp; i=1, \ldots, n &amp;（似然）
\end{aligned}
\end{split}\]</div>
<p>该模型混合了 <span class="math notranslate nohighlight">\(K\)</span> 个相互独立的高斯组份（ <span class="math notranslate nohighlight">\(K\)</span> 是超参数），模型中的隐变量包括数据点所在的类别 <span class="math notranslate nohighlight">\( \mathbf{c}_i\)</span>，各类别组份的均值 <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> 和方差 <span class="math notranslate nohighlight">\( \boldsymbol{\sigma}_k^2\)</span> （为简化问题，此处假设 <span class="math notranslate nohighlight">\( \boldsymbol{\sigma}_k^2\)</span> 为常数 1 ）。</p>
<p>设所有高斯组份的均值参数 <span class="math notranslate nohighlight">\(\boldsymbol{\mu}_k\)</span> 的先验为 0 均值的同方差（<span class="math notranslate nohighlight">\(\sigma^2\)</span>）高斯，如第一行所示；类别参数 <span class="math notranslate nohighlight">\( \mathbf{c}_i\)</span> 的先验为均匀的类别分布（因为缺乏关于类别的先验知识），如第二行所示。假设所有样本数据都是从某个方差为 <span class="math notranslate nohighlight">\(1\)</span> 的高斯混合模型中随机生成的，则任一数据点 <span class="math notranslate nohighlight">\(x_i\)</span> 的生成过程可以通过似然表达出来（如第三行所示），分为两步 ：</p>
<ul class="simple">
<li><p>第一步，从类别分布中随机抽取一个离散型的类标签 <span class="math notranslate nohighlight">\( \mathbf{c}_i\)</span> （此处采用独热变量形式）；</p></li>
<li><p>第二步，从类标签 <span class="math notranslate nohighlight">\( \mathbf{c}_i\)</span> 对应的均值为 <span class="math notranslate nohighlight">\(\mu =  \mathbf{c}_{i}^{\top} \boldsymbol{\mu}\)</span> 、方差为 <span class="math notranslate nohighlight">\(1\)</span> 的高斯分布中，随机抽取一个数据点 <span class="math notranslate nohighlight">\(x_i\)</span>。</p></li>
</ul>
<p>更细致的举例说明见图 2，五个分布用不同颜色表示，代表五个类别。每次从中选择一个类，如第三类粉红色，<span class="math notranslate nohighlight">\(\mathbf{c}_i=\{0，0，1，0，0\}\)</span> ，然后从第三类对应的高斯分布中抽取 <span class="math notranslate nohighlight">\(x_i\)</span> ，其均值为 <span class="math notranslate nohighlight">\(\mathbf{c}_i \cdot \boldsymbol{\mu}=\mu_3\)</span>。该点大概率出现在粉红色类覆盖的区域内。</p>
</div>
<div class="section" id="id12">
<h4>2.3.2 利用概率图表示和理解<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h4>
<p>概率图模型是表达随机变量之间结构和关系的有力工具（参见概率图理论文献）。上述高斯混合模型的形式化生成过程，在概率图模型中体现为三个随机变量及它们之间的依赖关系（见下图），其中高斯组份的均值 <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> 和数据点所属类别 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 为隐变量，<span class="math notranslate nohighlight">\(\mathbb{x}_i\)</span> 为观测变量。此外，可以做更细致的划分，将 <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> 视为全局随机变量，其作用发挥在所有数据上，而 <span class="math notranslate nohighlight">\(\mathbb{z}_i\)</span> 为局部随机变量，只和数据点 <span class="math notranslate nohighlight">\(x_i\)</span> 相关，与其他点无关。局部变量用矩形框与全局变量分隔开来。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108085458-b4a9.webp" /></p>
<blockquote>
<div><p>图 5：高斯混合模型的概率图（1）白圈为隐变量，灰圈为观测变量；（2）矩形框内为局部变量，外部为全局变量；（3）矩形框表示其内部的变量 <span class="math notranslate nohighlight">\(z_i\)</span> 和 <span class="math notranslate nohighlight">\(x_i\)</span> 独立重复了 n 次；（4）箭头表示随机变量之间的条件依赖关系。</p>
</div></blockquote>
<p>基于上述概率图，可以解析地写出所有随机变量的联合概率分布（详情参见概率图原理）：</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{\mu}, \mathbf{z}, \mathbf{x})=p(\boldsymbol{\mu}) \prod_{i=1}^{n} p\left(z_{i}\right) p\left(x_{i} \mid z_{i}, \boldsymbol{\mu}\right)
\]</div>
<p>那么在上述高斯混合模型中，到底需要变分方法推断什么？</p>
<p>根据统计机器学习框架，此例中观测数据 <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> 由无标签的数据点构成，我们需要根据这些数据点来推断 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 和 <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> 的分布。即给定观测数据 <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> 时，推断隐变量的后验概率。根据基础概率公式，其形式化描述为：</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{\mu}, \mathbf{c} \mid \mathbf{x})=\frac{p(\boldsymbol{\mu},\mathbf{c} , \mathbf{x})}{p(\mathbf{x})}
\]</div>
<p>式中，分子项为 <span class="math notranslate nohighlight">\(p(\boldsymbol{\mu},\mathbf{c} , \mathbf{x})\)</span> ， 可根据概率图模型理论写出表达式，但分母 <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> 涉及所有数据点的积分问题，是难以处理的，因此，需要用近似的、可处理的近似分布 <span class="math notranslate nohighlight">\(q(\boldsymbol{\mu},\mathbf{c})\)</span> 来代替后验分布 <span class="math notranslate nohighlight">\(p(\boldsymbol{\mu}, \mathbf{c} \mid \mathbf{x})\)</span> 。</p>
</div>
<div class="section" id="id13">
<h4>2.3.3 更一般的形式<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h4>
<p>我们把上述问题梳理一下，能够得到一个更一般的形式，可以用来描述各种各样的贝叶斯统计模型：</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108085516-aec3.webp" /></p>
<blockquote>
<div><p>图 6：包含全局变量和局部变量的一般性概率图表示</p>
</div></blockquote>
<p>在上面的概率模型中，后验概率的分子项为联合概率分布，可通过概率图得出表达式（图 6 下）；但分母中的边缘似然项是难以处理的（见下式）。即便隐变量是离散型变量，存在 <span class="math notranslate nohighlight">\(K\)</span> 个可能值，其计算复杂度也为 <span class="math notranslate nohighlight">\(O(K^n)\)</span> ，意味着其计算复杂度会随着数据量 <span class="math notranslate nohighlight">\(n\)</span> 的增长呈指数增长。</p>
<div class="math notranslate nohighlight">
\[
p(\beta, z \mid x)=\frac{p(\beta, z, x)}{p(x)}=\frac{p(\beta, z, x)}{\iint_{\beta z} p(x) d \beta d z}
\]</div>
</div>
</div>
<div class="section" id="id14">
<h3>2.4 目标函数的选择<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>要将推断转化为优化问题，需要选择或者构造一个变分分布族 <span class="math notranslate nohighlight">\(\mathcal{Q}\)</span> ，并设定合适的优化目标 <span class="math notranslate nohighlight">\(J(q)\)</span> 。 关于分布族的讨论放在下一节，此处先讨论优化目标 <span class="math notranslate nohighlight">\(J(q)\)</span> 。该目标需要捕获 <span class="math notranslate nohighlight">\(q\)</span> 和 <span class="math notranslate nohighlight">\(p\)</span> 之间的差距（或反之，相似性），而信息论为我们提供了一个直观并且好理解的工具，被称为 <strong>KL（ Kullback-Leibler ）散度</strong>。</p>
<p>从形式上理解，<em>KL 散度</em> 指两个分布之间的差异。 <span class="math notranslate nohighlight">\(q\)</span> 和 <span class="math notranslate nohighlight">\(p\)</span> 之间离散形式的 <strong>KL 散度</strong> 被定义为：</p>
<div class="math notranslate nohighlight">
\[
KL(q \| p) = \sum_x q(x) \log \frac{q(x)}{p(x)}
\]</div>
<p>在信息论中，此函数用于测量两个分布中所包含信息的差异大小。<span class="math notranslate nohighlight">\(KL\)</span> 散度具有以下性质，使其在近似推断任务中特别有用：</p>
<ul class="simple">
<li><p>对所有 <span class="math notranslate nohighlight">\(q\)</span> 和 <span class="math notranslate nohighlight">\(p\)</span> ，都有 <span class="math notranslate nohighlight">\(KL(q\|p) \geq 0\)</span> 。</p></li>
<li><p>当且仅当 <span class="math notranslate nohighlight">\(q = p\)</span> 时，<span class="math notranslate nohighlight">\(KL(q\|p) = 0\)</span> 。</p></li>
</ul>
<blockquote>
<div><p>相关证明过程可以作为练习，本文略过。</p>
</div></blockquote>
<p>但请注意： <span class="math notranslate nohighlight">\(KL\)</span> 散度不具备可逆性（或对称性），即 <span class="math notranslate nohighlight">\(KL(q\|p) \neq KL(p\|q)\)</span> 。这也是称之为<code class="docutils literal notranslate"><span class="pre">散度</span></code>，而不是<code class="docutils literal notranslate"><span class="pre">距离</span></code>的原因，因为距离是双向对称的。</p>
<p><span class="math notranslate nohighlight">\(KL\)</span> 散度虽然很好理解，但是是否能够直接使用 <span class="math notranslate nohighlight">\(KL\)</span> 散度进行变分推断呢？ 我们有必要针对一般化形式的目标分布 <span class="math notranslate nohighlight">\(p\)</span> 做一下分析。</p>
<p>假设 <span class="math notranslate nohighlight">\(p\)</span> 为一个泛化（离散、简单）的、具有如下形式的无方向概率模型（其实该形式是统计机器学习框架中后验分布的一般化形式，也是变分推断面临的主要场景）：</p>
<div class="math notranslate nohighlight">
\[
p(x_1,\ldots,x_n; \theta) = \frac{\tilde p(x_1,\ldots,x_n ; \theta)}{Z(\theta)} =\frac{1}{Z(\theta)} \prod_{k} \phi_k(x_k; \theta)
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(k\)</span> 为模型中因子的数量（有关因子的概念，参见无向概率图或马尔科夫随机场相关资料）， <span class="math notranslate nohighlight">\(\phi_k(\cdot)\)</span> 为因子， <span class="math notranslate nohighlight">\(Z(\theta)\)</span> 为归一化常数或配分函数。假设该形式能够涵盖所有可能的目标分布， 则在该一般化形式下，归一化常数 <span class="math notranslate nohighlight">\(Z(\theta)\)</span> 面临着难以处理的问题。由于 <span class="math notranslate nohighlight">\(KL\)</span> 散度的计算需要用到 <span class="math notranslate nohighlight">\(p(x)\)</span> ，因此 <span class="math notranslate nohighlight">\(Z(\theta)\)</span> 的存在导致无法直接使用 <span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 做为优化目标。</p>
<p>考虑到归一化常数虽然难以处理，但在观测数据确定的情况下是常数，因此，可以考虑仅处理形式上与 <span class="math notranslate nohighlight">\(KL\)</span> 散度相似的未归一化分子项 <span class="math notranslate nohighlight">\(\tilde p(x) = \prod_{k} \phi_k(x_k; \theta)\)</span> ，即将优化目标调整为：
$<span class="math notranslate nohighlight">\(
J(q) = \sum_x q(x) \log \frac{q(x)}{\tilde p(x)}
\)</span>$</p>
<p>上述目标函数中， <span class="math notranslate nohighlight">\(q(x)\)</span> 来自人工构造和 <span class="math notranslate nohighlight">\(\tilde p (x)\)</span> 来自于数据概率生成过程，各项均可处理。 更重要的是，其还具有以下重要性质：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*} 
J(q) &amp;= \sum_x q(x) \log \frac{q(x)} {\tilde p(x)} \\ 
&amp;= \sum_x q(x) \log \frac{q(x)}{p(x)} - \log Z(\theta) \\ 
&amp;= KL(q\|p) - \log Z(\theta) 
\end{align*}
\end{split}\]</div>
<p>也就是说，新的目标函数是 <span class="math notranslate nohighlight">\(KL\)</span> 散度与对数配分函数之差，而根据 <span class="math notranslate nohighlight">\(KL\)</span> 散度的性质， <span class="math notranslate nohighlight">\(KL(q\|p) \geq 0\)</span> ，则重排公式可得：</p>
<div class="math notranslate nohighlight">
\[
\log Z(\theta) = KL(q\|p) - J(q) \geq -J(q)
\]</div>
<p>该式表明，<span class="math notranslate nohighlight">\(-J(q)\)</span> 是对数配分函数 <span class="math notranslate nohighlight">\(\log Z(\theta)\)</span> 的下界。</p>
<p>在大多数应用场景中， 对配分函数 <span class="math notranslate nohighlight">\(Z(\theta)\)</span> 都有特别的解释。例如，在给定观测数据 <span class="math notranslate nohighlight">\(D\)</span> 的情况下，我们可能想计算随机变量 <span class="math notranslate nohighlight">\(x\)</span> 的边缘概率 <span class="math notranslate nohighlight">\(p(x \mid D) = p(x,D) / p(D)\)</span> 。在这种情况下，最小化 <span class="math notranslate nohighlight">\(J(q)\)</span> 等价于最大化对数边缘似然 <span class="math notranslate nohighlight">\(\log p(D)\)</span>  的下界。正是因为如此， <span class="math notranslate nohighlight">\(-J(q)\)</span> 被称为变分下界或证据下界（ <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> ），这种下界关系经常以如下不公式形式表示：</p>
<div class="math notranslate nohighlight">
\[
\log Z(\theta) \geq \mathbb{E}_{q(x)} [ \log \tilde p(x) - \log q(x) ]
\]</div>
<p>最关键的是，对数边缘似然 <span class="math notranslate nohighlight">\(\log Z(\theta)\)</span> 和变分下界 <span class="math notranslate nohighlight">\(-J(q)\)</span> 的差，正好是 <span class="math notranslate nohighlight">\(KL\)</span> 散度。 因此，最大化变分下界等效于通过 “挤压” <span class="math notranslate nohighlight">\(-J(q)\)</span> 和  <span class="math notranslate nohighlight">\(\log Z(\theta)\)</span> 之间的差，实现 <span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 的最小化。</p>
<hr class="docutils" />
<p><strong>讨论：</strong></p>
<p>刚刚为变分推断重新定义了优化目标，并且表明，最大化下界等效于最小化散度 <span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 。</p>
<p>回想之前说过的 <span class="math notranslate nohighlight">\(KL(q\|p) \neq KL(p\|q)\)</span> ; 当 <span class="math notranslate nohighlight">\(q = p\)</span> 时，两种散度都等于零，但当 <span class="math notranslate nohighlight">\(q \neq p\)</span> 时，两者的值不同。 这就提出了一个问题：为什么选择其中一个而非另一个，它们有何不同？</p>
<p>也许最重要的区别是计算效率：优化 <span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 时，涉及关于 <span class="math notranslate nohighlight">\(q\)</span> 求期望；而优化 <span class="math notranslate nohighlight">\(KL(p\|q)\)</span> 则需要关于 <span class="math notranslate nohighlight">\(p\)</span> 求期望，而这又是难以处理甚至无法评估的。</p>
<p>但当近似分布族 <span class="math notranslate nohighlight">\(\mathcal{Q}\)</span> 不包含真实分布 <span class="math notranslate nohighlight">\(p\)</span> 时，选择  <span class="math notranslate nohighlight">\(KL(p\|q)\)</span> 会影响返回的解。 通过观察会发现， 如果 <span class="math notranslate nohighlight">\(p(x) = 0\)</span> 并且 <span class="math notranslate nohighlight">\(q(x) &gt; 0\)</span> 时， <span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 是无限的 （ 该散度有时被称为 I 投影或信息投影 ） :</p>
<div class="math notranslate nohighlight">
\[
KL(q\|p) = \sum_x q(x) \log \frac{q(x)}{p(x)}
\]</div>
<p>这意味着，如果 <span class="math notranslate nohighlight">\(p(x) = 0\)</span> ，则必须有 <span class="math notranslate nohighlight">\(q(x) = 0\)</span> 。 我们称： <span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 对于 <span class="math notranslate nohighlight">\(q\)</span> 是零强制的，并且它通常会低估 <span class="math notranslate nohighlight">\(p\)</span> 的支持。</p>
<p>另一方面，当 <span class="math notranslate nohighlight">\(q(x) = 0\)</span> 并且 <span class="math notranslate nohighlight">\(p(x) &gt; 0\)</span> 时， <span class="math notranslate nohighlight">\(KL(p\|q)\)</span> 是无限的 （ 该散度被称为 M 投影或矩投影）。 因此，如果 <span class="math notranslate nohighlight">\(p(x) &gt; 0\)</span> ，则必须有 <span class="math notranslate nohighlight">\(q(x) &gt; 0\)</span> 。 此时，我们称： <span class="math notranslate nohighlight">\(KL(p\|q)\)</span> 对于 <span class="math notranslate nohighlight">\(q\)</span> 是零避免的，并且它通常会高估 <span class="math notranslate nohighlight">\(p\)</span> 的支持。</p>
<p>下图以图形方式说明了该现象。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108081010-6d1d.webp" /></p>
<blockquote>
<div><p>图 7：将单峰近似分布 <span class="math notranslate nohighlight">\(q\)</span>（红色）拟合到多峰 <span class="math notranslate nohighlight">\(p\)</span>（蓝色）。 (a) 使用 <span class="math notranslate nohighlight">\(KL(p||q)\)</span> 会导致 <span class="math notranslate nohighlight">\(q\)</span> 试图覆盖两个峰。 (b）(c) 使用 <span class="math notranslate nohighlight">\(KL(q||p)\)</span> 会迫使 <span class="math notranslate nohighlight">\(q\)</span> 选择 <span class="math notranslate nohighlight">\(p\)</span> 的其中一个峰。</p>
</div></blockquote>
<p>鉴于两种散度的上述性质，我们经常称 <span class="math notranslate nohighlight">\(KL(p\|q)\)</span> 为包容性 <span class="math notranslate nohighlight">\(KL\)</span> 散度， 而 <span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 是独占性的 <span class="math notranslate nohighlight">\(KL\)</span> 散度。</p>
<hr class="docutils" />
<p>小结：</p>
<ul class="simple">
<li><p>由于配分函数 <span class="math notranslate nohighlight">\(Z\)</span> 的存在，使得直接计算 <span class="math notranslate nohighlight">\(KL\)</span> 散度难以处理</p></li>
<li><p>采用等价的最大化 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 来代替最小化 <span class="math notranslate nohighlight">\(KL\)</span> 散度作为新的优化目标</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 中只包含变分分布 <span class="math notranslate nohighlight">\(q\)</span> 和目标分布的分子项 <span class="math notranslate nohighlight">\(\tilde p\)</span> ，并且两者都可计算，进而可以作为真正的优化目标</p></li>
<li><p><span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 和 <span class="math notranslate nohighlight">\(KL(p\|q)\)</span> 虽然都是散度，也都可以作为优化目标，但作为目标时两者达到的效果截然不同。对于多峰后验，前者倾向于选择能够覆盖其中某个峰的变分分布，而后者倾向于选择能够覆盖多个峰的变分分布。</p></li>
</ul>
</div>
</div>
<div class="section" id="id15">
<h2>3 经典方法 — 平均场、指数族与坐标上升算法<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id16">
<h3>3.1 平均场近似的基本原理<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>在构造变分分布时，需要在 <span class="math notranslate nohighlight">\(q(\mathbf{z};\boldsymbol{\lambda}) \)</span> 的表达能力和简单易处理性之间做权衡。其中一种常见的选择是使用完全因子分解的方法来构造分布，也称为平均场分布。</p>
<p>平均场近似假设所有隐变量相互独立，进而简化了推导。但这种独立性假设也会导致不太准确的近似，当后验中的随机变量存在高度依赖时尤其如此。因此，有人研究更为准确的近似方法，本文第 6 节会讨论此类更具表现力的变分分布。</p>
<p>平均场变分推断 ( MFVI ) 起源于统计物理学的<a class="reference external" href="https://ieeexplore.ieee.org/book/6267422">平均场理论</a>。在平均场近似中，基于独立性假设，变分分布被分解为各因子分布的乘积，而每个因子由其自身变分参数控制：</p>
<div class="math notranslate nohighlight">
\[
q(\mathbf{z};\boldsymbol{\lambda}) = \prod_{i=1}^N q(z_i ; \lambda_i)
\]</div>
<p>为了符号简单，我们在本节的其余部分省略了变分参数 <span class="math notranslate nohighlight">\(λ\)</span> 。我们现在回顾如何在平均场假设下最大化公式（ 3 ）中定义的 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 。</p>
<p>完全分解的变分分布允许通过简单的迭代更新来优化。为了看到这一点，我们专注于更新与隐变量 <span class="math notranslate nohighlight">\(z_j\)</span> 相关的变分参数 <span class="math notranslate nohighlight">\(λ_j\)</span> 。将平均场分布插入公式（ 3 ）允许我们表达 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 如下：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\mathscr{L}= \int q\left(z_{j}\right) \mathbb{E}_{q\left(z_{\neg j}\right)}\left[\log p\left(z_{j}, \boldsymbol{x} \mid \boldsymbol{z}_{\neg j}\right)\right] d z_{j} \\
-\int q\left(z_{j}\right) \log q\left(z_{j}\right) d z_{j}+c_{j}
\end{aligned}
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\boldsymbol{z}_{\neg j}\)</span> 表示集合 <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> 中除不包括 <span class="math notranslate nohighlight">\(z_{j}\)</span> 常数 <span class="math notranslate nohighlight">\(c_{j}\)</span> 包含所有关于常数的项<span class="math notranslate nohighlight">\(z_{j}\)</span>，例如与 <span class="math notranslate nohighlight">\(z_{\neg j}\)</span> 相关的熵项。因此，我们将完全期望分为对 <span class="math notranslate nohighlight">\(\boldsymbol{z}_{\neg j}\)</span> 的内部期望和对 <span class="math notranslate nohighlight">\(z_{j}\)</span> 的外部期望。</p>
<p>公式（ 6 ）采用负 <span class="math notranslate nohighlight">\(\mathrm{KL}\)</span> 散度的形式，对于变量 <span class="math notranslate nohighlight">\(j\)</span> 最大化</p>
<div class="math notranslate nohighlight">
\[
\log q^{*}\left(z_{j}\right)=\mathbb{E}_{q\left(z_{\neg j}\right)}\left[\log p\left(z_{j} \mid \boldsymbol{z}_{\neg j}, \boldsymbol{x}\right)\right]+\text { const }
\]</div>
<p>对这个结果求幂和归一化产生：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
q^{*}\left(z_{j}\right) &amp; \propto \exp \left(\mathbb{E}_{q\left(z_{\neg j}\right)}\left[\log p\left(z_{j} \mid z_{\neg j}, \boldsymbol{x}\right)\right]\right) \\
&amp; \propto \exp \left(\mathbb{E}_{q\left(z_{\neg j}\right)}[\log p(z, \boldsymbol{x})]\right)
\end{aligned}
\end{split}\]</div>
<p>使用公式。如图8所示，可以针对每个隐变量迭代地更新变分分布直到收敛。类似的更新也构成了变分消息传递算法 [216]（附录 A.3）的基础。</p>
<p>有关平均场近似及其几何解释的更多详细信息，请读者参阅 [14] 和 [205]。</p>
</div>
<div class="section" id="id17">
<h3>3.2 指数族分布的适用性<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>变分推断的另一个重要的技术点在于变分分布族 <span class="math notranslate nohighlight">\(\mathcal{Q}\)</span> 的选择。在高斯混合模型中，我们知道目标分布 <span class="math notranslate nohighlight">\(p\)</span> 是高斯的，那么将变分分布 <span class="math notranslate nohighlight">\(q\)</span> 构造为高斯的，会更容易逼近 <span class="math notranslate nohighlight">\(p\)</span> 。</p>
<p>但对于更为复杂的目标分布怎么办？答案是：<strong>可以用更广泛的指数族分布来构造！！！</strong></p>
<p>数学上已经证明，<span class="math notranslate nohighlight">\(p\)</span> 不一定必须是高斯的，只要它属于指数族分布，那么我们都可以将变分分布 <span class="math notranslate nohighlight">\(q\)</span> 构造为同一指数族的分布。因为指数族分布有一些很好的性质，允许我们很巧妙地简化自然梯度的推导。事实上指数族分布非常宽泛，基本涵盖了常见的分布形态，如：高斯分布、<span class="math notranslate nohighlight">\(\chi ^2\)</span> 分布、伯努利分布、指数分布、贝塔分布、伽马分布、泊松分布等。</p>
<p>但是，当 <span class="math notranslate nohighlight">\(p\)</span> 本身确实并不属于指数族时， <span class="math notranslate nohighlight">\(q\)</span> 可能永远无法近似 <span class="math notranslate nohighlight">\(p\)</span> ，而这也是变分推断的一个缺陷，即最终计算得出的结果是一个难以提前估计的近似。</p>
</div>
<div class="section" id="id18">
<h3>3.3 坐标上升优化算法<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>在构造 <span class="math notranslate nohighlight">\(q\)</span> 时，除了对分布族做选择之外，还需要考虑不同随机变量之间的依赖关系问题。其中一种最简单的近似，就是假设隐变量可以被分解为多个因子，而因子之间相互独立，根据概率图模型，此时隐变量的联合分布可以被分解为多个因子分布的乘积（见下图）。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108085537-3530.webp" /></p>
<blockquote>
<div><p>图 8：独立因子假设及其概率图表示</p>
</div></blockquote>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108085543-3d28.webp" /></p>
<blockquote>
<div><p>图 9：完全因子分解后的平均场变分族</p>
</div></blockquote>
<p>在这种因子分解形式的近似中，完全因子分解是使用最广泛的一种构造形式，即每个隐变量有自己独立的分布，与其他隐变量无关，数学形式为： <span class="math notranslate nohighlight">\(q(z) = q_1(z_1) q_2(z_2) \cdots q_n(z_n) \)</span>，其中所有的 <span class="math notranslate nohighlight">\(q_i(z_i)\)</span> 都是一元离散变量上的类别分布（ <code class="docutils literal notranslate"><span class="pre">Catalogical</span> <span class="pre">Distribution</span></code> ），可描述为一维表形式。</p>
<p>事实表明，对于 <span class="math notranslate nohighlight">\(\mathcal{Q}\)</span> 的这种选择很容易优化，并且工作地出奇得好。在优化变分下界时，这种因子分解的方法也许是目前最流行的一种选择。</p>
<p>这种采用完全因子分解方法的变分推断，被称为<strong>平均场近似推断</strong>。此方法包含以下需要解决的优化问题：</p>
<div class="math notranslate nohighlight">
\[
\min_{q_1, \ldots, q_n} J(q) 
\]</div>
<p>解决此优化问题的标准方法是在 <span class="math notranslate nohighlight">\(q_j\)</span> 上实施坐标下降做最小化（或坐标上升做最大化）： 我们在 <span class="math notranslate nohighlight">\(j=1,2,\ldots,n\)</span> 上做迭代，对于每一个 <span class="math notranslate nohighlight">\(j\)</span> ，在保持其他”坐标” <span class="math notranslate nohighlight">\(q_{-j} = \prod_{i \neq j} q_i\)</span> 固定的情况下，仅关于 <span class="math notranslate nohighlight">\(q_j\)</span> 来优化 <span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 。</p>
<p>最有趣的是：<strong>单一坐标的优化问题具有一个简单的闭式解</strong>：</p>
<div class="math notranslate nohighlight">
\[
 \log q_j(z_j) \gets \mathbb{E}_{q_{-j}} \left[ \log \tilde p(z) \right] + \textrm{const}
\]</div>
<p>注意上述公式两侧都包含一元隐变量 <span class="math notranslate nohighlight">\(z_j\)</span> 的函数，我们只是用右侧的另一个函数取代了 <span class="math notranslate nohighlight">\(q(z_j)\)</span> 。右式中的常数项为新分布的归一化常数，此外还包含一个因子总和的期望：</p>
<div class="math notranslate nohighlight">
\[
\log \tilde p(z) = \sum_k \log \phi(z_k)
\]</div>
<p>根据无向概率图模型（即马尔科夫随机场理论），只有在变量 <span class="math notranslate nohighlight">\(z_j\)</span> 的马尔科夫毯内的因子，才是 <span class="math notranslate nohighlight">\(z_j\)</span> 的函数 ，马尔科夫毯之外的其余因子对于 <span class="math notranslate nohighlight">\(z_j\)</span> 都可视为常数，可以纳入常数项 <span class="math notranslate nohighlight">\(\textrm{constant}\)</span>。</p>
<p>这样能够大量减少期望公式中的因子数量；当 <span class="math notranslate nohighlight">\(z_j\)</span> 的马尔科夫毯很小时，可以比较容易给出 <span class="math notranslate nohighlight">\(q(z_j)\)</span> 的解析公式。 例如，假设变量 <span class="math notranslate nohighlight">\(z_j\)</span> 是具有 <span class="math notranslate nohighlight">\(K\)</span> 个可能值的离散型变量，并且在 <span class="math notranslate nohighlight">\(z_j\)</span> 的马尔科夫毯中有 <span class="math notranslate nohighlight">\(F\)</span> 个因子和 <span class="math notranslate nohighlight">\(N\)</span> 个变量，则计算期望值的时间复杂度为 <span class="math notranslate nohighlight">\(O(K F K^N)\)</span>：即对于每个 <span class="math notranslate nohighlight">\(z_j\)</span> 的 <span class="math notranslate nohighlight">\(K\)</span> 种可能的取值，需要对 <span class="math notranslate nohighlight">\(N\)</span> 个变量的所有 <span class="math notranslate nohighlight">\(K^N\)</span> 次赋值以及 <span class="math notranslate nohighlight">\(F\)</span> 个因子求和。</p>
<p>坐标下降过程的结果是：</p>
<p>依据 <span class="math notranslate nohighlight">\(KL(q\|p)\)</span> 迭代地拟合了用于近似 <span class="math notranslate nohighlight">\(p\)</span> 的完全因子式 <span class="math notranslate nohighlight">\(q(z) = q_1(z_1) q_2(z_2) \cdots q_n(z_n) \)</span> ； 而坐标下降的每一步，都增加了变分下界，使其向  <span class="math notranslate nohighlight">\(\log Z(\theta)\)</span> 进一步收紧。 最终的因子 <span class="math notranslate nohighlight">\(q_j(z_j)\)</span> 不会完全等于真正的边缘分布 <span class="math notranslate nohighlight">\(p(z_j)\)</span> ，但对于许多实际目的，它们通常表现足够好，如确定 <span class="math notranslate nohighlight">\(z_j\)</span> 的峰值： <span class="math notranslate nohighlight">\(\max_{z_j} p(z_j)\)</span> 。</p>
<p>还是用高斯混合模型的例子，根据概率图模型有：</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{\mu},\mathbf{c},\mathbf{x})=p(\boldsymbol{\mu})\prod_{i=1}^{n} p(c_i)p(x_i \mid c_i,\boldsymbol{\mu})
\]</div>
<p>该模型中涉及的隐变量为 <span class="math notranslate nohighlight">\(\mathbf{z} = \{\boldsymbol{\mu},\mathbf{c} \}\)</span> ，其中 <span class="math notranslate nohighlight">\(\boldsymbol{\mu} =\{ \mu_k\}, k=1...5\)</span>  为 5 个高斯组份的均值；<span class="math notranslate nohighlight">\(\mathbf{c}\)</span> 为所有数据点的类别向量。<span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> 作为全局变量，其变分分布被构造为高斯分布  <span class="math notranslate nohighlight">\(q(u_k;m_k,S_k)\)</span> ， <span class="math notranslate nohighlight">\(\nu_\mu = \{m_k,S_k\},k=1...5\)</span>  为需要优化求解的变分参数。 <span class="math notranslate nohighlight">\(c_i\)</span> 为局部变量，其变分分布的构造形式为多项分布  <span class="math notranslate nohighlight">\(q(c_i ;\psi_i)\)</span> ，其中 <span class="math notranslate nohighlight">\(\nu_c=\{ \psi_i \}, i=1...n\)</span> 为控制每个数据点类别的变分参数（注意 <span class="math notranslate nohighlight">\(\psi_i\)</span> 为向量）。</p>
<p>按照平均场近似方法，各因变量之间相互独立，则变分分布 <span class="math notranslate nohighlight">\(q(\boldsymbol{\mu},c)\)</span> 可被构造为：</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{\mu},\mathbf{c} \mid \mathbf{x}) \approx q(\boldsymbol{\mu},c)=\prod_{k=1}^{K} q(\mu_k;m_k,S_k^2) \prod_{i=1}^{n}q(c_i ;\psi_i)
\]</div>
</div>
</div>
<div class="section" id="id19">
<h2>4 提升可扩展性 — 随机变分推断<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h2>
<p>上节中讲解了平均场变分分布的构造原理，并介绍了采用坐标上升法对其做优化的算法。但该方法存在两个方面的问题：</p>
<ul class="simple">
<li><p>坐标上升过程需要所有样本参与计算，不适合大样本</p></li>
<li><p>梯度下降并非最优方向，收敛速度较慢</p></li>
</ul>
<p>随机变分推断方法针对上述问题，分别进行了处理。下面分别介绍。</p>
<div class="section" id="id20">
<h3>4.1 如何适应大数据 ？<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<p>在最原始的坐标上升方法中，所有的隐变量同等对待，逐一更新一遍效率较低。为此，有学者对其做了改进，将隐变量分为两种类型，即仅关系单个数据点的局部参数，和关系到所有数据点的全局参数（ <span class="math notranslate nohighlight">\(\lambda\)</span> ） 。算法 1 对其进行了高层次的总结：</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108160249-c296.webp" /></p>
<p>如算法中所示，即便是这样，在更新全局参数之前，也需要先循环遍历所有文档一遍（单批次梯度下降）。当文档太多时，这就会成为了一个新问题，模型更新的频率太低。不过，该算法将参数区分为全局参数和局部参数的作法，为随机梯度下降提供了一个思路。</p>
<p>为了适应随机梯度下降，可将借鉴算法 1 的思路，将下界分解为两部分（采用对数形式）：<strong>由局部参数 <span class="math notranslate nohighlight">\(\phi_i\)</span> 参数化的逐数据点项</strong> 和 <strong>由全局参数 <span class="math notranslate nohighlight">\(\lambda\)</span> 参数化的全局项</strong>：</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\left(\lambda, \phi_{1: n}\right)=\underbrace{\mathbb{E}_{q}[\log p(\beta)-\log q(\beta \mid \lambda)]}_{\text {global contribution }}+\sum_{i=1}^{n} \underbrace{\left\{\mathbb{E}_{q}\left[\log p\left(w_{i}, z_{i} \mid \beta\right]-\log q\left(z_{i} \mid \phi_{i}\right)\right\}\right.}_{\text {per-data point contribution }}
\]</div>
<p>令下式为全局分布：</p>
<div class="math notranslate nohighlight">
\[
f(\lambda):=\mathbb{E}_{q}[\log p(\beta)-\log q(\beta \mid \lambda)]
\]</div>
<p>且令下式为第 <span class="math notranslate nohighlight">\(i\)</span> 个数据点的逐数据点分布。</p>
<div class="math notranslate nohighlight">
\[
g_{i}\left(\lambda, \psi_{i}\right):=\mathbb{E}_{q}\left[\log p\left(w_{i}, z_{i} \mid \beta\right]-\log q\left(z_{i} \mid \phi_{i}\right)\right.
\]</div>
<p>则下界可简写为：</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}\left(\lambda, \phi_{1: n}\right)=f(\lambda)+\sum_{i=1}^{n} \mathrm{g}_{i}\left(\lambda, \phi_{i}\right)
\]</div>
<p>为了优化目标，可以首先关于参数 <span class="math notranslate nohighlight">\(\phi_{1: n}\)</span> 最大化，这将产生单参数的下界：</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\lambda)=f(\lambda)+\sum_{i=1}^{n} \max _{\phi_{i}} \mathrm{g}_{i}\left(\lambda, \phi_{i}\right)
\]</div>
<p>令每个数据点的优化为：</p>
<div class="math notranslate nohighlight">
\[
\phi_{i}^{*}=\arg \max _{\phi} \mathrm{g}_{i}\left(\lambda, \phi_{i}\right)
\]</div>
<p>则单参数下界 <span class="math notranslate nohighlight">\(\mathcal{L}(λ)\)</span> 的梯度具有以下形式 ：</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial \mathcal{L}(\lambda)}{\partial \lambda}=\frac{\partial f(\lambda)}{\partial \lambda}+\sum_{i=1}^{n} \frac{\partial \mathrm{g}_{i}\left(\lambda, \phi_{i}^*\right)}{\partial \lambda} \tag{7}
\]</div>
<p>由于逐数据点项是每个数据点贡献的总和，每个数据点的导数也可以被累加起来计算每个数据点项的导数，如公式（ 7 ）的第二项所示。这使我们可以使用随机梯度算法来频繁地更新模型，以获得更好的收敛性。 一旦全局参数 <span class="math notranslate nohighlight">\(\lambda\)</span> 被估计出来，每个 <span class="math notranslate nohighlight">\(\phi_i\)</span> 也都可以在线估计出来。</p>
</div>
<div class="section" id="id21">
<h3>4.2 参数梯度与自然梯度<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<p>坐标上升法以及上述面向随机梯度方法的改造，都解决不了另外一个问题：下界的优化是在变分参数空间内进行的，并非基于概率分布，而这会导致更新和收敛速度变慢。这是由于变分参数仅仅是为了描述变分分布的，下界在变分参数空间中的梯度，通常并不是概率分布空间中上升（或下降）最快的那个方向。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211114112004-358c.webp" /></p>
<blockquote>
<div><p>图 10：变分参数空间中的梯度与概率分布空间中的梯度</p>
</div></blockquote>
<p>产生上述原因的根本原因在于：<strong>变分参数空间中的欧式距离无法准确度量分布之间的距离。</strong></p>
<p>举例说明这种现象：</p>
<p>两组具有相同均值和方差的高斯分布对（可以想象为变分分布 <span class="math notranslate nohighlight">\(q\)</span> 和 真实分布 <span class="math notranslate nohighlight">\(p\)</span> ），虽然两者在变分参数（此例指均值）空间中具有相同的“距离”，但其 KL 散度（即下图中同颜色的两个高斯分布之间的重叠区域，下界会有与其相对应的反应）却截然不同。如果固定均值为 0 ，仅考虑方差作为变分参数时，会有类似现象产生。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108161120-f068.webp" /></p>
<blockquote>
<div><p>图 11：具有相同变分参数（此处为均值）的两对高斯分布，虽然在变分参数空间中，两者距离相等，但是在概率分布的相似性度量上（可直观理解为两个分布之间的重叠区域），两者并不相等。同时表明，在参数空间的欧式距离上求梯度，不能代表最速下降（或上升）方向。</p>
</div></blockquote>
<p>那能否能够直接基于概率分布空间做优化求解呢？答案是肯定的：那就是<strong>将自然梯度方向作为优化的梯度方向</strong>。</p>
<blockquote>
<div><p>自然梯度是 Amari 于 1998 年提出的，主要用于衡量基于统计模型（如 KL 散度）的目标函数。此处知识点可参见 <a class="reference external" href="https://arxiv.org/pdf/1412.1193">文献综述</a></p>
</div></blockquote>
<p>无论是从 KL 散度的角度，还是从变分下界的角度，我们期望的目标函数 <span class="math notranslate nohighlight">\(\mathcal{L}(\lambda,\phi_{1..n})\)</span> 都是基于概率分布的。 <a class="reference external" href="https://arxiv.org/pdf/1412.1193">文献</a> 证明： Fisher 信息矩阵（Fisher Information Matrix, FIM）是两个概率分布之间 KL 散度的二阶近似，它表示了统计流形（即概率分布空间）上的局部曲率。进而推导得出，自然梯度可以定义为：</p>
<div class="math notranslate nohighlight">
\[
\delta \theta^*==\frac{1}{\lambda} F^{-1}\nabla_\theta \mathcal{L}(\lambda,\phi_{1..n}) 
\]</div>
<p>式中的 <span class="math notranslate nohighlight">\(F\)</span> 为 Fisher 信息矩阵。根据公式也可以看出，自然梯度考虑了参数空间上的曲率信息 <span class="math notranslate nohighlight">\( \nabla_\theta \mathcal{L}(\lambda,\phi_{1..n})\)</span>。</p>
</div>
<div class="section" id="svi-sgd">
<h3>4.3 随机变分推断（ SVI ）：自然梯度与 SGD 的结合<a class="headerlink" href="#svi-sgd" title="Permalink to this headline">¶</a></h3>
<p>既然给出了目标函数的最速梯度方向，那么与 4.1 节的随机梯度下降相结合就成为一种非常自然的想法。大家都知道， SGD 是小批量梯度下降的特例，由于每次仅随机地使用一个样本（这也是取名为随机梯度下降的原因），因此会引入较大的方差，但总体趋向于最优解。以下算法 2 为<strong>随机变分变分推断算法</strong>， 其基本思想是：在每一轮迭代中，随机抽取一个样本数据点并计算最优局部参数，然后根据自然梯度公式更新全局参数，直至收敛。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211108163233-7872.webp" /></p>
</div>
<div class="section" id="svi">
<h3>4.4 随机变分推断（ SVI ）中的一些技巧<a class="headerlink" href="#svi" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id22">
<h4>4.4.1 自适应学习率<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id23">
<h4>4.4.2 方差减少策略<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
<div class="section" id="id24">
<h2>5 提升通用性 — 黑盒变分推断<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h2>
<div class="section" id="bbvi">
<h3>5.1 为何要做黑盒变分推断（BBVI）？<a class="headerlink" href="#bbvi" title="Permalink to this headline">¶</a></h3>
<p>上面章节中，我们针对特定模型做出变分推断，其中大家应该已经注意到了，在 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 的计算表达式中，需要人为设定 <span class="math notranslate nohighlight">\(q\)</span> ，并给出其数学期望的解析表达式（事实上，<a class="reference external" href="http://www.nowpublishers.com/article/Details/MAL-001">文献</a> 表明，该方法只适用于<strong>条件共轭指数族分布</strong>）。考虑到现实世界中可能存在无数种模型，而且大部分可能是非共轭的，即便符合条件共轭假设，为每一个模型设计一种变分方案显然也是不可接受的。</p>
<p>因此，人们自然而然在思考：是否存在一个不需特定于某种模型的通用解决方案 ？这个解决方案最好将像黑匣子一样，只需输入模型和海量数据，然后就自动输出变分分布（或变分参数）。事实表明，这是有可能的，此类推断方法被称为<strong>黑盒变分推断（BBVI）</strong>。</p>
<blockquote>
<div><p>黑盒变分推断的概念最早出现在文献 <a class="reference external" href="https://arxiv.org/pdf/1401.0118">Ranganath et al., 2014</a> 中和 <a class="reference external" href="https://arxiv.org/pdf/1401.1022">Sal-imans 和 Knowles，2014</a>; <a class="reference external" href="https://arxiv.org/abs/1312.6114v10">Kingma and Welling, 2014</a> 和 <a class="reference external" href="https://arxiv.org/abs/1401.4082">Rezende et al., 2014</a> 提出了利用重参数化技巧实现反向传播和优化的方法；<a class="reference external" href="https://arxiv.org/abs/1505.05770">Rezende and Mohamed, 2015</a> 提出了归一化流的 BBVI 方案、<a class="reference external" href="https://arxiv.org/abs/158.06499">Tran et al.,2016</a> 提出了变分高斯过程的 BBVI 方案，均提升了变分推断的精度；<a class="reference external" href="https://arxiv.org/abs/1603.00788">Alp Kucukelbir et al, 2016</a> 提出自动微分变分推断方法（ ADVI ）；<a class="reference external" href="https://arxiv.org/abs/1505.00519">Yuri Burda et al., 2016</a> 在 VAE 基础上，提出了重要性加权变分自编码器；<a class="reference external" href="https://arxiv.org/abs/1807.09034">J Domke and D Sheldon, 2018</a> 对其进行了泛化，提出了重要性加权变分推断。</p>
</div></blockquote>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211115171851-9244.webp" /></p>
<blockquote>
<div><blockquote>
<div><p>图 12： 变分推断概念图，愿景是：（1）可以轻松对任何模型进行变分推断；（2）可以利用海量数据进行推断；（3）用户只需指定模型而不需要做其他数学工作。</p>
</div></blockquote>
</div></blockquote>
<p>BBVI 大致分为两种类型：</p>
<ul class="simple">
<li><p><strong>基于评分梯度</strong>的黑盒变分推断（ BBVI ）</p></li>
<li><p><strong>基于重参数化梯度</strong>的黑盒变分推断（ BBVI ）</p></li>
</ul>
<p>后者是变分自编码器 (VAE) 的基础。</p>
</div>
<div class="section" id="id25">
<h3>5.2 使用评分梯度的 BBVI<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h3>
<p>考虑如下概率模型，其中  <span class="math notranslate nohighlight">\(\mathbf{x}\)</span>  是观测变量， <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 是隐变量，其变分分布为 <span class="math notranslate nohighlight">\(q(\mathbf{z} \mid \lambda)\)</span> 。变分下界 (<code class="docutils literal notranslate"><span class="pre">ELBO</span></code>) 为：</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\lambda) \triangleq \mathbb{E}_{q_{\lambda}(\mathbf{z})}[\log p(\mathbf{x}, \mathbf{z})-\log q(\mathbf{z} \mid \lambda)] \tag{9}
\]</div>
<p><code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 关于 <span class="math notranslate nohighlight">\(\lambda\)</span> 的梯度为：</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\lambda} \mathcal{L}=\mathbb{E}_{q}\left[\nabla_{\lambda} \log q(\mathbf{z} \mid \lambda)(\log p(\mathbf{x}, \mathbf{z})-\log q(\mathbf{z} \mid \lambda))\right] \tag{10}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\nabla_{\lambda} \log q(\mathbf{z} \mid \lambda)\)</span> 被称为评分函数。</p>
<p>使用式（ 10 ） 中的评分梯度，就可以利用蒙特卡罗的优势，用变分分布的样本来计算 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 的含噪声无偏梯度：</p>
<div class="math notranslate nohighlight">
\[
\nabla_\lambda \approx \frac{1}{S} \sum_{S=1}^{S} \nabla_\lambda \log q(z_S \mid \lambda)(\log p(\mathbf{x},z_S) - \log q(z_S \mid \lambda))
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(z_S \sim q(\mathbf{z} \mid \lambda) \)</span> 。</p>
<br>
<hr class="docutils" />
<p><strong>式 (10) 的证明</strong></p>
<blockquote>
<div><p>此处见 <a class="reference external" href="http://www.cs.columbia.edu/~blei/fogm/2018F/materials/RanganathGerrishBlei2014.pdf">参考文献</a>
与推导出式（10）需要两个基本事实：</p>
</div></blockquote>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\nabla_{\lambda} q_{\lambda}(\mathbf{z})=\frac{1}{q_{\lambda}(\mathbf{z})} \nabla_{\lambda} q_{\lambda}(\mathbf{z})=q_{\lambda}(\mathbf{z}) \nabla_{\lambda} q_{\lambda}(\mathbf{z})\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbb{E}_{q}\left[\nabla_{\lambda} \log q_{\lambda}(\mathbf{z})\right]=0\)</span> ， 即对数似然梯度（评分函数）的期望为零。</p></li>
</ul>
<p>基于这两个事实，可以推导出 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 的评分梯度：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
\nabla_{\lambda} \mathcal{L} &amp;=\nabla_{\lambda} \int_{z}\left[q_{\lambda}(z) \log p(x, z)-q_{\lambda}(z) \log q_{\lambda}(z)\right] \mathrm{d} z \\
&amp;=\int_{z}\left\{\log p(x, z) \nabla_{\lambda} q_{\lambda}(z)-\left[\nabla_{\lambda} q_{\lambda}(z) \log q_{\lambda}(z)+q_{\lambda}(z) \frac{1}{q_{\lambda}(z)} \nabla_{\lambda} q_{\lambda}(z)\right]\right\} \mathrm{d} z \\
&amp;=\int_{z} \nabla_{\lambda} q_{\lambda}(z)\left[\log p(x, z)-\log q_{\lambda}(z)-1\right] \mathrm{d} z \\
&amp;=\int_{z} q_{\lambda}(z) \nabla_{\lambda} q_{\lambda}(z)\left[\log p(x, z)-\log q_{\lambda}(z)-1\right] \mathrm{d} z \\
&amp;=\mathbb{E}_{q_{\lambda}}\left[\nabla_{\lambda} q_{\lambda}(z)\left(\log p(x, z)-\log q_{\lambda}(z)\right)\right]-\mathbb{E}_{q_{\lambda}}\left[\nabla_{\lambda} q_{\lambda}(z)\right] \\
&amp;=\mathbb{E}_{q_{\lambda}}\left[\nabla_{\lambda} q_{\lambda}(z)\left(\log p(x, z)-\log q_{\lambda}(z)\right)\right]
\end{aligned}
\end{split}\]</div>
</div>
<hr class="docutils" />
<div class="section" id="id26">
<h3>5.3 使用重参数化梯度的 BBVI<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h3>
<p>依然采用上节中的模型，变分下界 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 为（为方便重复式 9）：</p>
<div class="math notranslate nohighlight">
\[
\mathcal{L}(\lambda) \triangleq \mathbb{E}_{q_{\lambda}(\mathbf{z})}[\log p(\mathbf{x}, \mathbf{z})-\log q(\mathbf{z} \mid \lambda)]
\]</div>
<p>假设变分分布可以表示成如下变换：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp;\epsilon \sim S(\epsilon)\\
&amp;\mathrm{z}= t(\epsilon, \lambda) 
\end{aligned} \Leftrightarrow \quad z \sim q(\mathbf{z} \mid \lambda)
\end{split}\]</div>
<p>例如：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
&amp;\epsilon \sim \mathcal{N}(0,1) \\
&amp;\mathrm{z}=\mu + \epsilon \cdot \sigma
\end{aligned} \Leftrightarrow \quad \mathrm{z} \sim \mathcal{N}\left(\mu, \sigma^{2}\right)
\end{split}\]</div>
<p>另外假设 <span class="math notranslate nohighlight">\(\log p(\mathbf{x},\mathbf{z})\)</span> 和 <span class="math notranslate nohighlight">\(\log q(\mathbf{z})\)</span> 关于 <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> 可微，则可以得到 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 关于 <span class="math notranslate nohighlight">\(\lambda\)</span> 的重参数化的梯度：</p>
<div class="math notranslate nohighlight">
\[
\nabla_{\lambda} \mathcal{L}=\mathbb{E}_{S(\epsilon)} \left[\nabla_{\mathbf{z}} \left [ (\log p(\mathbf{x}, \mathbf{z})-\log q(\mathbf{z} )\right ] \nabla_\lambda t(\epsilon,\lambda)\right] \tag{10}
\]</div>
<p>进而可以使用自动微分来获取梯度，但并不是所有的分布都可以重参数化。</p>
<blockquote>
<div><p>注解：</p>
<p>(1) 此部分最经典案例见 <a class="reference external" href="http://www.cs.columbia.edu/~blei/fogm/2018F/materials/KingmaWelling2013.pdf">变分自编码器</a></p>
<p>(2) 可微性是重参数化技巧使用的重要条件，这意味着其适用范围不如评分函数方法。</p>
</div></blockquote>
<p>BBVI 的方差减少：BBVI 需要一套与第 3.2 节中审查的 SVI 不同的方差减少技术。与 SVI 的噪声来自有限数据点集的二次采样不同，BBVI 噪声来自可能具有无限支持的随机变量。在这种情况下，诸如 SVRG 之类的技术不适用，因为完整梯度不是有限多项的总和，并且无法保存在内存中。因此，BBVI 涉及一组不同的控制变量和其他方法，这里将简要回顾一下从梯度估计器中减去得分函数的蒙特卡罗期望：</p>
</div>
<div class="section" id="id27">
<h3>5.4 方差减少策略<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h3>
<p>Variance Reduction for BBVI: BBVI requires a
different set of techniques for variance reduction than those reviewed for SVI in Section 3.2. In contrast to SVI where the noise resulted from subsampling from a finite set of data points, the BBVI noise originates
from random variables with possibly infinite support. In this case, techniques such as SVRG are not applicable, since the full gradient is not a sum over finitely many terms and cannot be kept in memory. Hence, BBVI involves a different set of control variates and other methods which shall briefly be reviewed here one subtracts a Monte Carlo expectation of the score function from the gradient estimator:</p>
<p>As required, the score function control variate has
expectation zero under the variational distribution. The
weight w is selected such that it minimizes the variance
of the gradient.
While the original BBVI paper introduces both
Rao-Blackwellization and control variates, [194] points
out that good choices for control variates might be
model-dependent. They further elaborate on local expectation gradients, which take only the Markov blanket of each variable into account. A different approach
is presented by [167], which introduces overdispersed
importance sampling. By sampling from a proposal distribution that belongs to an overdispersed exponential
family and that places high mass on the tails of the
variational distribution, the variance of the gradient is
reduced</p>
</div>
</div>
<div class="section" id="id28">
<h2>6 提升准确性 —  新的目标函数和结构化变分近似<a class="headerlink" href="#id28" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id29">
<h3>6.1 平均长变分的起源和局限性<a class="headerlink" href="#id29" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id30">
<h3>6.2 采用新的散度做变分推断<a class="headerlink" href="#id30" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id31">
<h3>6.3 结构化变分推断<a class="headerlink" href="#id31" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id32">
<h3>6.4 其他非标准的变分推断方法<a class="headerlink" href="#id32" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="id33">
<h2>7 摊销式变分推断与深度学习<a class="headerlink" href="#id33" title="Permalink to this headline">¶</a></h2>
<p>考虑第 <span class="math notranslate nohighlight">\(4\)</span> 节的设置，其中每个数据点 <span class="math notranslate nohighlight">\(x_i\)</span> 由其具有变分参数 <span class="math notranslate nohighlight">\(ξ_i\)</span> 的隐变量 <span class="math notranslate nohighlight">\(z_i\)</span> 控制。传统的变分推断需要为每个数据点 <span class="math notranslate nohighlight">\(x_i\)</span> 优化 <span class="math notranslate nohighlight">\(ξ_i\)</span>，这在计算上过于昂贵，特别是当这种优化嵌入到全局参数的更新循环时。摊销推断背后的基本思想是使用强大的预测器根据 <span class="math notranslate nohighlight">\(x_i\)</span> 的特征来预测最优 <span class="math notranslate nohighlight">\(z_i\)</span>，即 <span class="math notranslate nohighlight">\(z_i = f(x_i)\)</span> 。这样，局部变分参数就被数据的函数替换，而函数中的参数在所有数据点之间共享，即推断被摊销了。我们在 7.1 节详细介绍了这种方法背后的主要思想，并在 7.2 和 7.3 节中展示了如何以变分自动编码器的形式应用它。</p>
<div class="section" id="amortized-variational-families">
<h3>7.1 摊销变分推断（Amortized Variational Families）<a class="headerlink" href="#amortized-variational-families" title="Permalink to this headline">¶</a></h3>
<p>术语『摊销推断』指利用来自过去计算的推断来支持未来的计算 <code class="docutils literal notranslate"><span class="pre">[36]、[50]</span></code>。对于变分推断，摊销推断通常是指对局部变量的推断。与为每个数据点近似单独的隐变量不同，如图 2(a) 所示，摊销变分推断假设局部变分参数可以通过数据的函数进行预测。因此，一旦估计了该函数，就可以通过该函数传递新数据点来获取潜在变量，如图 2(b) 所示。这种情况下使用的深度神经网络也被称为推断网络。因此，具有推断网络的摊销变分推断将概率建模与深度学习的表示能力结合到了一起。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/stats-20211223113610-8a74.webp" /></p>
<blockquote>
<div><p>图 2. 随机变分推断的概率图表示 (a)，和变分自编码器 (b)，其中虚线线条表示对变分的近似</p>
</div></blockquote>
<p>例如，摊销推断已应用于<code class="docutils literal notranslate"><span class="pre">深度高斯过程</span> <span class="pre">(DGP)</span></code> <code class="docutils literal notranslate"><span class="pre">[35]</span></code>。 该模型中的推断难以处理，作者就将平均场变分推断与诱导点一起应用（参见 <code class="docutils literal notranslate"><span class="pre">[35]</span></code> 第 3.3 节)。进一步，<code class="docutils literal notranslate"><span class="pre">[34]</span></code> 建议将这些隐变量估计为推断网络的函数，而不是单独估计这些隐变量，从而允许深度高斯过程扩展到更大的数据集并加速收敛。另外，还可以通过将摊销误差反馈到推断模型中来迭代摊销 <code class="docutils literal notranslate"><span class="pre">[116]</span></code>。</p>
</div>
<div class="section" id="vae">
<h3>7.2 变分自编码器 （VAE）<a class="headerlink" href="#vae" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id34">
<h3>7.3 更灵活的 VAE<a class="headerlink" href="#id34" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id35">
<h4>7.3.1 标准化流 VAE<a class="headerlink" href="#id35" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id36">
<h4>7.3.2 重要性加权 VAE<a class="headerlink" href="#id36" title="Permalink to this headline">¶</a></h4>
</div>
</div>
<div class="section" id="id37">
<h3>7.4  结构化 VAE<a class="headerlink" href="#id37" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id38">
<h4>7.4.1 结构化 VAE<a class="headerlink" href="#id38" title="Permalink to this headline">¶</a></h4>
</div>
</div>
<div class="section" id="id39">
<h3>7.5 解决僵尸单元的问题<a class="headerlink" href="#id39" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id40">
<h4>7.5.1 有损 VAE<a class="headerlink" href="#id40" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
<div class="section" id="id41">
<h2>8 讨论<a class="headerlink" href="#id41" title="Permalink to this headline">¶</a></h2>
<p>下面是一些活跃的研究方向和开放性问题：</p>
<div class="section" id="id42">
<h3>8.1 变分推断理论方面<a class="headerlink" href="#id42" title="Permalink to this headline">¶</a></h3>
<p>尽管在建模和推断方面取得了进展，但很少有作者讨论 VI  的理论方面 [95]、[133]、[213]。一个重要方向是量化变分分布替换真实后验时的近似误差 [133]。与此相关的一个问题是预测误差，例如，使用 VI 近似来做贝叶斯预测分布的边缘化计算。我们还推测 VI 理论可以从与信息论的联系中受益。这已经在 [186]、[187] 中举例说明。信息论还激发了新模型和推断方案的发展 [2]、[13]、[193]。例如，信息瓶颈 [193] 最近推动了深度变分信息瓶颈 [2]。我们期望融合这两条研究线会产生更多有趣的结果。</p>
</div>
<div class="section" id="id43">
<h3>8.2 变分推断和深度学习<a class="headerlink" href="#id43" title="Permalink to this headline">¶</a></h3>
<p>尽管最近在各领域取得了成功，但深度学习仍然缺乏原则性的不确定性估计、缺乏其特征表示的可解释性，并且难以包含先验知识。贝叶斯方法（例如贝叶斯神经网络 [137] 和变分自动编码器）正在改进这些方面。最近的工作旨在使用可解释性概率模型作为 VAE 的先验 [38]、[77]、[91]、[168]。在此类模型中，变分推断是必不可少的组成部分。在贝叶斯深度架构中，如何使变分推断计算更为高效且易于实现，正在成为一个重要研究方向 [48]</p>
</div>
<div class="section" id="id44">
<h3>8.3 变分推断和策略梯度<a class="headerlink" href="#id44" title="Permalink to this headline">¶</a></h3>
<p>策略梯度估计对于强化学习 (RL)[183]​​ 和随机控制很重要。这些应用中的技术挑战与变分推断非常相似 [98]、[99]、[110]、[173]、[211] 。例如，SVGD 已作为 Steinpolicy 梯度被应用于 RL 设置 [110]。 变分推断在强化学习中的应用目前是一个活跃的研究领域。</p>
</div>
<div class="section" id="id45">
<h3>8.4 自动变分推断<a class="headerlink" href="#id45" title="Permalink to this headline">¶</a></h3>
<p>概率编程允许从业者快速实现和修改模型，而不必担心推断问题。用户只需要指定模型，推断引擎就会自动进行推断。流行的概率编程工具包括但不限于：Stan[28]，涵盖了大量的高级 VI 和 MCMC 推断方法； Net[126] 基于变分消息传递和 EP；Automatic Statistician[52] 和 Anglican[198] 主要依靠采样方法；Ed-ward[200] 支持 BBVI 和 MonteCarlo 采样 ； Zhusuan[176]的特点是用于贝叶斯深度学习的 VI 。这些工具的长期目标是改变概率建模的研究方法，使用户能够快速修改和改进模型，并使其他受众可以访问它们。</p>
<p>尽管目前努力使从业者更容易使用 VI，但对于非专家来说，其使用仍然不简单。例如，人工识别后验的对称性并打破这些对称性是 <a class="reference external" href="http://Infer.Net">Infer.Net</a> 所必需的。此外，诸如控制变量等减少方差的方法可以极大地加速收敛，但需要模型进行特定设计才能获得最佳性能。在撰写本文时，当前的概率编程工具箱尚未解决此类问题。我们相信这些方向对于推进概率建模在科学和技术中的影响非常重要。</p>
</div>
</div>
<div class="section" id="id46">
<h2>9 总结<a class="headerlink" href="#id46" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>概率机器学习是将领域知识与数据联系起来的机器学习方法</p></li>
<li><p>它提供了一种用于分析数据的计算方法</p></li>
<li><p>概率机器学习的最终目标是形成一种具有表现力、可扩展、易于开发的方法</p></li>
<li><p>后验推断的关键是算法问题</p></li>
<li><p>变分推断为后验推断提供了可扩展和通用方法</p></li>
<li><p>平均长近似和坐标上升方法是最为基础的变分推断方法</p></li>
<li><p>随机变分推断将 VI 扩展到海量数据</p></li>
<li><p>黑盒变分推断将 VI 泛化到多种模型</p></li>
</ul>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Append-01-MCMC_Tutorial.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">附录 A： MCMC 推断</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Append-03-GaussianProcessTutorial_01.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">附录 C： 高斯过程</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Osvaldo Martin<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>