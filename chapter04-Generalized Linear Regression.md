> 第**5**章　利用逻辑回归对结果进行分\
> 类
>
> 上一章中，我们学习了线性回归模型的核心内容，在这类模型\
> 中，我们假设被预测的变量是定量的（连续变量）。这一章我们将学
>
> 习如何处理定性的变量（或者称为类别变量），比如颜色、性别、生 物种类、政党等。注意有些变量既可以看作是定性的又可以看作是定 量的，比如红色和绿色，如果单看名字，它们是定性的，如果从它们 的波长来看，又是定量的（分别为650nm和510nm）。处理类型变量
>
> 时一个常见的问题是对于某个观测值赋予类别标签，这类问题称为分 类问题。分类问题属于有监督问题，因为我们已经有一些分类好的样 本，只需要对新的样本预测其正确的类别，同时学习模型的参数用于 描述特征到类别之间的映射关系。
>
> 本章将会讨论以下内容：
>
> 逻辑回归和逆连结函数；\
> 一元逻辑回归；\
> 多元逻辑回归；\
> softmax函数和多项逻辑回归。
>
> 188
>
> **5.1**　逻辑回归
>
> 我母亲有一道拿手好菜叫sopa seca，其做法源于意大利面，直译 过来叫做干汤面，这个名字听起来似乎有点矛盾，不过一旦了解它的
>
> 做法之后，你会觉得这个叫法其实挺合理的。同样，逻辑回归也是如 此，它主要用来解决分类问题。逻辑回归模型是线性回归模型的一种 扩展，这也是其名称来源。在理解如何将一个回归模型应用于分类问 题之前，先将线性模型的核心部分改写成如下形式：

![](C:/Program Files/Typora/media/image1646.png){width="1.2702012248468941in" height="0.187419072615923in"}

> 其中，*f*称作逆连结函数。为什么这里把*f*称作逆连结函数而不是 连结函数？原因是传统上人们认为此类函数是用来连结输出变量和线
>
> 性模型的，不过在构建贝叶斯模型的时候你将看到，反过来思考可能 更容易一些。因此，为了避免疑惑，我们这里统称逆连结函数。前一 章中的所有线性模型其实都包含一个逆连结函数，不过我们书写的时 候将其省略了，因为它其实是一个恒等函数（函数的返回值和输入值 相同）。恒等函数在这里也许没什么用，不过它可以让我们用一种更 一般的形式思考不同的模型。从原理上讲，许多其他函数都可以充当 逆连结函数，不过这一章只讨论逻辑函数，其数学形式如下：

![](C:/Program Files/Typora/media/image1647.png){width="2.1759995625546806in" height="0.4268996062992126in"}

> 从分类的角度来看，逻辑函数最重要的特点之一是不论参数*z*的\
> 值为多少，其输出值总是介于0到1之间。因此，该函数将整个实轴压
>
> 缩到了区间\[0,1\]内。逻辑函数也称作**S**型函数（sigmoid function），\
> 因为它的形状看起来像S，可以运行以下几行代码来看一下。
>
> ![](C:/Program Files/Typora/media/image1648.png){width="3.818897637795276e-2in" height="0.41302821522309713in"}z = np.linspace(-10, 10, 100)
>
> 189 ![](C:/Program Files/Typora/media/image1651.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}
>
> ![](C:/Program Files/Typora/media/image1653.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}logistic = 1 / (1 + np.exp(-z))\
> plt.plot(z, logistic)
>
> plt.xlabel(\'\$z\$\', fontsize=18)\
> plt.ylabel(\'\$logistic(z)\$\', fontsize=18)

![](C:/Program Files/Typora/media/image1657.png){width="5.643024934383202in" height="3.883753280839895in"}

> **5.1.1**　逻辑回归模型
>
> 现在我们知道逻辑函数长什么样了，接下来将继续学习如何用来 对结果进行分类。先从简单的问题开始，假设类别只有两类，比如正 常邮件/垃圾邮件、安全/不安全、阴天/晴天、健康/生病等。首先对
>
> ![](C:/Program Files/Typora/media/image1658.png){width="0.7288035870516185in" height="0.1770067804024497in"}类别进行编码，假设变量*y*只能有两个值，0或1，也就是说 。 这样描述之后，问题就有点像抛硬币问题了，在那个例子中我们用到 了伯努利分布作为似然。这里的区别是：现在*q*不再是从一个beta分
>
> 布中生成的，而是由一个线性模型定义的。一个线性模型可以返回实 轴上的任意值，但是伯努利分布的值限定在\[0,1\]区间内。因此我们使 用了一个逆连结函数将线性模型的返回值映射到一个适合伯努利分布 的区间内，从而将一个线性回归模型转换成了分类模型：
>
> 190

![](C:/Program Files/Typora/media/image1660.png){width="1.7387171916010498in" height="0.4164873140857393in"}

> 下面的Kruschke图显示了逻辑回归模型（应该）包含的先验。注 意与单参数线性回归模型的区别是：这里用到了伯努利分布而不是高 斯分布（或者t分布），并使用了逻辑函数生成区间\[0,1\]范围内的参
>
> 数*θ*，从而适于输入到伯努利分布。

![](C:/Program Files/Typora/media/image1661.png){width="3.768956692913386in" height="5.25816491688539in"}

> **5.1.2**　鸢尾花数据集
>
> 这里我们将逻辑回归应用到鸢尾花数据集上。在构建模型之前， 我们先了解下该数据集。鸢尾花数据集是一个很经典的数据集，包含 有Setosa、Versicolour和Virginica 3个种类，这3个类别标签就是我们
>
> 191
>
> 想要预测的量，即因变量。其中每个种类包含有50个数据，每个数据 包含4种变量（或者称为特征，机器学习中通常这么称呼），这4种变 量就是我们要分析的自变量，分别是：花瓣长度、花瓣宽度、花萼长 度和花萼宽度。花萼有点类似小叶，在花还是芽时包围着花，有保护 作用。seaborn中包含鸢尾花数据集，我们可以用如下代码将其导入 成Pandas里的Dataframe格式：
>
> iris = sns.load_dataset(\"iris\") iris.head()

+-----+----------------+-------------+--------------+-------------+---------+
|     | > sepal_length | sepal_width | petal_length | petal_width | species |
+=====+================+=============+==============+=============+=========+
| > 0 | 5.1            | 3.5         | 1.4          | 0.2         | setosa  |
+-----+----------------+-------------+--------------+-------------+---------+
| > 1 | 4.9            | 3.0         | 1.4          | 0.2         | setosa  |
+-----+----------------+-------------+--------------+-------------+---------+
| > 2 | 4.7            | 3.2         | 1.3          | 0.2         | setosa  |
+-----+----------------+-------------+--------------+-------------+---------+
| > 3 | 4.6            | 3.1         | 1.5          | 0.2         | setosa  |
+-----+----------------+-------------+--------------+-------------+---------+
| > 4 | 5.0            | 3.6         | 1.4          | 0.2         | setosa  |
+-----+----------------+-------------+--------------+-------------+---------+

> 可以用seaborn中的stripplot函数将每个种类的花萼长度画出
>
> 来：
>
> sns.stripplot(x=\"species\", y=\"sepal_length\", data=iris, jitter=True)
>
> 192

![](C:/Program Files/Typora/media/image1672.png){width="5.643024934383202in" height="3.967050524934383in"}

> 上图中*y*轴是连续的，而*x*轴是离散的类别变量；图中的点在*x*轴\
> 上是分散开来的，这并没有什么实际意义，只是一个画图的小技巧而
>
> 已，通过将jitter参数设为True，能够避免所有点都重叠在一条直 线上，你可以试着将jitter参数设为False。这里唯一重要的是*x*轴 的含义，即分别代表setosa、versicolour和virginica 3个类别。你还可\
> 以用seaborn中的其他作图函数来画这些点（比如violin plot），
>
> 只需要一行代码就能完成。
>
> 另外一种观察数据的方法是用pairplot画出散点图矩阵，用该\
> 函数可以得到一个 4×4的网格（因为我们有4种特征）。网格是对称 的，上三角和下三角表示的是同样的信息。由于对角线上的散点图其
>
> 实是变量本身，因此这里用一个特征的KDE图代替了散点图。可以看 到，每个子图中，分别用3种颜色表示3种不同的类别标签，这一点与 前面图中的表示一致。
>
> sns.pairplot(iris, hue=\'species\', diag_kind=\'kde\')
>
> 193

![](C:/Program Files/Typora/media/image1678.png){width="5.643024934383202in" height="4.977035214348207in"}

> 深入学习之前，花点时间分析下前面这幅图，进一步熟悉这个数 据集并了解变量与标签之间的关系。
>
> **5.1.3**　将逻辑回归模型应用到鸢尾花数据集
>
> 我们先从一个简单的问题开始：用花萼长度这一特征（自变量） 来区分setosa 和 versicolor这两个种类。和前面一样，这里用0和1对类 别变量进行编码，利用Pandas可以这么做：
>
> df = iris.query(species == (\'setosa\', \'versicolor\')) y_0 = pd.Categorical(df\[\'species\'\]).codes
>
> x_n = \'sepal_length\'
>
> x_0 = df\[x_n\].values
>
> 现在数据已经表示成了合适的格式，终于可以用PyMC3来建模
>
> 194
>
> 了。留意下面代码中的第一部分与线性回归模型的相似之处。此外还 留意两个确定变量：theta和bd。theta是对变量mu应用逻辑函数之 后的值，bd是一个有边界的值，用于确定分类结果，稍后会详细讨
>
> 论。还有一点值得提的是除了像下面这样明确写出逻辑函数的完整形 式之外，还可以简单地使用PyMC3中的pm.math.sigmoid函数，该\
> 函数是Theano中sigmoid函数的一个别名。
>
> 对于其他线性模型而言，将数据进行中心化和（或）标准化处理 会有利于采样。不过，在这个例子中，我们暂且不对数据做进一步处 理。
>
> with pm.Model() as model_0:
>
> alpha = pm.Normal(\'alpha\', mu=0, sd=10) beta = pm.Normal(\'beta\', mu=0, sd=10)
>
> mu = alpha + pm.dot(x_0, beta)
>
> theta = pm.Deterministic(\'theta\', 1 / (1 + pm.math.exp(-mu))) bd = pm.Deterministic(\'bd\', -alpha/beta)
>
> yl = pm.Bernoulli(\'yl\', theta, observed=y_0) start = pm.find_MAP()
>
> step = pm.NUTS()
>
> trace_0 = pm.sample(5000, step, start)\
> chain_0 = trace_0\[1000:\]
>
> varnames = \[\'alpha\', \'beta\', \'bd\'\]\
> pm.traceplot(chain_0, varnames)

![](C:/Program Files/Typora/media/image1688.png){width="5.643024934383202in" height="2.7279975940507435in"}

> 195
>
> 和往常一样，我们将后验的总结打印出来，然后将bd与通过另外
>
> 一种方式得到的值进行比较。
>
> pm.df_summary(trace_0, varnames)

+---------+--------+------+----------+---------+----------+
|         | > mean | sd   | mc_error | hpd_2.5 | hpd_97.5 |
+=========+========+======+==========+=========+==========+
| > alpha | -23.49 | 4.07 | 1.77e-01 | -31.38  | -15.72   |
+---------+--------+------+----------+---------+----------+
| > beta  | 4.34   | 0.75 | 3.28e-02 | 2.88    | 5.75     |
+---------+--------+------+----------+---------+----------+
| > bd    | 5.42   | 0.07 | 1.09e-03 | 5.27    | 5.55     |
+---------+--------+------+----------+---------+----------+

> 现在，我们将数据及拟合的sigmoid（S-型）曲线画出来。
>
> theta = trace_0\[\'theta\'\].mean(axis=0)
>
> idx = np.argsort(x_0)
>
> plt.plot(x_0\[idx\], theta\[idx\], color=\'b\', lw=3);\
> plt.axvline(trace_0\[\'bd\'\].mean(), ymax=1, color=\'\'r\'\')
>
> bd_hpd = pm.hpd(trace_0\[\'bd\'\])
>
> plt.fill_betweenx(\[0, 1\], bd_hpd\[0\], bd_hpd\[1\], color=\'r\', alpha=0.5)
>
> plt.plot(x_0, y_0, \'o\', color=\'k\')
>
> theta_hpd = pm.hpd(trace_0\[\'theta\'\])\[idx\]
>
> plt.fill_between(x_0\[idx\], theta_hpd\[:,0\], theta_hpd\[:,1\], color=\'b\', a lpha=0.5)
>
> plt.xlabel(x_n, fontsize=16)\
> plt.ylabel(r\'\$\\theta\$\', rotation=0, fontsize=16)
>
> 196

![](C:/Program Files/Typora/media/image1699.png){width="5.643024934383202in" height="4.01911198600175in"}

> 前面这张图表示了花萼长度与花的种类（setosa = 0, versicolor\
> =1）之间的关系。蓝色的S型曲线表示theta的均值，这条线可以解释
>
> ![](C:/Program Files/Typora/media/image1700.png){width="0.8433300524934383in" height="0.187419072615923in"}为：在知道花萼长度的情况下花的种类是versicolor的概率，即\
> 。半透明的蓝色区间是95%HPD区间。注意：在这种情况
>
> ![](C:/Program Files/Typora/media/image1701.png){width="0.23946412948381451in" height="0.22906824146981627in"}下，逻辑回归其实就是回归，因为我们做的就是在给定特征（或者特 征的线性组合）的条件下，回归出一个数据点的类别为1的概率。不 过需要时刻注意观察到的变量是二值变量而推断的是连续的概率，因 此我们需要引入一个规则将连续的概率转换成一个0-1结果。决策边\
> 界在图中用红色来表示，此外还有一个95%HPD区间。根据决策边\
> 界，*x*值位于其左侧的（这里对应花萼长度）属于类别0（setosa），\
> 而位于其右侧的属于类别1（versicolor）。决策边界在*x*轴上的取值为 对应*y*=0.5时的点，可以证明其结果是，推导过程如下。
>
> 根据模型的定义，我们有如下关系：

![](C:/Program Files/Typora/media/image1702.png){width="1.8324212598425196in" height="0.187419072615923in"}

> 197
>
> ![](C:/Program Files/Typora/media/image1704.png){width="0.5413965441819772in" height="0.13535761154855644in"}根据逻辑函数的定义，当 时，对应的输入为0，则有：

![](C:/Program Files/Typora/media/image1705.png){width="3.144268372703412in" height="0.187419072615923in"}

> ![](C:/Program Files/Typora/media/image1704.png){width="0.5413965441819772in" height="0.13535761154855644in"}移项后可以得出，当 时，对应有：

![](C:/Program Files/Typora/media/image1707.png){width="0.7079800962379702in" height="0.38525153105861765in"}

> 值得一提的是：决策边界是一个标量，也就是说是一个数值，这 一点对于一维数据来说是合理的。因此我们只需要一个标量就可以将 数据分成两组（或者两类）。很重要的一点是：尽管决策边界看起来 很合理，但是这里选取的0.5并不是什么特殊值，你完全可以选其他0 到1之间的值。只有当我们认为将标签0（setosa）错误地标为标签
>
> 1（versicolor）时的代价，与反过来将标签1（versicolor）错误地标为 标签0（setosa）时的代价相同时，选取0.5作为决策边界才是可行\
> 的。不过大多数情况下，分类出错的代价并不是对称的。
>
> 做预测
>
> 一旦有了*α*和*β*之后，我们就能用其对新的数据进行分类。可以创 建一个非常基础的函数，接受花萼长度作为参数，返回分类结果为 versicolor的概率值。具体代码如下所示：
>
> def classify(n, threshold):
>
> \"\"\"
>
> A simple classifying function
>
> \"\"\"
>
> n = np.array(n)
>
> mu = trace_0\[\'alpha\'\].mean() + trace_0\[\'beta\'\].mean() \* n prob = 1 / (1 + np.exp(-mu))
>
> return prob, prob \> threshold
>
> classify(\[5, 5.5, 6\], 0.4)
>
> 从前面的例子可以看出，很难只根据花萼长度就将setosa和\
> versicolor这两类花区分出来。事实上如果你仔细看过joinplot画的
>
> 198
>
> 图，就可以很清楚地观察到这点。仔细观察数据可以发现，类别为\
> versicolor的花的花萼长度最小值约为4.9，而类别为setosa的花的花萼 长度最大值约为5.8。换句话说，两种花的花萼长度在4.9到5.8的范围 内有重叠。
>
> 如果只使用另外一种变量呢？请查看本章练习中的第一题，我们 将在那里探讨这个问题。
>
> 199
>
> **5.2**　多元逻辑回归
>
> 与多变量线性回归类似，多变量逻辑回归使用了多个自变量。这 里将花萼长度与花萼宽度结合在一起，注意这里需要对数据做一些预 处理。
>
> df = iris.query(species == (\'setosa\', \'versicolor\'))\
> y_1 = pd.Categorical(df\[\'species\'\]).codes
>
> x_n = \[\'sepal_length\', \'sepal_width\'\]
>
> x_1 = df\[x_n\].values
>
> **5.2.1**　决策边界
>
> 如果你对如何推导决策边界不感兴趣的话，可以略过这个部分直 接跳到模型实现部分。 根据模型，我们有：

![](C:/Program Files/Typora/media/image1719.png){width="2.4154647856517935in" height="0.187419072615923in"}

> ![](C:/Program Files/Typora/media/image1704.png){width="0.5413965441819772in" height="0.13535761154855644in"}根据逻辑函数的定义，当逻辑回归的参数为0时，我们有 ，也就是说：

![](C:/Program Files/Typora/media/image1721.png){width="4.497761373578303in" height="0.187419072615923in"}

> ![](C:/Program Files/Typora/media/image1704.png){width="0.5413965441819772in" height="0.13535761154855644in"}移项之后可以得出 ，当

![](C:/Program Files/Typora/media/image1723.png){width="1.8220089676290463in" height="0.46854877515310583in"}

> 这个决策边界的表达式与直线的表达式在数学形式上是一样的， 其中第1项表示截距，第2项表示斜率，这里的括号只是为了表达上更 清晰，如果你愿意的话完全可以去掉。为什么决策边界是直线呢？想 想看，如果我们有一个特征，还有一维的数据，可以用一个点将数据 分成两组；如果有两个特征，也就有一个2维的数据空间，从而我们
>
> 200
>
> 可以用一条直线来对其分割；对于3维的情况，边界是一个平面，对 于更高的维度，我们对应有一个超平面。事实上，从概念上讲，超平 面可以大致定义为*n*维空间中*n*-1维的子空间，因此我们总是可以将决 策边界称为超平面。
>
> **5.2.2**　模型实现
>
> 如果要用PyMC3写出多元逻辑回归模型，可以借助其向量化表\
> 示的优势，只需要对前面的单参数逻辑回归模型做一些简单的修改即
>
> 可。
>
> with pm.Model() as model_1:
>
> alpha = pm.Normal(\'alpha\', mu=0, sd=10)
>
> beta = pm.Normal(\'beta\', mu=0, sd=2, shape=len(x_n))
>
> mu = alpha + pm.math.dot(x_1, beta)
>
> theta = 1 / (1 + pm.math.exp(-mu))
>
> bd = pm.Deterministic(\'bd\', -alpha/beta\[1\] - beta\[0\]/beta\[1\] \* x_1\[ :,0\])
>
> yl = pm.Bernoulli(\'yl\', p=theta, observed=y_1)
>
> trace_1 = pm.sample(5000) chain_1 = trace_1\[100:\]\
> varnames = \[\'alpha\', \'beta\'\] pm.traceplot(chain_1)

![](C:/Program Files/Typora/media/image1729.png){width="5.643024934383202in" height="2.7279975940507435in"}

> 201
>
> 注意在前面的图中，与前一个例子的区别是我们得到的不再是一 个单独的决策边界的曲线，而是得到了100个，每个曲线对应一个数 据点。
>
> 正如我们对单个预测变量所做的那样，可以将数据以及决策边界 画出来，下面的代码中没有画sigmoid（现在是一个2D的曲面）。如 果愿意的话，你可以画出3D的图来表示sigmoid曲面。
>
> idx = np.argsort(x_1\[:,0\])
>
> bd = chain_1\[\'bd\'\].mean(0)\[idx\]\
> plt.scatter(x_1\[:,0\], x_1\[:,1\], c=y_0) plt.plot(x_1\[:,0\]\[idx\], bd, color=\'r\');
>
> bd_hpd = pm.hpd(chain_1\[\'bd\'\])\[idx\]
>
> plt.fill_between(x_1\[:,0\]\[idx\], bd_hpd\[:,0\], bd_hpd\[:,1\], color=\'r\', al pha=0.5);
>
> plt.xlabel(x_n\[0\], fontsize=16) plt.ylabel(x_n\[1\], fontsize=16)
>
> 我们已经看到过的，决策边界现在是一条直线，不要被95%HPD 区间的曲线给误导了。图中半透明的曲线是由于在中间部分（也就是 HPD区间较窄的部分）有多条直线造成的。
>
> 202

![](C:/Program Files/Typora/media/image1736.png){width="5.643024934383202in" height="4.071172353455818in"}

> **5.2.3**　处理相关变量
>
> 前一章中我们了解了在处理高度相关的变量时可能会遇到一些奇 怪的问题。例如，如果用花瓣长度和花瓣宽度作为特征重跑前面的模 型，会得到什么样的结果呢？
>
> 如果完成了上面的练习，你可能会注意到beta系数要比以前分布 得更广，而且95%HPD区间（红色的区间）也更宽。下面的热力图显 示了4个变量之间的相关性，可以看到（前一个例子中用到的）花萼 长度与花萼宽度之间的相关性要远低于（后一个例子中用到的）花瓣
>
> 长度与花瓣宽度之间的相关性。我们知道，相关的变量会得到更广的 系数组合，因而能更好地解释数据，或者从另外一个角度来说，相关 的变量限制模型的能力更弱。
>
> 当不同类别的数据能被完美区分时（即用线性模型分隔之后不同\
> 类的数据没有重叠），类似的问题就会出现。一个解决办法是不使用
>
> 203
>
> 相关的变量，不过这个办法可能有时候并不适用。另外一个办法是给 先验增加更多的信息，如果我们掌握了一些有用的信息，可以使用一 些携带信息的先验，或者更一般地，可以使用一些弱信息的先验。
>
> Andrew Gelman和Stan的开发团队建议在进行逻辑回归时使用如下先\
> 验：

![](C:/Program Files/Typora/media/image1738.png){width="1.7595406824146982in" height="0.187419072615923in"}

> 这里*s*的取值可以根据期望的尺度引入弱信息，正态参数*v*的值为 3到7附近。该先验的含义是：我们期望系数比较小，同时引入了重 尾，从而得到一个比高斯分布更鲁棒的模型。如果你忘记了的话，可 以回忆一下第3章和第4章中的内容。
>
> corr = iris\[iris\[\'species\'\] != \'virginica\'\].corr() mask = np.tri(\*corr.shape).T\
> sns.heatmap(corr.abs(), mask=mask, annot=True)

![](C:/Program Files/Typora/media/image1743.png){width="5.643024934383202in" height="4.060761154855643in"}

> 前面的图中，使用了一个掩码操作去掉了热力图中的上三角和对
>
> 204
>
> 角线上的元素，因为这部分提供的是无效信息或者冗余信息。同时还 需要注意，这里打印的是相关性的绝对值，原因是这里我们只关心相 关性的强弱而不关心是正相关还是负相关。
>
> **5.2.4**　处理类别不平衡数据
>
> 鸢尾花数据集的一个优点是：其中不同类别的样本量是均衡的， setosas、versicolors和virginicas各有50个。鸢尾花数据集的流行归功 于Fisher，当然，Fisher还让另一个东西也流行了------*p*值。实际使用 中许多数据集中的类别都是不平衡的，也就是说，其中一类数据的数 量要远多于其他类别的数据。当这种情况发生时，逻辑回归会遇到一 些问题，相比数据平衡时的情况，逻辑回归得到的边界没有那么准确 了。
>
> 现在我们看个实际的例子，这里我们随机从setosa类别中去掉一 些数据点。
>
> df = iris.query(species == (\'setosa\', \'versicolor\')) df = df\[45:\]
>
> y_3 = pd.Categorical(df\[\'species\'\]).codes
>
> x_n = \[\'sepal_length\', \'sepal_width\'\]
>
> x_3 = df\[x_n\].values
>
> 和之前一样，你可以在自己电脑上运行多元逻辑回归，这里我直 接将结果表示出来。
>
> ![](C:/Program Files/Typora/media/image1749.png){width="3.8190069991251095e-2in" height="2.0789818460192477in"}idx = np.argsort(x_3\[:,0\])
>
> bd = trace_3\[\'bd\'\].mean(0)\[idx\]\
> plt.scatter(x_3\[:,0\], x_3\[:,1\], c=y_3) plt.plot(x_3\[:,0\]\[idx\], bd, color=\'r\');
>
> bd_hpd = pm.hpd(trace_3\[\'bd\'\])\[idx\]
>
> plt.fill_between(x_3\[:,0\]\[idx\], bd_hpd\[:,0\], bd_hpd\[:,1\], color=\'r\', al pha=0.5);
>
> plt.xlabel(x_n\[0\], fontsize=16)
>
> ![](C:/Program Files/Typora/media/image1752.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}205
>
> ![](C:/Program Files/Typora/media/image1754.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}plt.ylabel(x_n\[1\], fontsize=16)

![](C:/Program Files/Typora/media/image1758.png){width="5.643024934383202in" height="3.98787510936133in"}

> 可以看到，决策边界向样本量更少的类别偏移了，而且不确定性 也比以前更大了。这是逻辑回归在处理不均衡数据时的常见表现。在 一些数据中，类别之间的间隔可能不像这个例子中这么完美，此时用 逻辑回归分类得到的结果中类别重叠的现象更严重。不过你可能觉得 不确定性变得更大有可能是因为数据总量变少了，而不只是因为
>
> setosas类别的数据相比versicolors更少。这是有可能的，你可以完成\
> 练习部分的第2题之后，亲自验证为什么不确定性变大的原因是数据 不平衡。
>
> **5.2.5**　如何解决类别不平衡的问题
>
> 一个显而易见的解决方案是，对数据集中的每一类都获取几乎相 同数量的样本，如果你自己收集或者生成数据的话一定要记住这点。 如果你并不能控制数据集，那么在对类别不平衡的数据进行解释时可
>
> 206
>
> 要当心了，你可以通过检查模型的不确定性以及运行后验预测检查来 确定模型是否对你有用。另外一种做法是给数据加入更多的先验信\
> 息，如果可能的话，可以运行本章剩余部分提到的一些其他模型。
>
> **5.2.6**　解释逻辑回归的系数
>
> 解释逻辑回归的系数时一定要非常小心，因为逆连结函数（逻辑 函数）引入了非线性。

![](C:/Program Files/Typora/media/image1760.png){width="1.7491294838145233in" height="0.187419072615923in"}

> 逻辑函数的逆函数是logit函数，其形式为：

![](C:/Program Files/Typora/media/image1761.png){width="1.8740671478565178in" height="0.46854877515310583in"}

> 因此，根据第一个等式，我们有：

![](C:/Program Files/Typora/media/image1762.png){width="1.5408989501312336in" height="0.187419072615923in"}

> 即

![](C:/Program Files/Typora/media/image1763.png){width="1.9053018372703412in" height="0.46854877515310583in"}

> 注意这里模型中的*θ*是*y*=1的概率值，因此：

![](C:/Program Files/Typora/media/image1764.png){width="2.51957895888014in" height="0.46854877515310583in"}

> ![](C:/Program Files/Typora/media/image1765.png){width="0.5934547244094488in" height="0.30195319335083115in"}其中称作发生比，是另一种表示概率的方式。比如，掷骰 子得到
>
> 点数2的概率为1/6，因而其发生比为1:5（即0.2），也就是说有1 个期望发生的事件和5个不期望发生的事件。
>
> 207
>
> 回到逻辑回归上，系数*β*的意义是：当*x*增加单位量的时候，发生 比的对数增量。需要注意的是，*β*并不是指当*x*增加时*p*(*y*=1)的增量， 因为二者之间的关系并不是线性的。如果*β*是正数，那么增加*x*会在某 种程度上增加*p*(*y*=1)，但是具体的增量取决于当前*x*的值。这一点在
>
> 前面画的S型曲线上可以看出来，*y*对*x*的斜率随着*x*的变化而变化，但 是发生比的对数相对于*x*的斜率则是线性的。
>
> **5.2.7**　广义线性模型
>
> 现在对本章所学的内容总结下，分析一下本章内容是如何与前一 章中线性回归模型联系起来的。我们所做的就是将模型扩展应用到类 别变量，具体做法是引入了逆连结函数，并且将高斯分布替换成了另 外一种分布（伯努利分布）。总的来说就是：通过改变似然、先验及 逆连结函数，我们把前一章中的线性回归模型应用到了不同的数据/
>
> 问题上。
>
> 逻辑回归模型并非线性回归模型的唯一扩展。事实上，有一系列 模型都可以看作是线性回归模型的一般形式，通常称为广义线性模 型。统计学中一些常用的广义线性模型有：
>
> softmax回归（下一章会见到），将逻辑回归应用到多于两个类\
> 别的分类问题。
>
> 方差分析（ANalysis Of VAriance，ANOVA），其中有一个连续 的因量和两个以上的离散的自变量。ANOVA模型主要用来比较 不同组之间的相似程度，该方法用的是线性回归模型。\
> 泊松回归，我们将在第7章学习泊松回归模型的一个变种。
>
> 本书没有包含广义线性模型这部分内容，如果你想深入学习，特 别是ANOVA模型，我强烈推荐John Kruschke的《Doing Bayesian
>
> 208
>
> Data Analysis》一书，书中针对如何构建基于广义线性模型的贝叶斯 模型有很详细的介绍。
>
> **5.2.8**　**Softmax**回归或多项逻辑回归
>
> 现在我们知道了如何处理二分类问题，接下来将我们所学的内容 推广到多分类问题。一种做法是使用多项逻辑回归，该模型也被称作 softmax回归，原因是这里使用的是softmax函数而非逻辑函数，
>
> softmax函数的形式如下：

![](C:/Program Files/Typora/media/image1771.png){width="2.2488801399825022in" height="0.45813648293963255in"}

> 要计算向量***μ***中第*i*个元素对应的softmax输出，需要将该元素的指 数除以向量***μ***中每个元素的指数之和。softmax函数保证了输出值为正 数而且和为1。当*k*=2时，softmax函数就变成了逻辑函数。另外， softmax函数与统计学中的玻尔兹曼分布形式是一样的，玻尔兹曼分 布也是物理学中用来描述分子系统中概率分布的一个强大分支。
>
> 在玻尔兹曼分布中（某些领域中的softmax）有一个称为温度的\
> 参数（*T*），在数学形式上前面式子中的***μ***变成了***μ***/*T*，当*T*→∞时，概
>
> 率分布变得非常均匀，因而所有状态都是等可能的；当*T*→0时，只有 最可能的状态会输出，因而softmax表现得就像一个max函数，这也是 其名字来源。
>
> softmax回归模型与逻辑回归模型的另一个区别是：伯努利分布\
> 换成了类别分布。类别分布其实是伯努利分布推广到两个以上输出时
>
> 的一般形式。此外，伯努利分布（抛一次硬币）是二项分布（抛多次 硬币）的特殊情况，类似地，类别分布（掷一次骰子）是多项分布\
> （掷*N*次骰子）的特殊情况。
>
> 209

![](C:/Program Files/Typora/media/image1773.png){width="3.768956692913386in" height="5.268577209098862in"}

> 这里继续使用鸢尾花数据集，不过这次用到其中的3个类别标签 （setosa、versicolor及virginica）和4个特征（花萼长度、花萼宽度、 花瓣长度及花瓣宽度），同时对数据进行标准化处理（也还可以做中
>
> 心化处理），这样采样效率更高。
>
> iris = sns.load_dataset(\'iris\')
>
> y_s = pd.Categorical(iris\[\'species\'\]).codes\
> x_n = iris.columns\[:-1\]
>
> x_s = iris\[x_n\].values
>
> x_s = (x_s -- x_s.mean(axis=0))/x_s.std(axis=0)
>
> 从PyMC3的代码可以看出，逻辑回归模型与softmax模型之间的 变化很小，留意alpha系数和beta系数的长度。这段代码中用到了 Theano中的softmax函数，根据PyMC3开发者的惯例，按import
>
> 210
>
> theano.tensor as tt这种方式导入的Theano。
>
> with pm.Model() as model_s:
>
> alpha = pm.Normal(\'alpha\', mu=0, sd=2, shape=3)\
> beta = pm.Normal(\'beta\', mu=0, sd=2, shape=(4,3))
>
> mu = alpha + pm.dot(x_s, beta)
>
> theta = tt.nnet.softmax(mu)
>
> yl = pm.Categorical(\'yl\', p=theta, observed=y_s) start = pm.find_MAP()
>
> step = pm.NUTS()
>
> trace_s = pm.sample(2000, step, start)\
> pm.traceplot(trace_s)

![](C:/Program Files/Typora/media/image1783.png){width="5.643024934383202in" height="1.7804866579177603in"}

> 那么我们的模型表现如何呢？可以根据准确预测的样本个数来判 断。下面的代码中使用了参数的均值来计算每个点分别属于3个类别 的概率值，然后使用argmax函数求出概率最大的类别作为结果，最
>
> 后将结果与观测值进行比较。
>
> data_pred = trace_s\[\'alpha\'\].mean(axis=0) + np.dot(x_s, trace_s\[\'beta\'\] .mean(axis=0))
>
> y_pred = \[\]
>
> for point in data_pred:\
> y_pred.append(np.exp(point)/np.sum(np.exp(point), axis=0))\
> np.sum(y_s == np.argmax(y_pred, axis=1))/len(y_s)
>
> 分类结果显示准确率约为98%，也就是说，只错分了3个样本。\
> 不过，真正要评估模型的效果需要使用模型没有见过的数据，否则，
>
> 可能会高估了模型对其他数据的泛化能力。下一章我们会详细讨论这
>
> 211
>
> 个主题，目前暂且把它当做自动一致性检查，证明我们的模型运行正 常。
>
> 也许你已经注意到了，后验（或者更准确地说，每个参数的边缘 分布）看起来分布得很宽；事实上，它们与先验分布得一样宽。尽管 我们能做出正确的预测，但这看起来并不令人满意。在前面的线性/
>
> 逻辑回归问题中，对于有相关性的数据或者可以完美分割的数据，我 们也遇到过类似不可识别的问题。在这个例子中，后验分布较广是因 为受到了所有概率之和为1的限制。在这种情况下，我们用到的参数
>
> 个数比实际定义模型所需要的参数个数更多。简单来说就是，假如10 个数的和为1，你只需要知道9个数就可以了，剩下的1个数可以用1减 去这9个数之和算出来。解决这个问题的办法是将额外的参数固定为
>
> 某个值（比如0）。下面的代码展示了如何用PyMC3来实现。
>
> with pm.Model() as model_sf:
>
> alpha = pm.Normal(\'alpha\', mu=0, sd=2, shape=2)\
> beta = pm.Normal(\'beta\', mu=0, sd=2, shape=(4,2))
>
> alpha_f = tt.concatenate(\[\[0\] , alpha\])
>
> beta_f = tt.concatenate(\[np.zeros((4,1)) , beta\], axis=1)
>
> mu = alpha_f + pm.math.dot(x_s, beta_f) theta = tt.nnet.softmax(mu)
>
> yl = pm.Categorical(\'yl\', p=theta, observed=y_s) start = pm.find_MAP()
>
> step = pm.NUTS()
>
> trace_sf = pm.sample(5000, step, start)

![](C:/Program Files/Typora/media/image1793.png){width="5.643024934383202in" height="1.7804866579177603in"}

> 212
>
> **5.3**　判别式和生成式模型
>
> 目前为止，我们已经讨论了逻辑回归及其扩展，所有这些情况都 是直接计算 *p*(*y* \| *x*)，也就是说，在知道*x*的条件下，计算出类别*y*的概 率值。换句话说，我们所做的是直接根据自变量到因变量之间的关系 进行建模，然后用一个阈值对得到的（连续的）概率值进行评判，从 而得到分类结果。
>
> 上面这种方法不是唯一的，另一种方法是先对*p*(*x* \| *y*)建模，即类 别计算特征的分布，然后再进行分类，这类模型称为生成式分类器，
>
> 因为我们得到的模型可以从每个类别中生成采样。与此相反，逻辑回 归属于判别式分类器，因为它只能判断一个样本是不是属于某一类\
> 别，并不能从每个类别中生成样本。
>
> 这里我们不打算深入生成式分类器模型，不过可以通过一个例子 来说明这类模型用于分类的核心思想。我们只使用两个类别和一个特 征，与本章的第一个例子用到的数据一样。
>
> 下面的代码用PyMC3实现了一个生成式分类器，从代码中可以\
> 看出，现在决策边界变成了高斯分布期望的估计值的均值，当分布是
>
> 正态分布且标准差相同时，这个决策边界是正确的。这些假设是由一 种称作线性判别分析（Linear Discriminant Analysis，LDA）的模型做 出的，尽管名字上是判别式分析，不过LDA模型其实是生成式的。
>
> ![](C:/Program Files/Typora/media/image1796.png){width="3.818897637795276e-2in" height="1.589608486439195in"}with pm.Model() as lda:
>
> mus = pm.Normal(\'mus\', mu=0, sd=10, shape=2) sigmas = pm.Uniform(\'sigmas\', 0, 10)
>
> setosa = pm.Normal(\'setosa\', mu=mus\[0\], sd=sigmas\[0\], observed=x_0\[ :50\])
>
> 213
>
> ![](C:/Program Files/Typora/media/image1800.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"} versicolor = pm.Normal(\'setosa\', mu=mus\[1\], sd=sigmas\[1\], observed= x_0\[50:\])
>
> bd = pm.Deterministic(\'bd\', (mus\[0\]+mus\[1\])/2)
>
> start = pm.find_MAP()
>
> step = pm.NUTS()
>
> trace = pm.sample(5000, step, start)

![](C:/Program Files/Typora/media/image1804.png){width="5.643024934383202in" height="2.707173009623797in"}

> 下面再将setosa=0和versicolor=1两个类别与花萼长度的关系 画出来，一同画出来的还有一条红色的决策边界以及对应的95%HPD 区间。

![](C:/Program Files/Typora/media/image1805.png){width="5.01833552055993in" height="3.633860454943132in"}

> 214
>
> 打印出模型的总结，对决策边界进行检查。
>
> pm.df_summary(trace_lda)

+------------+--------+------+----------+---------+----------+
|            | > mean | sd   | mc_error | hpd_2.5 | hpd_97.5 |
+============+========+======+==========+=========+==========+
| > mus\_\_0 | 5.01   | 0.06 | 8.16e-04 | 4.88    | 5.13     |
+------------+--------+------+----------+---------+----------+
| > mus\_\_1 | 5.93   | 0.06 | 6.28e-04 | 5.81    | 6.06     |
+------------+--------+------+----------+---------+----------+
| > sigma    | 0.45   | 0.03 | 1.52e-03 | 0.38    | 0.51     |
+------------+--------+------+----------+---------+----------+
| > bd       | 5.47   | 0.05 | 5.36e-04 | 5.38    | 5.56     |
+------------+--------+------+----------+---------+----------+

> 可以看到LDA模型得到了与逻辑回归类似的结果。
>
> 线性判别式模型可以使用多元高斯分布对类别建模，从而将其扩 展到超过一个特征的情况。此外，还可以对不同类别的数据共享同一 个方差（或者是同一个方差矩阵，对于超过一个特征的情况来说）的 假设进行放松。这样便得到了称作二次判别分析（Quadratic Linear
>
> Discriminant，QDA）的模型，此时决策边界不再是线性的，而是二\
> 次的。
>
> 通常，当特征基本符合高斯分布时，LDA或QDA的效果要比逻\
> 辑回归更好，如果假设不成立，逻辑回归的效果要更好一些。使用判
>
> 别式模型分类的一个好处是：在模型中融合先验更容易（或者说更自 然）；比如我们可以将数据均值和方差的信息融入到模型中去。
>
> 需要注意：LDA和QDA的决策边界是封闭式的，对于两个类别
>
> 215
>
> 和一个特征的情况，应用LDA的时候只需要分别计算出每个类别分布 的均值，然后求二者的均值就得到了决策边界。在前面的模型中，采 用了更贝叶斯的一种方式；我们估计出两个高斯分布的参数，然后将 这些估计融入公式中，不过公式是怎么来的呢？这里不深入更细节的 内容，只需要知道要得到该公式，我们需要假设数据是符合高斯分布 的，因此LDA只有在数据分布接近高斯分布的时候更有效。显然，在 某些问题中，当我们想对正态性的假设放松的时候（比如t分布或者
>
> 多元t分布等），就不能再使用LDA（或QDA）了，不过我们仍然可\
> 以用PyMC3从数值上计算出决策边界。
>
> 216
>
> **5.4**　总结
>
> 本章我们学习了如何扩展单参数线性回归模型用于处理分类问\
> 题，在只有两种类别时如何用逻辑回归进行贝叶斯分类，以及在超过
>
> 两种类别时如何利用softmax回归进行贝叶斯分类。我们还学习了什\
> 么是逆连结函数，如何用来构建广义线性模型，广义模型大大地扩展 了线性模型所能解决的问题。此外还学习了建模过程中可能需要注意 的地方，比如遇到有关联的变量、完美分类的类别或者有偏的类别时 应该怎么处理。我们重点关注的是判别式模型，稍微了解了下生成式 模型，并学习了二者之间的主要区别。
>
> 217
>
> **5.5**　深入阅读
>
> 《Doing Bayesian Data Analysis, Second Edition》的第21章和第22 章。
>
> 《Statistical Rethinking》的第10章。
>
> 《An Introduction to Statistical Learning by Gareth James and others (second edition)》的第4章。\
> 查看PyMC3中有关逻辑回归的例子：[https://pymc-](https://pymc-devs.github.io/pymc3/notebooks/GLM-logistic.html。这个例子还包含了我们下一章中将要讨论的模型比较相关的内容。)\
> [devs.github.io/pymc3/notebooks/GLM-logistic.html。这个例子还包](https://pymc-devs.github.io/pymc3/notebooks/GLM-logistic.html。这个例子还包含了我们下一章中将要讨论的模型比较相关的内容。) [含了我们下一章中将要讨论的模型比较相关的内容。](https://pymc-devs.github.io/pymc3/notebooks/GLM-logistic.html。这个例子还包含了我们下一章中将要讨论的模型比较相关的内容。)
>
> 218
>
> **5.6**　练习
>
> （1）使用花瓣长度和花瓣宽度作为变量重跑第一个模型。二者 的结果有何区别？两种情况下的95%HPD区间分别是多少？
>
> （2）重跑练习（1），这次使用t分布作为弱先验信息。尝试使 用不同的正态参数*v*。
>
> （3）回到第1个例子中，用逻辑回归根据花萼长度判断属于\
> setosa还是versicolor。尝试用第1章中的单参数回归模型来解决这个问 题，线性回归的结果相比逻辑回归的效果如何？线性回归的结果能解
>
> 释为概率吗？提示：检查*y*值是否位于\[0,1\]区间内。
>
> （4）假设我们不用softmax回归，而是用单参数线性模型，并将 类别编码为setosa =0, versicolor =1, virginica = 2。在单参数线性回归 模型中，如果我们交换类别的编码方式会发生什么？结果会保持一样 还是会有所不同？
>
> （5）在处理不均衡数据的例子中，将df = df\[45:\]改\
> 为df\[22:78\]，这样做得到的数据点个数几乎没变，不过现在类别变
>
> 得均衡了，试比较这两种情况的结果，哪种情况得到的结果与使用完 整数据集得到的结果更相似呢？
>
> （6）比较逻辑回归模型与LDA模型的似然，用函数sample_ppc
>
> 生成预测数据并比较，确保理解其中的不同。
>
> 219