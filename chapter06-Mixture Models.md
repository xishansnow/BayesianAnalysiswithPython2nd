> 第**7**章　混合模型

> 一种常见的构建模型的方式是将简单的模型组合在一起得到一个 更复杂的模型。在统计学中，这类模型通常称为混合模型。混合模型 有许多用途，比如直接对亚种群建模，或者作为一个小技巧来处理无 法用简单分布描述的复杂概率分布。这一章我们将学习如何构建这类 混合模型，同时还将看到，前面几章中的一些模型本质上其实也是混 合模型，而在这一章中，我们将从混合模型的视角去揭示它们。
>
> 本章我们将学习以下内容：
>
> 有限混合模型；\
> 零膨胀泊松分布； 零膨胀泊松回归； 鲁棒逻辑回归；\
> 基于模型的聚类； 连续混合模型。
>
> 255
>
> **7.1**　混合模型
>
> 有时候，某种现象或者过程无法用一个简单的分布（比如高斯分 布、伯努利分布或者其他常见的概率分布）描述，但可以使用这些分 布的组合来描述。这种假设数据是从一系列分布的组合中得到的模
>
> 型，称作混合模型。
>
> 一种应用混合模型的场景是：数据集来自于一些亚种群的组合。 比如，在描述身高分布的时候，我们会很自然地将成人分为男人和女 人两个亚种群，此外，如果想要在分析中包含未成年人，可能需要加 入第3个亚种群去描述未成年人（在这个种群中可能不需要区分男孩
>
> 和女孩）。另外一个有关混合模型的经典例子是用其来描述一组手写 数字（MNIST数据集），这种情况下，可以很自然地使用10个亚种\
> 群对数据建模。
>
> 某些情况下，选择混合模型可能是出于数学上（或计算上）方便 性的考虑，而不是因为我们希望尽量用亚种群去描述数据。以高斯分 布为例，许多单峰分布和近似对称的分布都可以用高斯分布去近似， 不过对于多峰分布或者偏斜分布呢？我们还可以使用高斯分布对其建 模吗？答案是如果使用混合高斯分布的话是可以的。在高斯混合模型 中，每个高斯分布都有不同的均值以及相同（或不同）的标准差。通 过组合高斯分布，我们可以给模型增加灵活性，从而适应复杂的数据 分布。事实上，我们可以用多个高斯分布去拟合任意分布，而不论该 分布有多么复杂或者奇怪，具体的高斯分布个数取决于拟合的精度和 数据的具体分布情况。充分应用该思想的一个例子是前面见过的KDE 方法，用来画出数据的分布（而不是用直方图），该方法对每个点都 应用一个分布（在Scipy的实现中用的是高斯分布），然后将每个单
>
> 256
>
> 独的高斯分布都求和之后用来拟合数据的分布。KDE属于非参的方 法，我们将在第8章详细讨论非参方法。现在先只关注如何用混合高 斯模型来拟合任意分布。
>
> 不论我们是真的相信数据中存在亚种群，还是为了利用其数学简 洁性（又或者是二者皆有之），混合模型都是一种不错的选择，使用 混合分布去描述数据可以给我们的模型增加很大的灵活性。
>
> **7.1.1**　如何构建混合模型
>
> 构建有限混合模型的思想是：我们有一定数量的亚种群，每个亚 种群都可以用一种分布来表示，然后还有符合这些分布的数据点，不 过我们并不知道每个点具体属于哪个分布，因而我们希望正确地区分 每个点。这类问题可以通过多层模型来实现，在模型的上层有一个随 机变量，通常称为隐层变量（该变量一般无法直接观测到），然后根 据隐变量的函数来确定每个观测点属于哪个分布。也就是说，隐变量 决定使用哪个分布对数据点进行建模。在文献中，人们经常使用字母 *z*来表示隐变量。
>
> 让我们从一个简单的例子来着手构建混合模型。假设我们有一个 数据集，希望用3个高斯变量对其描述。
>
> clusters = 3
>
> n_cluster = \[90, 50, 75\] n_total = sum(n_cluster) means = \[9, 21, 35\]\
> std_devs = \[2, 2, 2\]
>
> mix = np.random.normal(np.repeat(means, n_cluster), np.repeat(std_devs, n_cluster))
>
> sns.kdeplot(np.array(mix))\
> plt.xlabel(\'\$x\$\', fontsize=14)
>
> 257

![](C:\Program Files\Typora\media\image2028.png){width="5.01833552055993in" height="3.529737532808399in"}

> 在许多真实场景中，当我们想着手构建模型的时候，通常从更简 单有效的模型开始，然后再逐步变复杂。这种做法的优点是：可以一 步步建立对问题和数据的直观认识，同时避免一上来就被复杂的模型 难倒了（复杂的模型通常更难以调试）。
>
> 因此，先假设我们的数据可以用3个高斯分布来描述（或者更一 般地，设为*k*个高斯分布）。做出该假设的原因可能是我们已经有足 够的实验或理论知识做支撑，也可能是我们通过直接观察得出了该假
>
> 设。接下来我们还假设已经知道了每个高斯分布的均值和标准差。
>
> 有了这些假设之后，问题简化成了将每个数值归类到3个已知的 高斯分布中。有许多方法可以解决这个问题，显然，这里我们选择用
>
> 贝叶斯的方法解决，并构建一个概率模型。
>
> 这里借用抛硬币问题中的思想来构建模型。在抛硬币问题中，可 以使用伯努利分布来描述可能出现的结果。由于我们不知道正面朝上 或者反面朝上的概率，于是采用了一个beta先验分布来描述。现在高 斯混合模型遇到的问题也很相似，只不过现在我们有*k*个结果。
>
> 258
>
> 伯努利分布更一般的形式是类别分布，beta分布的一般形式是狄 利克雷分布。狄利克雷分布初看可能有点奇怪，因为它是一种单纯 形，有点像*n*维的三角形：1-单纯形是一个线段；2-单纯形是一个三 角形；3-单纯形是正四面体，以此类推。为什么是一个单纯形？直观 上看，这是因为该分布是一个长度为*k*的向量，其中每个元素都为正 数而且所有元素之和为1。在理解为何狄利克雷分布是beta分布的一 般形式之前，我们先回顾下beta分布的一些特性。前面我们用beta分 布来描述有两种结果的问题，其中之一的概率为*p*，另一个概率为1- *p*，因而我们可以认为beta分布返回的是一个长度为2的向量，\[*p*, 1- *p*\]。当然，在实践中，我们通常忽略了1-*p*这一项，因为它已经通过*p* 完全被定义了。此外，beta分布也可以通过两个标量α和β来定义。这 些参数如何类比到狄利克雷分布中呢？我们先考虑下最简单的狄利克 雷分布，用来对有3种结果的问题建模。此时的狄利克雷分布返回一 个长度为3的向量\[*p*,*q*,*r*\]，其中*r* = 1 *−* (*p* + *q*)。我们也可以用3个标量 来参数化地描述狄利克雷分布，可以称为*α*、*β*和*γ*，不过这不方便推 广到更高维的情况，所以这里使用一个长度为*k*的向量*α*来描述，其中 *k*对应可能出现的结果的种类个数。我们可以将beta分布和狄利克雷 分布想象成描述概率分布的分布，为了更直观地理解，我们将不同参 数对应的狄利克雷分布画了出来，留意下图中的每个三角形与参数相 近的beta分布之间的联系。
>
> 259

![](C:\Program Files\Typora\media\image2031.png){width="3.768956692913386in" height="3.0195395888013996in"}

> 上图是在Thomas Boggs代码的基础上稍做修改后生成的，你可以 在本书相关的代码中找到源码，在本章阅读更多部分可以了解更多细 节。
>
> 现在我们对狄利克雷分布有了更多了解，也就意味着掌握了构建 混合模型所需要的全部基础。一种可视化的方法是：将其看作是基于 高斯估计模型的（*k*面）抛硬币问题，当然，你可能更愿意将其看作
>
> （*k*面）掷骰子问题。用Kruschke图可以将模型画成如下形式：
>
> 260

![](C:\Program Files\Typora\media\image2033.png){width="3.768956692913386in" height="7.371842738407699in"}

> 这里圆角矩形框表示我们有*k*个高斯似然（以及对应的先验）， 类别变量决定具体使用哪一个描述数据。
>
> 记住，我们假设已经知道了这些高斯分布的均值和标准差；只需 要将每个点赋给一个高斯分布即可。下面构建模型中的一个细节是， 我们已经有了两个采样器，Metropolis和ElemwiseCategorical，后者是
>
> 261
>
> 专门为离散变量的采样设计的。
>
> with pm.Model() as model_kg:
>
> p = pm.Dirichlet(\'p\', a=np.ones(clusters))
>
> category = pm.Categorical(\'category\', p=p, shape=n_total)
>
> means = pm.math.constant(\[10, 20, 35\])
>
> y = pm.Normal(\'y\', mu=means\[category\], sd=2, observed=mix)
>
> step1 = pm.ElemwiseCategorical(vars=\[category\], values=range(cluste rs))
>
> step2 = pm.Metropolis(vars=\[p\])
>
> trace_kg = pm.sample(10000, step=\[step1, step2\])
>
> chain_kg = trace_kg\[1000:\]
>
> varnames_kg = \[\'p\'\]
>
> pm.traceplot(chain_kg, varnames_kg)

![](C:\Program Files\Typora\media\image2039.png){width="5.643024934383202in" height="0.8017399387576553in"}

> 现在我们已经知道了高斯混合模型的基本结构，接下来我们再添 加一层更复杂的，估计出高斯分布的参数。我们假设有3个不同的均 值和一个共享的标准差。
>
> 和往常一样，模型可以很容易地用PyMC3的语法来描述。
>
> with pm.Model() as model_ug:
>
> p = pm.Dirichlet(\'p\', a=np.ones(clusters))
>
> category = pm.Categorical(\'category\', p=p, shape=n_total)
>
> means = pm.Normal(\'means\', mu=\[10, 20, 35\], sd=2, shape=clusters) sd = pm.HalfCauchy(\'sd\', 5)
>
> y = pm.Normal(\'y\', mu=means\[category\], sd=sd, observed=mix)
>
> step1 = pm.ElemwiseCategorical(vars=\[category\], values=range(cluste rs))
>
> step2 = pm.Metropolis(vars=\[means, sd, p\])
>
> trace_ug = pm.sample(10000, step=\[step1, step2\])
>
> 然后再看一下迹。
>
> 262
>
> chain = trace\[1000:\]
>
> varnames = \[\'means\', \'sd\', \'p\'\] pm.traceplot(chain, varnames)

![](C:\Program Files\Typora\media\image2049.png){width="5.643024934383202in" height="2.6759372265966754in"}

> 以及推断总结的表格。
>
> pm.df_summary(chain, varnames)

+---------------+-----------+----------+----------+-----------+-----------+
|               | > mean    | sd       | mc_error | hpd_2.5   | hpd_97.5  |
+===============+===========+==========+==========+===========+===========+
| > means\_ \_0 | 21.053935 | 0.310447 | 0.012280 | 20.495889 | 21.735211 |
+---------------+-----------+----------+----------+-----------+-----------+
| > means\_ \_1 | 35.291631 | 0.246817 | 0.008159 | 34.831048 | 35.781825 |
+---------------+-----------+----------+----------+-----------+-----------+
| > means\_ \_2 | 8.956950  | 0.235121 | 0.005993 | 8.516094  | 9.429345  |
+---------------+-----------+----------+----------+-----------+-----------+
| > sd          | 2.156459  | 0.107277 | 0.002710 | 1.948067  | 2.368482  |
+---------------+-----------+----------+----------+-----------+-----------+
| > p\_ \_0     | 0.235553  | 0.030201 | 0.000793 | 0.179247  | 0.297747  |
+---------------+-----------+----------+----------+-----------+-----------+
| > p\_ \_1     | 0.349896  | 0.033905 | 0.000957 | 0.281977  | 0.412592  |
+---------------+-----------+----------+----------+-----------+-----------+

![](C:\Program Files\Typora\media\image2054.png){width="3.8190069991251095e-2in" height="0.2151968503937008in"}

> 263 ![](C:\Program Files\Typora\media\image2067.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}
>
> ![](C:\Program Files\Typora\media\image2074.png){width="3.818678915135608e-2in" height="0.506738845144357in"}p\_ \_2 0.347436 0.032414 0.000942 0.286669 0.410189
>
> 接下来再做后验预测检查，看看模型从数据中学到了什么。
>
> ppc = pm.sample_ppc(chain, 50, model)\
> for i in ppc\[\'y\'\]:
>
> sns.kdeplot(i, alpha=0.1, color=\'b\')
>
> sns.kdeplot(np.array(mix), lw=2, color=\'k\') plt.xlabel(\'\$x\$\', fontsize=14)

![](C:\Program Files\Typora\media\image2102.png){width="5.01833552055993in" height="3.540149825021872in"}

> 注意查看图中颜色较浅的蓝色线条所表示的不确定性，从中可以 看到，在*x*较小或者较大的值附近，不确定性较低，而在中间部分不 确定性较高。这一点符合直观认识，因为不确定性较高的区域对应高 斯分布重叠的地方，因而比较难以分辨一个点具体属于哪一个高斯分 布。我承认，这个问题比较简单而且不那么有挑战性，不过该问题有 助于强化我们的直觉，即一个简单的模型可以扩展并应用到复杂问题 中。
>
> **7.1.2**　边缘高斯混合模型
>
> 264
>
> 前面的模型中，我们显式地定义了隐变量*z*，这样做带来的一个\
> 问题是，对离散的隐变量*z*采样通常会导致模型收敛较慢而且不利于 探索分布的尾部。使用一些针对离散隐变量特殊定制的采样器能有助
>
> 于改进采样过程，除此之外，还有一种做法是换一种参数化表示方\
> 法，重新构建一个等价的模型。注意，在混合模型中，观测变量*z*是\
> 根据隐变量*z*构建的，即：
>
> *p*( *y* \| *z*,θ)
>
> 我们可以认为隐变量*z*是一个可以被边缘化的多余变量，因而得
>
> 到：
>
> *p*( *y* \| *θ*)
>
> 幸运地是，PyMC3包含一个分布可以高效地处理该过程，不需\
> 要我们手动处理中间的数学过程，因此我们可以按如下形式构造高斯
>
> 混合模型。
>
> with pm.Model() as model_mg:
>
> p = pm.Dirichlet(\'p\', a=np.ones(clusters))
>
> means = pm.Normal(\'means\', mu=\[10, 20, 35\], sd=2, shape=clusters) sd = pm.HalfCauchy(\'sd\', 5, shape=clusters)
>
> y = pm.NormalMixture(\'y\', w=p, mu=means, sd=sd, observed=mix)
>
> step = pm.Metropolis()\
> trace_mg = pm.sample(2000, step)
>
> 这里将运行模型、画图以及探索结果的过程留给读者当做练习。
>
> **7.1.3**　混合模型与计数类型变量
>
> 有时候，我们会用到一些计数类型的数据，比如放射性核的衰
>
> 265
>
> 减、每对夫妻的小孩个数或者是推特的粉丝个数。这些例子的相同之 处是，我们可以用非负的离散数字来建模{0, 1, 2, 3\...}。这类变量称\
> 作计数变量，通常使用泊松分布对其建模。
>
> 泊松分布
>
> ![](C:\Program Files\Typora\media\image2109.png){width="0.10411417322834646in" height="0.12494641294838145in"}假设希望统计一条马路上每个小时经过的红色小汽车数量，我们 就可以选用泊松分布来描述该数据。泊松分布通常用来描述在一段固 定的时间（区间）内某一特定事件发生的次数。因而泊松分布假设在 这段固定的时间（区间）内，事件之间的发生是相互独立的。该离散 分布只有一个参数 ，对应分布的均值和方差。泊松分布的概率质量 函数为：

![](C:\Program Files\Typora\media\image2110.png){width="1.395138888888889in" height="0.4164873140857393in"}

> 其中：
>
> ![](C:\Program Files\Typora\media\image2111.png){width="0.10411417322834646in" height="0.12494641294838145in"}是单位时间（空间）内时间发生的均值 *k*是正整数，如0，1，2，...\
> *k!*表示*k*的阶乘
>
> ![](C:\Program Files\Typora\media\image2115.png){width="0.10411417322834646in" height="0.12494641294838145in"}下面的图中，我们可以看到一系列不同 值对应的泊松分布的例
>
> 子。
>
> lam_params = \[0.5, 1.5, 3, 8\]
>
> k = np.arange(0, max(lam_params) \* 3)
>
> for lam in lam_params:
>
> y = stats.poisson(lam).pmf(k)
>
> plt.plot(k, y, \'o-\', label=\"\$\\\\lambda\$ = {:3.1f}\".format(lam)) plt.legend()
>
> plt.xlabel(\'\$k\$\', fontsize=14)
>
> plt.ylabel(\'\$pmf(k)\$\', fontsize=14)
>
> 266

![](C:\Program Files\Typora\media\image2121.png){width="5.01833552055993in" height="3.5089140419947507in"}

> ![](C:\Program Files\Typora\media\image2122.png){width="0.10411417322834646in" height="0.12494641294838145in"}注意，可以是一个浮点数，而*k*只能是正整数。前面的图中，每 个点表示对应*x*取值的分布情况，这里点与点之间用曲线连接起来是 为了帮助我们理解该分布的形状，记住，泊松分布是离散的分布。泊 松分布可以看作是二项分布的一种特殊形式（当*n*非常大，而*p*很小 时）。这里不深入过多的数学细节，我们可以这么思考，由于可能看 到或者看不到红色小汽车，我们可以用一个二项分布来对红色小汽车 的个数建模，此时我们有：

![](C:\Program Files\Typora\media\image2123.png){width="1.145262467191601in" height="0.187419072615923in"}

> 然后，二项分布的均值为：

![](C:\Program Files\Typora\media\image2124.png){width="0.82250656167979in" height="0.187419072615923in"}

> 二项分布的方差为：

![](C:\Program Files\Typora\media\image2125.png){width="1.62419072615923in" height="0.19783136482939634in"}

> 不过考虑到即使你站在一条非常繁忙的马路上，看到红色小汽车 的机会相比于整个城市的汽车总量仍然很少，因而我们有：
>
> 267

![](C:\Program Files\Typora\media\image2127.png){width="2.051061898512686in" height="0.187419072615923in"}

> 于是，我们可以做如下近似：

![](C:\Program Files\Typora\media\image2128.png){width="1.0411482939632546in" height="0.187419072615923in"}

> 现在均值和方差是用同一个数来表示的，于是我们可以有信心地 说这里的变量服从泊松分布，此时我们有：

![](C:\Program Files\Typora\media\image2129.png){width="1.4784306649168855in" height="0.187419072615923in"}

> 零膨胀泊松模型
>
> 当我们计数时，数字0可能会多次出现，原因有多个，有可能在 我们统计红色汽车个数时刚好没有车通过那条马路，或者是我们碰巧
>
> 错过了（有可能我们没有注意到大卡车后面的红色小汽车）。因此， 如果使用的是泊松分布，在进行后验预测检查时，不会得到一个非常 漂亮的拟合，因为如果数据真的符合泊松分布的话，我们不会看到这 么多的0。
>
> 如何解决这个问题呢？我们可以尝试找到模型预测比观察到的0 更少的原因，并将其加入到模型中。不过大多数情况下，只需要假设
>
> ![](C:\Program Files\Typora\media\image2130.png){width="0.353989501312336in" height="0.16659448818897638in"}我们有一个由泊松分布组成的混合模型即可，其中泊松分布的概率为 ，取到额外0的概率为。如果选用混合模型，那么我们得到的就
>
> ![](C:\Program Files\Typora\media\image2130.png){width="0.353989501312336in" height="0.16659448818897638in"}是零膨胀泊松模型（Zero-Inflated Poisson，ZIP）。在有些书中，你\
> 也许会看到表示额外的0，而表示泊松分布的概率。这个问题不 太大，只需要在具体例子中注意区分就好。一个基本的零膨胀泊松模 型的表示形式如下：

![](C:\Program Files\Typora\media\image2134.png){width="2.2905260279965005in" height="0.4997856517935258in"}

> ![](C:\Program Files\Typora\media\image2130.png){width="0.353989501312336in" height="0.16659448818897638in"}其中是取到额外0的概率。 我们可以根据这些等式用PyMC3
>
> 268
>
> ![](C:\Program Files\Typora\media\image2137.png){width="0.10411417322834646in" height="0.12494641294838145in"}来构建模型，不过PyMC3已经自带了ZIP分布，因此这个模型写起来 更容易一些。由于Python已经有一个匿名函数关键字lambda了，因此 我们这里使用了lam来表示上式中的，于是我们可以用以下PyMC3
>
> 代码实现ZIP模型：
>
> with pm.Model() as ZIP:
>
> psi = pm.Beta(\'p\', 1, 1)\
> lam = pm.Gamma(\'lam\', 2, 0.1)
>
> y = pm.ZeroInflatedPoisson(\'y\', lam, psi, observed=counts) trace = pm.sample(1000)
>
> pm.traceplot(trace\[:\]);

![](C:\Program Files\Typora\media\image2142.png){width="5.643024934383202in" height="1.770074365704287in"}

> 泊松回归与**ZIP**回归
>
> ZIP模型看起来可能有点笨，不过有时候我们需要用它来估计一 些简单的分布（甚至是泊松分布或者高斯分布）。此外，泊松分布或
>
> 者ZIP分布还可以用于线性模型。在第4章中，我们看到过，一元线性 回归模型可以由一个线性模型（通过一个恒等逆连结函数）和作为噪 声或者误差的高斯分布（或t分布）组成。第5章中，我们还看到了如 何将这个模型用于分类，其中使用了逻辑函数或者softmax函数作为
>
> ![](C:\Program Files\Typora\media\image2143.png){width="0.10411417322834646in" height="0.11453412073490814in"}逆连结函数，同时用伯努利分布或者类别分布对因变量建模。根据同 样的思想，当因变量为计数变量时，可以用一个泊松分布或者ZIP分 布进行回归分析，下图就是一个ZIP回归，不过如果我们要做泊松回 归就不需要包含，因为我们不需要对多余零建模。注意，现在我们 可以使用指数函数作为逆连结函数，该选择保证了线性模型的返回值
>
> 269
>
> 始终都是正的。

![](C:\Program Files\Typora\media\image2146.png){width="4.393646106736658in" height="4.310653980752406in"}

> 为了用例子说明ZIP回归模型是如何实现的，这里我们使用从\
> <http://www.ats.ucla.edu/stat/data/fish.csv>[得到的数据集，该数据集也可](http://www.ats.ucla.edu/stat/data/fish.csv得到的数据集，该数据集也可以在本书的代码中找到。)
>
> [以在本书的代码中找到。](http://www.ats.ucla.edu/stat/data/fish.csv得到的数据集，该数据集也可以在本书的代码中找到。) 问题描述如下：我们在一个公园管理处工 作，希望提升游客的体验，因此我们决定对访问公园的250组游客进 行问卷调查，在收集到的数据中包含以下内容：
>
> 他们抓到的鱼的条数；\
> 每组中有多少个小孩；\
> 他们是否带了露营车到公园。
>
> 我们将使用该数据，根据小孩个数和是否带露营车这两个变量来 预测它们抓到的鱼的条数。下面用Pandas通过如下代码载入数据：
>
> fish_data = pd.read_csv(\'fish.csv\')
>
> 270
>
> 这里将探索数据集的过程当做练习留给读者，你可以自由使用画 图函数或者Pandas中的describe()函数等。我们可以按如下形式先将 Kruschke图转换成PyMC3中的代码：
>
> with pm.Model() as ZIP_reg:\
> psi = pm.Beta(\'psi\', 1, 1)
>
> alpha = pm.Normal(\'alpha\', 0, 10)
>
> beta = pm.Normal(\'beta\', 0, 10, shape=2)
>
> lam = pm.math.exp(alpha + beta\[0\] \* fish_data\[\'child\'\] + beta\[1\] \* fish_data\[\'camper\'\])
>
> y = pm.ZeroInflatedPoisson(\'y\', lam, psi, observed=fish_data\[\'count \'\])
>
> trace_ZIP_reg = pm.sample(1000)
>
> chain_ZIP_reg = trace_ZIP_reg\[100:\]
>
> pm.traceplot(chain_ZIP_reg)

![](C:\Program Files\Typora\media\image2160.png){width="5.643024934383202in" height="2.707173009623797in"}

> 和往常一样，你需要对采样过程做完整的检查，为了更好地理解 推断结果，可以用如下代码画一幅图出来：
>
> ![](C:\Program Files\Typora\media\image2161.png){width="3.818678915135608e-2in" height="1.8603258967629046in"}children = \[0, 1, 2, 3, 4\]
>
> fish_count_pred_0 = \[\]
>
> fish_count_pred_1 = \[\]
>
> thin = 5
>
> for n in children:
>
> without_camper = chain_ZIP_reg\[\'alpha\'\]\[::thin\] + chain_ZIP_reg\[\'be ta\'\]\[:,0\]\[::thin\] \* n
>
> with_camper = without_camper + chain_ZIP_reg\[\'beta\'\]\[:,1\]\[::thin\]\
> fish_count_pred_0.append(np.exp(without_camper))
>
> ![](C:\Program Files\Typora\media\image2164.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}271
>
> ![](C:\Program Files\Typora\media\image2166.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"} fish_count_pred_1.append(np.exp(with_camper))

![](C:\Program Files\Typora\media\image2170.png){width="5.01833552055993in" height="3.5505621172353456in"}

> **7.1.4**　鲁棒逻辑回归
>
> 前面我们已经知道了如何在不直接对多余零建模的情况下，解决 零过剩的问题。根据Kruschke的建议，另外一种类似的方法是，构建 一个更鲁棒的逻辑回归。在逻辑回归中，我们将数据建模成二项式的 （即0和1），因而数据集有可能包含多余的0或者1。这里还是以前面 的鸢尾花数据集为例，不过增加了一些异常点，代码实现如下：
>
> iris = sns.load_dataset(\"iris\")
>
> df = iris.query(\"species == (\'setosa\', \'versicolor\')\")
>
> y_0 = pd.Categorical(df\[\'species\'\]).codes
>
> x_n = \'sepal_length\'
>
> x_0 = df\[x_n\].values
>
> y_0 = np.concatenate((y_0, np.ones(6)))
>
> x_0 = np.concatenate((x_0, \[4.2, 4.5, 4.0, 4.3, 4.2, 4.4\])) x_0\_m = x_0 - x_0.mean()
>
> plt.plot(x_0, y_0, \'o\', color=\'k\')
>
> 272

![](C:\Program Files\Typora\media\image2176.png){width="5.01833552055993in" height="3.4360279965004374in"}

> 其中有一些versicolors的花萼长度异常短，这里可以通过一个混 合模型来修复。我们假设变量的输出源自两个部分，其一是概率为*π* 的随机结果，另外一个是概率为1 *− π*的逻辑回归分布。对应的数学 形式如下：

![](C:\Program Files\Typora\media\image2177.png){width="2.956861329833771in" height="0.187419072615923in"}

> 注意当*π =* 1时，*p* = 0.5；当*π =* 2时，就得到了逻辑回归。可以 在第5章模型实现的基础上实现上面的模型，代码实现如下：
>
> ![](C:\Program Files\Typora\media\image2178.png){width="3.818678915135608e-2in" height="3.141026902887139in"}with pm.Model() as model_rlg:
>
> alpha_tmp = pm.Normal(\'alpha_tmp\', mu=0, sd=100) beta = pm.Normal(\'beta\', mu=0, sd=10)
>
> mu = alpha_tmp + beta \* x_0\_m
>
> theta = pm.Deterministic(\'theta\', 1 / (1 + pm.math.exp(-mu)))
>
> pi = pm.Beta(\'pi\', 1, 1)
>
> p = pi \* 0.5 + (1 - pi) \* theta
>
> alpha = pm.Deterministic(\'alpha\', alpha_tmp - beta \* x_0.mean()) bd = pm.Deterministic(\'bd\', -alpha/beta)
>
> yl = pm.Bernoulli(\'yl\', p=p, observed=y_0)
>
> trace_rlg = pm.sample(2000, start=pm.find_MAP())
>
> ![](C:\Program Files\Typora\media\image2181.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}273
>
> ![](C:\Program Files\Typora\media\image2183.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}varnames = \[\'alpha\', \'beta\', \'bd\', \'pi\'\] pm.traceplot(trace_rlg, varnames)

![](C:\Program Files\Typora\media\image2187.png){width="5.643024934383202in" height="3.665096237970254in"}

> 如果将以上结果与第5章中的结果进行比较，我们可以看到二者 差不多：
>
> pm.df_summary(trace_rlg, varnames)

+---------+--------+-------+----------+---------+----------+
|         | > mean | sd    | mc_error | hpd_2.5 | hpd_97.5 |
+=========+========+=======+==========+=========+==========+
| > alpha | -88.04 | 32.06 | 1.99     | -151.35 | -32.68   |
+---------+--------+-------+----------+---------+----------+
| > beta  | 16.15  | 5.87  | 0.36     | 6.21    | 27.95    |
+---------+--------+-------+----------+---------+----------+
| > bd    | 5.45   | 0.05  | 0.00     | 5.34    | 5.55     |
+---------+--------+-------+----------+---------+----------+
| > pi    | 0.30   | 0.07  | 0.00     | 0.15    | 0.45     |
+---------+--------+-------+----------+---------+----------+

> 274

![](C:\Program Files\Typora\media\image2193.png){width="5.01833552055993in" height="3.633860454943132in"}

> 275
>
> **7.2**　基于模型的聚类
>
> 聚类是统计学和机器学习中无监督学习的一部分，与分类有点类 似，不过更复杂一些，因为聚类问题中我们没有正确的标签。
>
> 笼统地说，聚类就是在没有标签的情况下将属性相近的数据点归 类到一起，使得组内的数据距离较近，而组间数据的距离较远。聚类 有许多应用，比如，在种系遗传学（生物学中研究不同物种之间的进 化关系的一门学科）中，聚类可以用来指导解决一些进化问题；另外 一个更偏商业的应用是，根据一个用户的历史消费记录，将其与其他 用户聚类到一组，从而猜出他们可能对哪些电影、书籍、歌曲等感兴 趣。在一些其他无监督学习的任务中，我们可能希望直接做聚类，或 者是将其作为探索式数据分析的一部分。接下来将学习一些常见的聚 类准则，用于判断两个点是否属于同一个组。
>
> 通常我们使用欧几里得距离来衡量两个点之间的距离。直观上\
> 看，欧几里得距离就是连接两个点的线段长度，尽管我们测量的不是
>
> 实际的物理距离（比如说米或者光年），仍然可以定义出欧几里得距 离，对于*n*维空间中的两个点*q~i~*和*p~i~*，可以将欧几里得距离定义成如下
>
> 形式：

![](C:\Program Files\Typora\media\image2196.png){width="2.821512467191601in" height="0.35401465441819774in"}

> 在机器学习中有一个算法叫做K-均值聚类，使用的就是欧几里得 距离。我们暂时不会深入学习这个算法，不过我建议你先自己了解 下，因为这个算法是聚类问题中最经典的一个例子，而且很容易理 解、解释并实现。在Sebastian Raschka写的《Python Machine\
> Learning》一书中你可以了解更多有关该算法的内容。
>
> 276
>
> 计算欧几里得距离或其他类似的距离并非解决聚类问题的唯一方 法。另外一种方法是从概率的角度来看待聚类问题，假设数据是从某 些概率分布中抽样得到的，然后可以得到一个概率模型，这类方法通 常称为基于模型的聚类。
>
> 在使用贝叶斯的框架解决聚类问题的时候，混合模型是一个很自 然的选择。事实上，本章第一个例子就是使用的混合模型对（未标注 的）亚种群进行建模。利用概率模型可以计算出每个点属于各个类别 的概率，这称作软聚类，与之对应的是硬聚类（每个点属于某个类
>
> 别的概率要么是0，要么是1）。当然，我们可以通过某种规则或者阈 值将软聚类转换成硬聚类，就像逻辑回归一样，一种做法是，计算每 个点属于各个类别的概率，然后将概率最大的那个类别标签赋给该
>
> 点。
>
> **7.2.1**　固定成分聚类
>
> 本章的第一个例子就是将混合模型应用到分类问题的一个典型例 子，我们对数据有一个经验性的假设分布，然后用3个高斯分布来描 述数据中的亚种群。注意这里假设数据服从混合高斯分布，而不是从 原来数据之间相似性测度的角度来思考的。
>
> **7.2.2**　非固定成分聚类
>
> 在一些问题，比如手写数字的识别中，我们很容易就能看出类别 的个数；对于另外一些问题，我们可以有一些靠谱的猜测，比如对于 某个与鸢尾花数据集相似的数据集，我们可能知道其中样本的来源是 一个只有3种鸢尾花生长的地方；而对于一些其他问题而言，预先指
>
> 定类别个数的先验可能会有问题。一种贝叶斯的解决办法是使用一种 非参的方法来估计出类别的个数，比如我们可以使用狄利克雷过程。
>
> 277
>
> 在第8章中，我们将学习非参统计学；不过，本书并没有包含狄利克 雷过程，建议你在阅读完第8章之后，阅读并运行以下代码\
> [https://pymc-devs.github.io/pymc3/notebooks/dp_mix.html，这个介绍是](https://pymc-devs.github.io/pymc3/notebooks/dp_mix.html，这个介绍是由Austin) [由Austin](https://pymc-devs.github.io/pymc3/notebooks/dp_mix.html，这个介绍是由Austin) Rochford写的（PyMC3的开发者之一，也是本书的英文版的 审稿人）。
>
> 278
>
> **7.3**　连续混合模型
>
> 本章重点关注了离散的混合模型，不过我们也可以有连续的混合 模型。事实上我们已经了解了其中的一部分，连续混合模型之一就是 前面介绍过的鲁棒逻辑回归模型，由两个部分组成：一个逻辑回归和 一个随机估计。注意此时参数*π*不再是一个开关，而更像是一个球形
>
> 把手控制着逻辑回归和随机变量的比例，只有当*π*为0或1时，我们才 会得到完全随机的结果或者完全的逻辑回归结果。
>
> 多层模型也可以看做是连续混合模型，其中每个组的参数来自于 上层的连续分布。更具体点，假设我们要对几个不同的组做线性回 归，可以假设每个组都有自己的斜率或者所有组都共享相同的斜率。 此外，除了将问题看作这两个极限情况，我们还可以构建一个多层模 型，对这些极限值构建一个连续混合模型，此时这些极限值不过是分 层模型中的一些特例罢了。
>
> **7.3.1**　**beta-**二项分布与负二项分布
>
> beta-二项分布是一个离散分布，通常用来描述*n*次伯努利实验中 成功的次数*y*，其中每次实验成功的概率*p*未知，并且假设其服从参数
>
> 为*α*和*β*的伯努利beta分布，对应的数学形式如下：

![](C:\Program Files\Typora\media\image2202.png){width="5.195331364829396in" height="0.4789610673665792in"}

> 也就是说，为了找到观测到结果*y*的概率，我们遍历所有可能的 （连续的）*p*值然后求平均。因此，beta-二项分布也可以看作是连续 混合模型。
>
> 279
>
> 如果你觉得beta-二项分布听起来很熟悉，那一定是因为你对本书 的前两章学得很认真！在抛硬币的问题中，我们用到了该模型，尽管 我们当时显式地使用了一个beta分布和一个二项分布，你也可以直接 使用混合beta-二项分布。
>
> 类似的，还有负二项分布，可以将其看作是一个连续混合的伽马 泊松分布，也就是将一个来自伽马分布的值作为泊松分布的参数，然 后对泊松分布连同伽马分布求均值（积分）。该分布常用来解决计数 型数据中的一个常见问题过度离散。假设你用一个泊松分布来对计数 型数据建模，然后你意识到数据中的方差超出了模型的方差（使用欧 松分布的一个问题是，其均值与方差是有联系的，事实上是用同一个 参数描述的），那么解决该问题的一个办法是将数据看作是（连续
>
> 的）泊松分布的混合，其中的泊松分布的参数来自于一个伽马分布， 从而很自然地用到了负二项分布。使用混合分布之后，我们的模型有 了更好的灵活性，并且能够更好地适应从数据中观测到的均值和方
>
> 差。
>
> beta-二项分布和负二项分布都可以用作线性模型的一部分，而且 也都有零膨胀的版本，此外二者都已经在PyMC3中实现了。
>
> **7.3.2**　**t**分布
>
> 前面我们介绍了t分布是一种更鲁棒的高斯分布。从下面的数学\
> 表达式可以看到，t分布同样可以被看作是连续混合模型：

![](C:\Program Files\Typora\media\image2204.png){width="3.716900699912511in" height="0.4477241907261592in"}

> ![](C:\Program Files\Typora\media\image2205.png){width="0.10411417322834646in" height="0.12494641294838145in"}注意，这个表达式与前面的负二项分布的表达式很像，不过这里\
> 是参数为和*σ*的正态分布以及从参数为*v*的*Inv*χ^2^分布中采样得到的*σ*，
>
> 280
>
> ![](C:\Program Files\Typora\media\image2207.png){width="0.10411417322834646in" height="0.12494641294838145in"}也就是自由度，通常我们更倾向于称为正态参数。这里参数和beta- 二项分布里的参数*p*概念上相似，等价于有限混合模型中的隐变量*z*。 对于有限混合模型来说，很多时候我们可以在推断之前先对隐变量*z*\
> 做边缘化处理，从而得到一个更简单的模型，正如前面的边缘混合模 型中的例子一样。
>
> 281
>
> **7.4**　总结
>
> 这一章中，我们学习了混合模型。混合模型可以用来解决许多问 题，根据前面几章中学到的内容，我们可以很容易地构建有限混合模 型。这类模型可以很方便地应用在计数类型数据的零过剩问题中，或 者是在观测到过度分散的数据时用来扩展泊松模型。另外一个应用是 扩展逻辑回归，用来处理异常值。此外我们还简单地讨论了从贝叶斯 角度做聚类的一些核心概念。最后本章讨论了一些有关连续混合模型 的理论，以及如何将这类模型与前面几章中学到的概念联系起来，比 如多层模型、t分布。
>
> 282
>
> **7.5**　深入阅读
>
> 《Statistical Rethinking》中的第11章。
>
> 《Doing Bayesian Data Analysis, Second Edition》中的第21章。 《Bayesian Data Analysis, Third Edition》中的第22章。
>
> 283
>
> **7.6**　练习
>
> 1．修改本章第一个例子中的合成数据，从而使得模型更难复现 出真实的参数。尝试通过修改3个高斯分布的均值和方差增加它们的 重叠度；尝试修改每类数据的个数，并思考如何有针对性地优化模
>
> 型。
>
> 2．使用"鱼"数据集扩展本章中使用的模型，将人数这一变量作\
> 为线性模型的一部分。在对多余零建模的时候包含进该变量，你应该
>
> 会得到有两个线性回归的模型，其中之一将小孩个数和是否带帐篷之 间的关系与泊松分布的系数联系在一起，另外一个将人数与*ψ*联系在 一起。对于后者，你可能需要一个逻辑回归作为逆连结函数。
>
> 3．使用鲁棒逻辑回归中的例子，将其输入到非鲁棒逻辑回归模 型中，检查异常值是否对结果有影响。你可能需要增加或者减少异常
>
> 值的个数，从而更好地理解一般的逻辑回归与鲁棒逻辑回归之间的区 别。
>
> 4．阅读并运行PyMC3中有关混合模型的例子（[https://pymc-](https://pymc-devs.github.io/pymc3/examples.html#mixture-models）。) [devs.github.io/pymc3/examples.html\#mixture-models）。](https://pymc-devs.github.io/pymc3/examples.html#mixture-models）。)
>
> 5．用一个混合模型对三类鸢尾花进行聚类，只使用两个特征， 同时假设你不知道正确的标签。你可能需要定义一个三元高斯分布，
>
> 你可以从一个简单的共享的协方差矩阵开始。
>
> 284