
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>附录 G：模型集成 &#8212; Python贝叶斯分析(中文)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="附录 F：贝叶斯深度学习编程初步" href="Append-06-BayesianDeepLearningPymc3.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Python贝叶斯分析(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   封面
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  书籍正文
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter01-ThinkingProbabilistically.html">
   第 1 章 概率思维
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter02-ProgrammingProbabilistically.html">
   第 2 章 概率编程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter03-ModellingwithLinearRegression.html">
   第 3 章 线性回归模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter04-GeneralizedLinearRegression.html">
   第 4 章 广义线性回归模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter05-ModelComparison.html">
   第 5 章 模型比较
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter06-MixtureModels.html">
   第 6 章 混合模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter07-GaussianProcesses.html">
   第 7 章 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter08-InterefenceEngine.html">
   第 8 章 推断引擎
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter09-WheretoGoNext.html">
   第 9 章 下一步去哪儿？
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  文献阅读
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Append-01-MCMC_Tutorial.html">
   附录 A： MCMC 推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-02-VariationalInference_Tutorial.html">
   附录 B： 变分法推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-03-GaussianProcessTutorial_01.html">
   附录 C： 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-04-BayesianNN_Tutorial.html">
   附录 D：贝叶斯神经网络
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-05-BayesianDeepLearning_Tutorial.html">
   附录 E：贝叶斯深度学习
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-06-BayesianDeepLearningPymc3.html">
   附录 F：贝叶斯深度学习编程初步
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   附录 G：模型集成
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/Append-07-ModelSelectAndAveraging.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Append-07-ModelSelectAndAveraging.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/BayesianAnalysiswithPython2nd"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/BayesianAnalysiswithPython2nd/issues/new?title=Issue%20on%20page%20%2FAppend-07-ModelSelectAndAveraging.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/xishansnow/BayesianAnalysiswithPython2nd/master?urlpath=lab/tree/Append-07-ModelSelectAndAveraging.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        <a class="jupyterhub-button" href="https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/xishansnow/BayesianAnalysiswithPython2nd&urlpath=lab/tree/BayesianAnalysiswithPython2nd/Append-07-ModelSelectAndAveraging.md&branch=master"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   1 为什么做模型集成？
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   2 贝叶斯模型平均
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     2.1 概述
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bma">
     2.2 经典 BMA
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id4">
       2.2.1 后验模型概率
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id5">
       2.2.2 先验的选择
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id6">
         （1）模型的先验
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#id7">
         （2）模型参数的先验
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     2.3 基于预测的 BMA
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dic-waic">
       2.3.1
       <code class="docutils literal notranslate">
        <span class="pre">
         DIC
        </span>
       </code>
       与
       <code class="docutils literal notranslate">
        <span class="pre">
         WAIC
        </span>
       </code>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id9">
       2.3.2 贝叶斯堆叠
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id10">
   3 频率主义模型平均
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     3.1 简介
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     3.2 点估计中的模型平均
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id13">
       3.2.1 信息准则法
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#bagging">
       3.2.2 装袋法（Bagging）
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id14">
       3.2.3 最优权重法
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     3.3 区间估计中的模型平均
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#wald">
       3.3.1
       <code class="docutils literal notranslate">
        <span class="pre">
         Wald
        </span>
       </code>
       区间
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id16">
       3.3.2 分位数自助区间
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id17">
       3.3.3 模型平均的尾部区间
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     3.4 讨论
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id19">
       3.4.1 尺度的选择
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id20">
       3.4.2 模型集的选择
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id21">
       3.4.3 置信区间
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id22">
       3.4.4 混合模型
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id23">
       3.4.5 缺失数据
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id24">
       3.4.6 模型权重求和
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id25">
   4 总结和未来方向
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id26">
     4.1 要点总结
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id27">
     4.2 未来方向
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="g">
<h1>附录 G：模型集成<a class="headerlink" href="#g" title="Permalink to this headline">¶</a></h1>
<p>【摘要】贝叶斯模型平均(贝叶斯模型平均) 提供了一种连贯且系统的机制来解释模型的不确定性。它可以看作是贝叶斯推理在模型选择、组合估计和预测问题上的直接应用。贝叶斯模型平均产生了一个简单的模型选择标准和风险较低的预测。然而，贝叶斯模型平均的应用并不总是直截了当的，导致对其不同方面的不同假设和情境选择。</p>
<p>【原文】Fletcher, David. Why Model Averaging? SpringerBriefs in Statistics. 2018.</p>
<style>p{text-indent:2em;2}</style>
<div class="section" id="id1">
<h2>1 为什么做模型集成？<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p>模型平均是一种在估计中允许模型不确定性的方法，它可以提供比模型选择更好的估计和更可靠的置信区间。其应用领域非常广泛，涉及经济、药理、气象、水文等诸多领域。选择模型平均方法的原因在于：</p>
<p><strong>（1）模型选择及点估计方法容易产生过于自信的模型。</strong></p>
<p>在许多经典统计推断的理论中，参数估计通常面向单个模型，该模型大多是从一组候选模型中选择出来的最佳模型。在解释数据时，选择该最佳模型的过程经常会被忽略，很容易导致点估计偏差和精度高估。这种现象被称为 “无声的丑闻”，而许多研究人员仍然没有意识到这个问题。</p>
<p><strong>（2）模型平均是一种可以考虑模型不确定性的估计方法。</strong></p>
<ul class="simple">
<li><p>在频率主义框架中，模型平均通常来自于每候选模型估计值的加权平均，其中权重反映了模型估计的潜力值。模型权重可能基于 <code class="docutils literal notranslate"><span class="pre">Akaike</span> <span class="pre">信息准则</span> <span class="pre">(AIC)</span></code>、<code class="docutils literal notranslate"><span class="pre">交叉验证</span></code>或感兴趣参数估计的<code class="docutils literal notranslate"><span class="pre">均方误差</span> <span class="pre">(MSE)</span></code>。</p></li>
<li><p>在贝叶斯框架中，模型权重要么是模型为真的后验概率，要么是基于预测样本来确定，例如 <code class="docutils literal notranslate"><span class="pre">Watanabe-Akaike</span> <span class="pre">信息准则</span> <span class="pre">(WAIC)</span></code> 或<code class="docutils literal notranslate"><span class="pre">交叉验证</span></code>。通常模型权重被限制在单位单纯形上，即非负且总和为 1。</p></li>
</ul>
<p><strong>（3）模型平均也可以被视为一种平衡估计偏差和方差的手段。</strong></p>
<p>从频率主义角度来看，就像模型选择一样，较小的模型通常会提供具有较大偏差的估计，而较大的模型会导致估计值具有较高的方差。因此，模型平均在计算置信区间时考虑了模型不确定性，从而产生了比模型选择方法更宽、更可靠的区间。有趣的是，频率主义的一些作者只专注于实现模型平均估计的偏差和方差之间的平衡，而其他人则认为模型平均仅仅是一个允许模型不确定性的手段而已。</p>
<blockquote>
<div><p><strong>模型选择、模型集成、模型组合和模型平均的区别：</strong></p>
<ul class="simple">
<li><p><strong>模型选择</strong>指从若干模型中选择出人们认为最优的那个模型，由于评判方法、数据集等原因，模型选择很容易造成对模型过高的自信。</p></li>
<li><p><strong>模型集成</strong>采用某种方式融合多个已经训练好的模型，进而达到 “取长补短”、提高模型泛化能力的效果。模型集成方法通常在几个模型差异性较大、相关性较小的情境下效果更加明显。常用的集成方法有：投票法(voting)、平均法(averaging)、 堆叠法(Stacking)、混合法(Blending)、状态法（Bagging）、自助法（Boosting）等。</p></li>
<li><p><strong>模型组合</strong>是一种极易与模型集成混淆的方法，因为它也会组合多个模型，但与集成方法不同之处在于，模型组合指在预测变量的不同子空间上选择使用不同模型。也就是说，组合方法中的每一个模型只作用在预测变量的部分空间上，而集成方法中的所有模型都会作用于整个预测变量空间。</p></li>
<li><p><strong>模型平均</strong>是最基础的模型集成方法之一，其基本原理是对多个模型计算的结果进行加权平均，以得到最终结果，在回归问题及含阈值调节的场景中使用更多一些。</p></li>
</ul>
</div></blockquote>
</div>
<div class="section" id="id2">
<h2>2 贝叶斯模型平均<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id3">
<h3>2.1 概述<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>原则上，贝叶斯模型平均 (BMA) 是非常自然的，因为用于数据分析的贝叶斯框架可以很容易地结合参数不确定性和模型不确定性 [73, 87]。我们不是基于单个模型计算参数的后验分布，而是计算来自不同模型的后验分布的加权组合。</p>
<p>在 BMA 的经典版本中，模型的权重是它为真的后验概率，因为我们假设其中一个模型为真。最近，[181] 提出了一个明确的基于预测的 BMA 版本。这使用交叉验证来确定一组最佳权重，并且是称为堆叠的频率论方法的贝叶斯版本（第 3.2.3 节）。 1</p>
<p>经典 BMA 关注模型一致性（真实模型的识别） ，而基于预测的 BMA 具有直接与估计相关的优势。我们最初关注经典 BMA，因为它在 BMA 文献中受到了绝大多数关注。</p>
</div>
<div class="section" id="bma">
<h3>2.2 经典 BMA<a class="headerlink" href="#bma" title="Permalink to this headline">¶</a></h3>
<p>Once the priors are specified for both the models and the parameters in each model, classical BMA involves a well-defined procedure for obtaining the model-averaged posterior for any parameter of interest. As with other Bayesian methods, the apparent simplicity of classical BMA can bely difficulties in its implementation <span class="math notranslate nohighlight">\([8,32]\)</span>. The choice of priors can be problematic, as can the computations required to obtain the model-averaged posterior.</p>
<p>Suppose <span class="math notranslate nohighlight">\(y=\left(y_{1}, \ldots, y_{n}\right)^{\top}\)</span> contains the values of the response variable, and we wish to average over a set of <span class="math notranslate nohighlight">\(M\)</span> nested models, with <span class="math notranslate nohighlight">\(\beta_{m}\)</span> being the vector of <span class="math notranslate nohighlight">\(p_{m}\)</span> parameters in model <span class="math notranslate nohighlight">\(m\)</span>. As the notation implies, <span class="math notranslate nohighlight">\(\beta_{m}\)</span> will often be a set of regression coefficients, but it may include other types of parameter, such as the error variance in a normal linear model. <span class="math notranslate nohighlight">\({ }^{2}\)</span> Suppose we are interested in estimating a scalar parameter <span class="math notranslate nohighlight">\(\theta\)</span>. The model-averaged posterior for <span class="math notranslate nohighlight">\(\theta\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y)=\sum_{m=1}^{M} p(m \mid y) p(\theta \mid y, m),
\]</div>
<p>where <span class="math notranslate nohighlight">\(p(m \mid y)\)</span> is the posterior probability that model <span class="math notranslate nohighlight">\(m\)</span> is true, and <span class="math notranslate nohighlight">\(p(\theta \mid y, m)\)</span> is the posterior for <span class="math notranslate nohighlight">\(\theta\)</span> when we assume model <span class="math notranslate nohighlight">\(m\)</span> is true. <span class="math notranslate nohighlight">\({ }^{3}\)</span> Thus <span class="math notranslate nohighlight">\(p(\theta \mid y)\)</span> is a weighted combination of the posterior distributions obtained from the different models, the weights being the posterior model probabilities. Using Bayes’ theorem, the posterior probability for model <span class="math notranslate nohighlight">\(m\)</span> is given by</p>
<div class="math notranslate nohighlight">
\[
p(m \mid y) \propto p(m) p(y \mid m),
\]</div>
<p>where <span class="math notranslate nohighlight">\(p(m)\)</span> and <span class="math notranslate nohighlight">\(p(y \mid m)\)</span> are the prior probability, and the marginal (integrated) likelihood, for model <span class="math notranslate nohighlight">\(m\)</span>, with</p>
<div class="math notranslate nohighlight">
\[
p(y \mid m)=\int p\left(y \mid \beta_{m}, m\right) p\left(\beta_{m} \mid m\right) d \beta_{m},
\]</div>
<p>where <span class="math notranslate nohighlight">\(p\left(\beta_{m} \mid m\right)\)</span> is the prior for <span class="math notranslate nohighlight">\(\beta_{m}\)</span> and <span class="math notranslate nohighlight">\(p\left(y \mid \beta_{m}, m\right)\)</span> is the likelihood under model <span class="math notranslate nohighlight">\(m\)</span>. When the support of <span class="math notranslate nohighlight">\(\beta_{m}\)</span> is discrete, the integral in (2.3) is replaced by a summation.</p>
<p>The posterior probability for model <span class="math notranslate nohighlight">\(m\)</span> can also be written as</p>
<div class="math notranslate nohighlight">
\[
p(m \mid y) \propto p(m) B_{m},
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
B_{m}=\frac{p(y \mid m)}{p(y \mid 1)}
\]</div>
<p>is the Bayes factor for comparing model <span class="math notranslate nohighlight">\(m\)</span> and model 1 , the latter being an arbitrary reference model <span class="math notranslate nohighlight">\([114,146]\)</span>. It is well known that Bayes factors can be sensitive to the prior distributions for the parameters, even when <span class="math notranslate nohighlight">\(n\)</span> is large <span class="math notranslate nohighlight">\([8,97]\)</span>. An extreme case arises when one or more of the priors is improper, as this can lead to the marginal likelihood in (2.3) not being well defined [73, 161].</p>
<p>Two natural summaries of the model-averaged posterior for <span class="math notranslate nohighlight">\(\theta\)</span> are the mean and variance, given by</p>
<div class="math notranslate nohighlight">
\[
\mathrm{E}(\theta \mid y)=\sum_{m=1}^{M} p(m \mid y) \mathrm{E}(\theta \mid y, m)
\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[
\operatorname{var}(\theta \mid y)=\sum_{m=1}^{M} p(m \mid y)\left[\operatorname{var}(\theta \mid y, m)+\{\mathrm{E}(\theta \mid y, m)-\mathrm{E}(\theta \mid y)\}^{2}\right]
\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathrm{E}(\theta \mid y, m)\)</span> and <span class="math notranslate nohighlight">\(\operatorname{var}(\theta \mid y, m)\)</span> are the posterior mean and variance for <span class="math notranslate nohighlight">\(\theta\)</span> under model <span class="math notranslate nohighlight">\(m\)</span>. Thus the model-averaged posterior variance is influenced by both the parameter uncertainty associated with each model and the between-model variability in the posterior mean.</p>
<p>We can also use the model-averaged posterior to calculate a central <span class="math notranslate nohighlight">\(100(1-2 \alpha) \%\)</span> credible interval for <span class="math notranslate nohighlight">\(\theta\)</span>, given by <span class="math notranslate nohighlight">\(\left[\theta_{L}, \theta_{U}\right]\)</span>, where</p>
<div class="math notranslate nohighlight">
\[
\int_{-\infty}^{\theta_{L}} p(\theta \mid y) d \theta=\int_{\theta_{U}}^{\infty} p(\theta \mid y) d \theta=\alpha .
\]</div>
<p>An alternative choice would be to use the highest posterior density <span class="math notranslate nohighlight">\(100(1-2 \alpha) \%\)</span> credible region, i.e. the region of values for <span class="math notranslate nohighlight">\(\theta\)</span> that contain <span class="math notranslate nohighlight">\(100(1-2 \alpha) \%\)</span> of the posterior probability and for which the posterior density is never lower than outside the region. However, the central credible interval is easier to compute than the highest posterior density region, and has the advantage that the limits can be interpreted as quantiles of the posterior. In the examples we therefore use central credible intervals.</p>
<p>A by-product of classical BMA is the ability to calculate a posterior inclusionprobability (PIP) for each predictor variable, i.e. the sum of the posterior probabilities for all the models that include that variable <span class="math notranslate nohighlight">\([7,12,36]\)</span>. Some authors have suggested that these provide a useful summary of the relative importance of each predictor variable. However, as they are influenced by the choice of model set, the importance of a predictor variable can be exaggerated by including many models containing that variable [66]. In addition, a more useful summary of relative importance can be obtained by comparing model-averaged posterior distributions for the expected value of the response variable for a suitable set of values of the predictor variables (Sect. 1.4). An analogous issue arises with the use of summed model weights in frequentist model averaging (Sect. 3.7). <span class="math notranslate nohighlight">\({ }^{4}\)</span></p>
<div class="section" id="id4">
<h4>2.2.1 后验模型概率<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>When calculating the posterior model probabilities, it can be difficult to determine the marginal likelihood in (2.3). In some settings, such as GLMs with conjugate priors for the parameters, the marginal likelihood can be expressed analytically [98], but in general we need to use an approximation. The most well-known approximation, which does not require specification of the priors for the parameters, is</p>
<div class="math notranslate nohighlight">
\[
p(y \mid m) \approx \exp \left(-\mathrm{BIC}_{m} / 2\right)
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathrm{BIC}_{m}=-2 \log p\left(y \mid \widehat{\beta}_{m}, m\right)+p_{m} \log n,
\]</div>
<p>and <span class="math notranslate nohighlight">\(\widehat{\beta}_{m}\)</span> is the maximum likelihood estimate of <span class="math notranslate nohighlight">\(\beta_{m}\)</span>. The expression in (2.8) is the Bayesian information criterion for model <span class="math notranslate nohighlight">\(m[91,98,114,157] .{ }^{5}\)</span> The first term in (2.8) is influenced by the fit of the model, while the second can be thought of as a correction for overfitting which penalises more complex models.</p>
<p>Use of (2.7) in (2.2) leads to the approximation</p>
<div class="math notranslate nohighlight">
\[
p(m \mid y) \propto p(m) \exp \left(-\mathrm{BIC}_{m} / 2\right),
\]</div>
<p>which is sometimes referred to as the generalised BIC weight [114]. When we calculate the expression on the right-hand side of (2.9), <span class="math notranslate nohighlight">\(\mathrm{BIC}_{m}\)</span> is often replaced by</p>
<div class="math notranslate nohighlight">
\[
\mathrm{BIC}_{m}-\min _{k} \mathrm{BIC}_{k},
\]</div>
<p>in order to avoid large arguments in the exponential function [32]. <span class="math notranslate nohighlight">\({ }^{6}\)</span></p>
<p>Care is needed in specifying the value of <span class="math notranslate nohighlight">\(n\)</span> in (2.8). For a normal linear model, it is simply the number of observations. For a binomial model it is the total number of Bernoulli trials. When using a log-linear model to analyse a contingency table it is the sum of the counts rather than the number of cells in the table <span class="math notranslate nohighlight">\([98,145]\)</span>. In the context of survival analysis, [171] suggested setting <span class="math notranslate nohighlight">\(n\)</span> equal to the number of uncensored observations. For a hierarchical model, the choice of <span class="math notranslate nohighlight">\(n\)</span> depends on the focus of the analysis <span class="math notranslate nohighlight">\([37,98,138,139,188]\)</span>. For modelling survey data, [117] proposed a version of BIC that takes into account the design effect.</p>
<p>A higher-order approximation to the posterior model probabilities requires the priors for the parameters and their observed Fisher information matrix <span class="math notranslate nohighlight">\([96,98,116\)</span>, 183]. It is similar in spirit to TIC, Takeuchi’s information criterion, which has been suggested as an alternative to <span class="math notranslate nohighlight">\(\mathrm{AIC}\)</span> in the frequentist setting [32] (Sect.3.2.1).</p>
<p>Other approaches to approximating the posterior model probabilities have been proposed, involving marginal likelihoods <span class="math notranslate nohighlight">\([26,28,40,41,121,132,151]\)</span> or Markov chain Monte Carlo (MCMC) methods [5, 9, 19, 23, 24, 29, 34, 37, 50, 71, 74, <span class="math notranslate nohighlight">\(75,79,82,121,142,147,179]\)</span>. One conceptually-appealing method is reversiblejump MCMC (RJMCMC), in which we sample the parameter-space and model-space simultaneously [80]. However, RJMCMC can be prone to performance issues and be challenging to implement <span class="math notranslate nohighlight">\([8,9,37]\)</span>, to the extent that <span class="math notranslate nohighlight">\([83]\)</span> recommend use of the approximation in (2.9). Recently, [8] have developed an approach which has the advantage of using the MCMC output obtained from fitting each model separately, and which exploits the relationships between parameters from different models [151].</p>
</div>
<div class="section" id="id5">
<h4>2.2.2 先验的选择<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id6">
<h5>（1）模型的先验<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h5>
<p>A natural and common choice for the prior model probabilities is the uniform prior, in which <span class="math notranslate nohighlight">\(p(m)=1 / M\)</span>. The approximation to the posterior model probability in (2.9) then simplifies to the well-known BIC weight, given by</p>
<div class="math notranslate nohighlight">
\[
p(m \mid y) \propto \exp \left(-\mathrm{BIC}_{m} / 2\right) .
\]</div>
<p>However, use of the uniform model-prior can have hidden implications. For example, if some of the possible predictor variables are highly correlated, we may have modelredundancy, in that some models will provide very similar estimates of <span class="math notranslate nohighlight">\(\theta\)</span>. Use of a uniform prior will then dilute the prior probability allocated to any model which is not similar to the others <span class="math notranslate nohighlight">\([18,57,66,76]\)</span>. A method for dealing with this problem was proposed by [170], who suggested specifying prior model probabilities using the concept of the worth of a model, which is based on quantifying what we would expect to lose if we removed it from the model set when it is the true model. Another alternative is use of a Bernoulli prior in which each predictor variable has the same probability <span class="math notranslate nohighlight">\(p\)</span> of being included, independently of the others. The uniform prior is a special case, with <span class="math notranslate nohighlight">\(p=0.5\)</span>, and therefore corresponds to a prior expectation that half the predictor variables will be included [37]. In order to have a less informative prior on model size, we might use a beta-prior for <span class="math notranslate nohighlight">\(p[37,105,158]\)</span>.</p>
<p>Other approaches to specifying the model-prior involve empirical Bayes [37]; allowance for predictor variables being related, such as when some of the models include interaction terms [30]; and use of lower weights for models that are similar to others [66].</p>
<p>In order to allow for the possibility that the choice of model-prior may affect the form of the model-averaged posterior, [43] proposed use of credal model averaging, in which more than one model-prior is considered. This effectively allows one to preform a sensitivity analysis, in order to assess the extent to which the modelaveraged posterior is influenced by the choice of model-prior. Further examples of the use of credal model averaging can be found in [44-46, 185].</p>
</div>
<div class="section" id="id7">
<h5>（2）模型参数的先验<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h5>
<p>The posterior model probabilities can be sensitive to the choice of prior distribution for the parameters in a model, even if this prior would be regarded as non-informative in the single-model setting <span class="math notranslate nohighlight">\([8,35,93]\)</span>. In particular, as mentioned in Sect. <span class="math notranslate nohighlight">\(2.2\)</span>, the use of improper priors can lead to the Bayes factors, and hence the posterior model probabilities, not being well defined <span class="math notranslate nohighlight">\([10,82,93,161]\)</span>. It is also possible for apparently sensible priors for the parameters to cause the models to have conflicting implicit prior distributions for <span class="math notranslate nohighlight">\(\theta\)</span> [85].</p>
<p>In the normal linear model setting, Zellner’s g-prior has been used extensively, as it has several desirable properties, including computational convenience <span class="math notranslate nohighlight">\([37,154]\)</span>. It involves centring the predictor variables at zero, in order to remove any dependence between the intercept and the regression coefficients, and then specifying a joint prior for the intercept and error variance, plus a joint prior for the regression coefficients given the error variance. For model <span class="math notranslate nohighlight">\(m\)</span>, this leads to</p>
<div class="math notranslate nohighlight">
\[
p\left(\beta_{m 0}, \sigma_{m}^{2} \mid m\right) \propto 1 / \sigma_{m}^{2}
\]</div>
<p>and a multivariate normal prior for the regression coefficients, with mean zero and covariance matrix
$<span class="math notranslate nohighlight">\(
g_{m} \sigma_{m}^{2}\left(X_{m}^{\top} X_{m}\right)^{-1},
\)</span>$</p>
<p>where <span class="math notranslate nohighlight">\(\beta_{m 0}, \sigma_{m}^{2}\)</span>, and <span class="math notranslate nohighlight">\(X_{m}\)</span> are the intercept, error variance and design matrix for model <span class="math notranslate nohighlight">\(m\)</span> respectively, and <span class="math notranslate nohighlight">\(g_{m}\)</span> is a hyperparameter <span class="math notranslate nohighlight">\([37,187]\)</span>. This prior has a nice interpretation, as it can be thought of as containing <span class="math notranslate nohighlight">\(1 / g_{m}\)</span> as much information as that in the data. The resulting posterior model probability is given by</p>
<div class="math notranslate nohighlight">
\[
p(m \mid y) \propto p(m) \exp \left(-\mathrm{IC}_{m} / 2\right),
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathrm{IC}_{m}=-2 \log p\left(y \mid \widehat{\beta}_{m}, m\right)+p_{m} \log g_{m} .
\]</div>
<p><span class="math notranslate nohighlight">\(\mathrm{IC}_{m}\)</span> can be thought of as a generalised information criterion in which the correction for overfitting is <span class="math notranslate nohighlight">\(p_{m} \log g_{m}\)</span>. Setting <span class="math notranslate nohighlight">\(g_{m}\)</span> to be arbitrarily large, in order for the prior to be non-informative, can lead to strongly favouring the null model, an example of the Lindley-Jeffreys paradox <span class="math notranslate nohighlight">\([37,91,98,113]\)</span>. Using <span class="math notranslate nohighlight">\(g_{m}=n\)</span> gives the unit-information prior, which contains the same amount of information as a single observation, and leads to the posterior model probability being the generalised BIC weight in (2.9). Together with a uniform model-prior, this corresponds to using the BIC weight in (2.10), which was found perform well in a simulation study reported by [58]. An empirical Bayes procedure is also possible, in which the choice of <span class="math notranslate nohighlight">\(g_{m}\)</span> depends on the data [37]. As with the parameter <span class="math notranslate nohighlight">\(p\)</span> in the Bernoulli model-prior, we might also want to put a prior on <span class="math notranslate nohighlight">\(g_{m}\)</span>, rather than specify its value <span class="math notranslate nohighlight">\([105,109,111,162,186] .\)</span> A version of the g-prior for high-dimensional normal linear models was proposed by [120].</p>
<p>For GLMs, [146] considered several approximations to the Bayes factors, including one that leads to the generalised BIC weight in (2.9). Extensions of Zellner’s g-prior to this setting, including use of a prior on <span class="math notranslate nohighlight">\(g_{m}\)</span>, have been suggested by several authors; see [154] and the references therein. A calibrated information criterion (CIC) prior was proposed by [35]. This is based on the Jeffreys prior used in the single-model setting [91], and for model <span class="math notranslate nohighlight">\(m\)</span> it is given by</p>
<div class="math notranslate nohighlight">
\[
p\left(\beta_{m} \mid m\right)=(2 \pi)^{-p_{m} / 2}\left|c_{m}^{-1} J\right|^{1 / 2}
\]</div>
<p>where <span class="math notranslate nohighlight">\(J\)</span> is the observed Fisher information matrix for <span class="math notranslate nohighlight">\(\beta_{m}\)</span> and <span class="math notranslate nohighlight">\(c_{m}\)</span> is a hyperparameter. In conjunction with a uniform model-prior, this leads to the model-averaged posterior for <span class="math notranslate nohighlight">\(\theta\)</span> being approximated by a multivariate normal distribution with mean <span class="math notranslate nohighlight">\(\widehat{\beta}_{m}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(J^{-1}\)</span>. In addition, the posterior probability for model <span class="math notranslate nohighlight">\(m\)</span> is approximated by</p>
<div class="math notranslate nohighlight">
\[
p(m \mid y) \propto \exp \left(-\mathrm{CIC}_{m} / 2\right)
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\mathrm{CIC}_{m}=-2 \log p\left(y \mid \widehat{\beta}_{m}, m\right)+p_{m} \log c_{m},
\]</div>
<p>which has the same form as (2.11). The right-hand side of (2.12) is known as the CIC weight; the BIC weight in <span class="math notranslate nohighlight">\((2.10)\)</span> is a special case, corresponding to <span class="math notranslate nohighlight">\(c_{m}=n\)</span>.</p>
</div>
</div>
</div>
<div class="section" id="id8">
<h3>2.3 基于预测的 BMA<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>As mentioned in Sect. 2.1, classical BMA focusses attention on identification of the true model. Recently, several authors have considered use of prediction-based BMA <span class="math notranslate nohighlight">\([39,102,181]\)</span>. In addition to being a more natural approach to model averaging, this has the distinct advantages of not requiring a prior for the models, being less sensitive to the priors for the parameters, and only requiring the usual MCMC output for each individual model.</p>
<p>There are currently two types of prediction-based BMA. The first involves a criterion based on a measure of the within-sample prediction error plus a correction term which allows for overfitting. The second uses cross validation and is therefore based on the error associated with prediction of observation <span class="math notranslate nohighlight">\(i\)</span> having fitted the model to all the data except that observation <span class="math notranslate nohighlight">\((i=1, \ldots, n)\)</span>. The only difference between these approaches and classical BMA is that we combine posterior distributions using model weights that are not posterior model probabilities.</p>
<div class="section" id="dic-waic">
<h4>2.3.1 <code class="docutils literal notranslate"><span class="pre">DIC</span></code> 与 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code><a class="headerlink" href="#dic-waic" title="Permalink to this headline">¶</a></h4>
<p>In Bayesian model selection, the deviance information criterion (DIC) has long been used as an alternative to <span class="math notranslate nohighlight">\(\mathrm{BIC}[159,160]\)</span>. For model averaging, <span class="math notranslate nohighlight">\([15]\)</span> suggested use of DIC weights, with <span class="math notranslate nohighlight">\(\mathrm{BIC}_{m}\)</span> in (2.10) being replaced by
$<span class="math notranslate nohighlight">\(
\mathrm{DIC}_{m}=-2 \log p\left(y \mid \widehat{\beta}_{m}, m\right)+2 p_{m}^{D I C},
\)</span><span class="math notranslate nohighlight">\(
where \)</span>\widehat{\beta}<em>{m}<span class="math notranslate nohighlight">\( is a point estimate of \)</span>\beta</em>{m}<span class="math notranslate nohighlight">\( and \)</span>p_{m}^{D I C}<span class="math notranslate nohighlight">\( is a correction for overfitting, often referred to as the effective number of parameters [159]. Common choices are
\)</span><span class="math notranslate nohighlight">\(
\widehat{\beta}_{m}=\mathrm{E}\left(\beta_{m} \mid y, m\right)
\)</span><span class="math notranslate nohighlight">\(
and
\)</span><span class="math notranslate nohighlight">\(
p_{m}^{D I C}=2 \operatorname{var}\left\{\log p\left(y \mid \beta_{m}\right)\right\} .
\)</span><span class="math notranslate nohighlight">\(
The posterior mean in (2.14) and posterior variance in (2.15) are estimated by the mean of the posterior MCMC sample for \)</span>\beta_{m}<span class="math notranslate nohighlight">\( and the variance of the posterior MCMC sample for \)</span>\log p\left(y \mid \beta_{m}\right)<span class="math notranslate nohighlight">\( respectively. An alternative choice for \)</span>p_{m}^{D I C}<span class="math notranslate nohighlight">\( is possible \)</span>[22,<span class="math notranslate nohighlight">\(, \)</span>73,159]$, but this has the disadvantage of sometimes being negative.</p>
<p>DIC has much in common with AIC, which is also a prediction-based criterion (Sect. <span class="math notranslate nohighlight">\(3.2 .1\)</span> and [32]). DIC model weights have been used in a range of applications, including ecology <span class="math notranslate nohighlight">\([60,62,115,127,167]\)</span>, fisheries <span class="math notranslate nohighlight">\([92,177]\)</span>, medicine <span class="math notranslate nohighlight">\([143]\)</span> and physics [112].</p>
<p>The other prediction-based measure we consider is the Watanabe-Akaike Information Criterion (WAIC) <span class="math notranslate nohighlight">\([72,73,89,169,174] .{ }^{7}\)</span> This is more Bayesian than DIC (and BIC), in that it replaces <span class="math notranslate nohighlight">\(\widehat{\beta}_{m}\)</span> by the posterior distribution for <span class="math notranslate nohighlight">\(\beta_{m}\)</span>, and can work well in situations where DIC has problems [22]. The point estimate <span class="math notranslate nohighlight">\(\widehat{\beta}_{m}\)</span> in DIC leads to underestimation of the prediction uncertainty, and hence to the possibility that use of DIC will lead to overfitting. <span class="math notranslate nohighlight">\({ }^{8}\)</span> WAIC is also specified in terms of the pointwise predictive densities <span class="math notranslate nohighlight">\(p\left(y_{i} \mid \beta_{m}\right)\)</span>, rather than the joint predictive density <span class="math notranslate nohighlight">\(p\left(y \mid \beta_{m}\right)\)</span>, as the former has a close connection with cross validation [72] (Sect. 2.3.2). If the <span class="math notranslate nohighlight">\(y_{i}\)</span> are independent given the parameters, use of the joint density is equivalent to the pointwise-approach.</p>
<p>The value of WAIC for model <span class="math notranslate nohighlight">\(m\)</span> is given by
$<span class="math notranslate nohighlight">\(
\mathrm{WAIC}_{m}=-2 \sum_{i=1}^{n} \log p\left(y_{i} \mid y, m\right)+2 p_{m}^{\text {WAIC }},
\)</span><span class="math notranslate nohighlight">\(
where \)</span>p_{m}^{\text {WAIC }}<span class="math notranslate nohighlight">\( is again a correction for overfitting. The posterior predictive density in (2.16) is given by
\)</span><span class="math notranslate nohighlight">\(
p\left(y_{i} \mid y, m\right)=\int p\left(y_{i} \mid \beta_{m}, y, m\right) p\left(\beta_{m} \mid y, m\right) d \beta_{m}=\mathrm{E}\left\{p\left(y_{i} \mid \beta_{m}, y, m\right)\right\},
\)</span><span class="math notranslate nohighlight">\(
One choice for the correction term is
\)</span><span class="math notranslate nohighlight">\(
p_{m}^{\text {WAIC }}=\sum_{i=1}^{n} \operatorname{var}\left\{\log p\left(y_{i} \mid \beta_{m}, y, m\right)\right\}
\)</span><span class="math notranslate nohighlight">\(
As with DIC, the posterior mean in \)</span>(2.17)<span class="math notranslate nohighlight">\( and the posterior variance in (2.18) can be estimated by the mean of the posterior MCMC sample for \)</span>p\left(y_{i} \mid \beta_{m}, y, m\right)<span class="math notranslate nohighlight">\( and the variance of the posterior MCMC sample for \)</span>\log p\left(y_{i} \mid \beta_{m}, y, m\right)<span class="math notranslate nohighlight">\(. As with DIC, an alternative choice for \)</span>p_{m}^{\text {WAIC }}<span class="math notranslate nohighlight">\( is possible \)</span>[72,73]$; we consider that in (2.18) as it is closely related to leave-one-out cross validation (Sect. 2.3.2).</p>
<p>WAIC weights can be calculated using (2.10), with <span class="math notranslate nohighlight">\(\mathrm{BIC}_{m}\)</span> replaced by <span class="math notranslate nohighlight">\(\mathrm{WAIC}_{m} .\)</span> As DIC and WAIC are focussed on prediction, we would expect weights based on these criteria to be preferable to BIC weights, which are more focussed on identification of a true model [72]. As WAIC is more Bayesian than DIC, WAIC weights are based on a more reliable assessment of the prediction-uncertainty associated with each model. WAIC is also invariant to transformation of the parameters, whereas DIC will not be if we use (2.13), as the posterior mean is not transformation-invariant [159]. <span class="math notranslate nohighlight">\({ }^{9}\)</span> In addition, use of a pointwise-approach means that <span class="math notranslate nohighlight">\(p_{m}^{W A I C}\)</span> will be more stable than <span class="math notranslate nohighlight">\(p_{m}^{D I C}[73] .\)</span></p>
<p>As with BIC, when assessing the fit of a hierarchical model the exact form of DIC and WAIC will depend upon the focus of the analysis, as this will determine what we mean by prediction of a new observation [72, 122]; a similar issues arises when using AIC in the frequentist setting (Sect. 3.6.4).</p>
</div>
<div class="section" id="id9">
<h4>2.3.2 贝叶斯堆叠<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
<p>Stacking is a cross-validation-based approach to model averaging that has a long history in the frequentist setting [164] (Sect.3.2.3). Like the frequentist version, Bayesian stacking [181] uses a measure of out-of-sample prediction error, which does not require a correction for overfitting. If a logarithmic scoring rule is used to summarise the prediction performance <span class="math notranslate nohighlight">\([78,137]\)</span>, the model weights are chosen to be those that maximise the function
$<span class="math notranslate nohighlight">\(
\sum_{i=1}^{n} \log \sum_{m=1}^{M} w_{m} p\left(y_{i} \mid y_{-i}, m\right)
\)</span><span class="math notranslate nohighlight">\(
where \)</span>w_{m}<span class="math notranslate nohighlight">\( is the weight associated with model \)</span>m<span class="math notranslate nohighlight">\( and \)</span>y_{-i}<span class="math notranslate nohighlight">\( is the response vector \)</span>y<span class="math notranslate nohighlight">\( with \)</span>y_{i}<span class="math notranslate nohighlight">\( removed. \)</span>{ }^{10}<span class="math notranslate nohighlight">\( In order to maximise \)</span>(2.19)<span class="math notranslate nohighlight">\( using weights that lie on the unit simplex, we can use a constrained-optimisation method, such as quadratic programming [83]. Following [181], we refer to this approach as Bayesian stacking of predictive distributions (BSP). \)</span>{ }^{11}<span class="math notranslate nohighlight">\( Analogous to the form of the posterior predictive density used to calculate \)</span>\mathrm{WAIC}<em>{m}<span class="math notranslate nohighlight">\( in (2.16) (Sect. 2.3.1), we have
\)</span><span class="math notranslate nohighlight">\(
p\left(y_{i} \mid y_{-i}, m\right)=\int p\left(y_{i} \mid \beta_{m}, y_{-i}, m\right) p\left(\beta_{m} \mid y_{-i}, m\right) d \beta_{m}=\mathrm{E}\left\{p\left(y_{i} \mid \beta_{m}, y_{-i}, m\right)\right\},
\)</span><span class="math notranslate nohighlight">\(
where the posterior mean on the right-hand side is now with respect to \)</span>p\left(\beta</em>{m} \mid y_{-i}, m\right)<span class="math notranslate nohighlight">\(, and can be estimated by the mean of the corresponding posterior MCMC sample for \)</span>p\left(y_{i} \mid \beta_{m}, y_{-i}, m\right) .$</p>
<p>As computational effort will often be an important consideration in the Bayesian setting, [181] proposed use of Pareto-smoothed importance sampling [168], which only requires a single fit to the data for each model. On the other hand, if the sample size is small estimation of the weights may be unstable [181], an example of which arises in the toxicity example (Sect. 2.4.2).</p>
<p>Determining posterior model weights by minimising an objective function has also been suggested by <span class="math notranslate nohighlight">\([81,172] .\)</span> Likewise, in the context of forecasting in economic time series, <span class="math notranslate nohighlight">\([59,63]\)</span> have proposed using an estimate of out-of-sample prediction error to determine model weights. A decision-theoretic approach to BMA, also based on prediction error, was used by [16] in the context of high-dimensional multivariate regression models.</p>
<p>When <span class="math notranslate nohighlight">\(n\)</span> is large, BSP might be expected to produce weights that are similar to those based on WAIC, as the latter is asymptotically equivalent to use of Bayesian leave-one-out cross validation for model selection [174]. A discussion of the relative merits of DIC, WAIC and Bayesian cross validation can be found in [72].</p>
<p>In related work, interpretation of a model-averaged posterior as a mixture distribution has been advocated by [94]; see also [181]. <span class="math notranslate nohighlight">\({ }^{12}\)</span> As with the approach of [172], this leads to improper priors for the model parameters being acceptable.</p>
<p>Use of BSP can be motivated by the fact that classical BMA has been shown to have poorer prediction performance than frequentist stacking, particularly when the true model is not in the model set [33, 53, 178, 181]. Use of classical BMA leads to an asymptotic weight of one for the model closest to the true data-generating mechanism (in terms of Kullback-Leibler divergence). In contrast, BSP finds the optimal combination of predictive distributions that is closest to the data-generating mechanism (in terms of the scoring rule), and the asymptotic BSP weights can all be less than one [181]. A similar motivation led to the idea of Bayesian model combination in the machine-learning literature [100, 124, 126]. If one of the models is a good approximation to the data-generating mechanism, BSP may not perform as well as classical BMA when n is small.</p>
</div>
</div>
</div>
<div class="section" id="id10">
<h2>3 频率主义模型平均<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id11">
<h3>3.1 简介<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="id12">
<h3>3.2 点估计中的模型平均<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id13">
<h4>3.2.1 信息准则法<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="bagging">
<h4>3.2.2 装袋法（Bagging）<a class="headerlink" href="#bagging" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id14">
<h4>3.2.3 最优权重法<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h4>
</div>
</div>
<div class="section" id="id15">
<h3>3.3 区间估计中的模型平均<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<div class="section" id="wald">
<h4>3.3.1 <code class="docutils literal notranslate"><span class="pre">Wald</span></code> 区间<a class="headerlink" href="#wald" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id16">
<h4>3.3.2 分位数自助区间<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id17">
<h4>3.3.3 模型平均的尾部区间<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h4>
</div>
</div>
<div class="section" id="id18">
<h3>3.4 讨论<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id19">
<h4>3.4.1 尺度的选择<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id20">
<h4>3.4.2 模型集的选择<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id21">
<h4>3.4.3 置信区间<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id22">
<h4>3.4.4 混合模型<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id23">
<h4>3.4.5 缺失数据<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id24">
<h4>3.4.6 模型权重求和<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h4>
</div>
</div>
</div>
<div class="section" id="id25">
<h2>4 总结和未来方向<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id26">
<h3>4.1 要点总结<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h3>
<p><strong>（1）估计而不是识别</strong></p>
<p>模型平均是一种估计工具，因此与识别真实模型没有直接关系。在许多设置中，评估该估计对模型选择的敏感性也将很有用。</p>
<p><strong>（2）感兴趣的参数</strong></p>
<p>这应该在所有模型中具有相同的解释，因此回归系数的平均不太可能相关。</p>
<p><strong>（3）模型冗余</strong></p>
<p>如果在经典 BMA 中使用统一的模型先验，模型冗余会导致一些先验模型概率的稀释。这个问题在基于预测的 BMA 中不会出现，因为它不需要模型先验。同样，在 FMA 模型中，冗余会导致 AIC 权重被稀释。 AIC(w) 和堆叠中使用的单纯形约束缓解了这个问题。</p>
<p><strong>（4）区间估计</strong></p>
<p>目前没有区间能保证良好的覆盖，除非它等于最大模型的区间。 MATA 区间的优点是它可以基于任何计算单模型置信区间的方法。</p>
<p><strong>（5）尺度的选择</strong></p>
<p>BMA 是变换不变的，而 FMA 不是。然而，通常有一个自然尺度来执行 FMA，例如 GLM 中的线性预测尺度</p>
<p><strong>（6）对模型权重求和</strong></p>
<p>对模型权重求和不能提供预测变量重要性的有用度量；对预测变量的特定值的模型平均估计值进行比较是可取的。类似的评论适用于 BMA 中的后验包含概率。</p>
<p><strong>（7）混合模型</strong></p>
<p>AIC(w) 和堆叠的混合模型版本是可能的，WAIC 和 BSP 的分层版本也是如此。</p>
<p><strong>（8）模型集的选择</strong></p>
<p>使用一组单例模型，每个模型都涉及一个预测器，当有许多预测器时，这似乎是一种很有前途的方法。</p>
<p><strong>（9）贝叶斯学派的选择</strong></p>
<p>使用 WAIC 或 BSP 权重比经典 BMA 更可取，因为重点是预测而不是识别真实模型。使用这些权重还避免了后验模型概率的计算以及这些概率对参数先验的敏感性的问题。</p>
<p><strong>（10）频率学派的选择</strong></p>
<p>如果计算量不是问题，堆叠是一个不错的选择。 AIC(w) 是大 n 的一个很好的替代方案，但鲁棒性较差。这两种方法在元模型方面也有很好的解释</p>
</div>
<div class="section" id="id27">
<h3>4.2 未来方向<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h3>
<p><strong>（1）置信区间</strong></p>
<p>需要研究计算模型平均置信区间的最佳方法。这可能涉及在区间覆盖和宽度方面最佳的权重，如贝叶斯设置中的 [2] 所建议的那样。还需要评估何时仅使用最大模型的置信区间最佳。</p>
<p><strong>（2）置信分布</strong></p>
<p>BMA 的一个明显优势是使用后验分布来总结结果。在 FMA 中，拥有置信分布的模型平均版本会很有用，它提供了所有可能置信区间的摘要 [1]。</p>
<p><strong>（3）WAIC 和 BSP 的替代版本</strong></p>
<p>WAIC 的加权版本，类似于 FMA 中的 AIC(w)，出于同样的原因，AIC(w) 权重似乎比基于 AIC 的权重更可取。同样，对于 GLM，一个 BSP 版本可能有用，它涉及对每个模型的线性预测器进行加权，类似于 FMA 中的堆叠。</p>
<p><strong>（4）与收缩的比较</strong></p>
<p>最好进行模拟研究，将模型平均方法（例如 AIC(w) 和堆叠）与收缩方法（例如 lasso）进行比较。</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="Append-06-BayesianDeepLearningPymc3.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">附录 F：贝叶斯深度学习编程初步</p>
        </div>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Osvaldo Martin<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>