
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>第7章 高斯过程 &#8212; 用Python做贝叶斯分析</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="第6章 混合模型" href="chapter06-MixtureModels.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">用Python做贝叶斯分析</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   前言
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter01-ThinkingProbabilistically.html">
   第1章 概率思维
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter02-ProgrammingProbabilistically.html">
   第2章　概率编程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter03-ModellingwithLinearRegression.html">
   第3章　利用线性回归模型理解并预测数据
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter04-GeneralizedLinearRegression.html">
   第4章 利用Logistic 回归 对结果进行分类
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter05-ModelComparison.html">
   第
   <strong>
    5
   </strong>
   章　模型比较
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter06-MixtureModels.html">
   第6章　混合模型
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   第7章　高斯过程
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/chapter07-GaussianProcesses.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1>第7章　高斯过程<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<p>目前为止我们见过的所有模型都是参数化的模型，这些模型包含 固定个数的参数。此外还有一类模型称为非参模型，在非参模型中， 参数的个数是随着数据大小变化的，换句话说，模型中潜在着无限多 个参数，我们通过某种方式将其减少到某个值，从而正好能够用来描 述全部数据。这一章中，我们将学习核函数的概念，以及如何从核函 数的角度重新思考问题。高斯分布是统计学中的核心内容，不仅仅是 对传统的方法，对于贝叶斯统计学和机器学习也是如此。我们将以高 斯分布为例子，探讨如何将高斯模型的概念扩展到无限多维，并学习 这类描述函数的概率分布。尽管这么做一开始看起来有点奇怪，不过 好处是可以通过参数化的核函数来推断函数。</p>
<p>本章我们将学习：</p>
<p>非参统计；</p>
<p>核函数；</p>
<p>核函数回归；<br />
高斯过程以及函数的先验。</p>
<p>285</p>
<p><strong>8.1</strong>　非参统计</p>
<p>非参统计通常用来描述一类不依赖于参数化概率分布的统计工<br />
具/模型。根据这个定义，贝叶斯统计似乎不可能是非参的，因为前</p>
<p>面我们学到过，贝叶斯统计的第一步就是在概率模型中准确地将概率 分布组合在一起。第1章中说过，概率分布是构建概率模型的基石。 在贝叶斯框架中，非参模型是指包含有无限多参数的模型，因此，我 们将参数可以随着数据大小而变化的模型称作非参数化模型。对于非 参数化模型而言，理论上其参数个数是无限的，实际使用中会根据数 据将其收缩到一个有限的值，从而让数据本身来决定参数的个数。</p>
<p>286</p>
<p><strong>8.2</strong>　基于核函数的模型</p>
<p>核函数方法相关的研究是一个非常高产而且活跃的领域，有许多 书讨论这个主题，其流行也是因为核函数具有一些有趣的数学特点。 这里只需要知道，核函数可以作为非线性模型的基础，并且相对来说 比较容易计算。比较流行的核函数方法有两种：支持向量机</p>
<p>（Support Vector Machine，SVM）和高斯过程。后者是一种概率方<br />
法，也正是本章将要介绍的主题，前者不是概率的方法，这里不做深 入讨论，你可以阅读Jake Vanderplas写的《Python Data Science<br />
Handbook》和Sebastian Raschka写的《Python Machine Learning<br />
Bayesian Analysis with Python》这两本书了解更多。在继续深入之<br />
前，我们先了解一下什么是核以及如何使用。</p>
<p>你可能会发现，在统计学中，核函数的定义不止一种，并且根据 定义的不同，核函数的数学特性也有所不同。结合本章讨论的目的， 这里将核函数看作是一个对称函数，接受两个输入并返回一个永远为 正数的值，从而可以将核函数的输出看作是两个输入变量之间的相似 度。</p>
<p>核函数有许多种，其中某些经过特殊改造后能很好地用于解决图 像识别、文档分析等问题，还有些核函数适用于对周期性的函数建 模。有一个核函数在统计学和机器学习中非常流行，叫做高斯核，也 称作高斯径向基函数。</p>
<p><strong>8.2.1</strong>　高斯核函数</p>
<p>高斯核函数的定义如下：</p>
<p>287</p>
<p><img alt="" src="media/image2229.png" />{width=”2.488344269466317in” height=”0.5726706036745407in”}</p>
<p><img alt="" src="media/image2230.png" />{width=”0.6975688976377953in” height=”0.21865594925634296in”}其中称为欧氏距离的平方（Squared Euclidean Distance， SED），对于一个<em>n</em>维的空间，我们有：</p>
<p><img alt="" src="media/image2231.png" />{width=”4.508172572178478in” height=”0.2394805336832896in”}</p>
<p>注意，如果计算欧几里得距离的话需要求平方根。SED并不满足 三角不等式，因而并不是真实的距离，不过在许多常见的问题中，我 们只需要对不同的SED进行比较，例如，计算一些点中的最小欧氏距 离等价于找到最小的SED。</p>
<p><img alt="" src="media/image2232.png" />{width=”0.6142771216097987in” height=”0.15618219597550306in”}有一点不太明显，高斯核函数与高斯分布的数学形式其实有一些 相似之处，将常数项去掉之后，可以得到，其中，<em>w</em>控制着核 函数的宽度，在这里正比于方差，因而有时候也称作带宽。</p>
<p><strong>8.2.2</strong>　核线性回归</p>
<p>前面我们学习了线性回归模型的基本形式：</p>
<p><img alt="" src="media/image2233.png" />{width=”1.0932053805774278in” height=”0.187419072615923in”}</p>
<p><img alt="" src="media/image2234.png" />{width=”0.10411417322834646in” height=”0.10412182852143483in”}其中， 是噪声或者误差，通常是高斯分布。</p>
<p><img alt="" src="media/image2235.png" />{width=”1.6762489063867017in” height=”0.3331900699912511in”}</p>
<p>这里我们用<em>f</em> (.)来表示线性函数（没有噪声），如果使用了其他 逆连结函数（比如第5章中的逻辑函数），我们就将它包含在<em>f</em> (.)内。</p>
<p><img alt="" src="media/image2236.png" />{width=”0.10411417322834646in” height=”0.12494641294838145in”}这里向量表示一个系数向量，通常是一个系数和一个或多个斜率。</p>
<p>在第4章中，我们介绍了多项式回归的概念，同时提到了，多项 式回归的唯一实际用途可能就是作为统计学的教学科研工具（至少对</p>
<p>288</p>
<p>于2次或3次以上的多项式回归）。此外我们还学习了如何将多项式回 归用于非线性数据的建模过程中。</p>
<p>这里我们将多项式回归表示成如下形式：</p>
<p><img alt="" src="media/image2238.png" />{width=”1.2597889326334208in” height=”0.2707163167104112in”}</p>
<p>其中，<em>φ</em>函数表示一系列阶数逐渐增加的多项式，将向量<em><strong>x</strong></em>转成矩 阵后，矩阵的每一列都是向量<em><strong>x</strong></em>的幂次（逐渐增加），从而有效地将 数据映射到了更高维度的空间中，然后再从高维空间中找到一条直线 去拟合数据，最后将该直线投影回原始的低维空间时，不一定仍然是 直线，而有可能是一条曲线。整个过程通常称作将输入投影到特征空 间。</p>
<p>函数<em>φ</em>不一定是多项式，还有其他形式的函数将输入向量映射到 （高维的）特征空间中，在此条件下，除了使用<em>φ</em>函数之外，我们还 可以将其替换成一个核函数。尽管在数学形式上是等价的，使用核函</p>
<p>数会让计算更容易一些，这通常称作核函数技巧。这也是为什么核函 数是许多统计学方法和机器学习方法中的核心概念的主要原因，而特 征空间的概念在实际中反而不那么重要了（尽管在学习核函数方法的 时候还是能提供一些直观的解释）。</p>
<p>继续我们的讨论，这里不过多深入其中的数学细节，我们将φ替 换成一个核函数<em>K</em>，这里用<em>K</em>来表示高斯核函数，于是得到：</p>
<p><img alt="" src="media/image2239.png" />{width=”1.7178947944007in” height=”0.35401465441819774in”}</p>
<p>注意，这里用<em>x</em>来表示数据，此外还有一个向量叫做<em>x</em>′，后者也 是一个向量，通常称作结或者中心点，它均匀地分布在数据的范围 内。一个特例是<em>x</em> = <em>x</em>′，换句话说，我们完全可以将数据点作为结。</p>
<p>289</p>
<p><img alt="" src="media/image2241.png" />{width=”0.5518077427821523in” height=”0.21865594925634296in”}我们为什么要增加这些点呢？在直接回答这个问题之前，我们先 看一下如何使用结。注意，如果结距离原数据越近，核函数的返回值 越大。为了简化分析，假设<em>w</em>=1，然后如果，那么有</p>
<p><img alt="" src="media/image2242.png" />{width=”0.166583552055993in” height=”0.21865594925634296in”}；如果和相隔较远，那么 。换句话说，由<br />
于高斯核函数的输出衡量的是相似性，因此我们可以说，如果和 比较接近，那么函数这些点的均值也很接近，即。如果对改 动一点点，那么 也会相应地变化，<em>x</em>的变化越大，那么 的变化也就 越大。现在思考一下，我们可以将其看作是模型的一种特性，经验告 诉我们，有很多函数都有类似的表现，事实上，这类函数有一个名 字，称作平滑函数。尽管有些特例，不过许多问题都可以用平滑函数 来近似。</p>
<p><img alt="" src="media/image2252.png" />{width=”0.10411417322834646in” height=”0.12494641294838145in”}进一步解释一下我们在做的事情，我们可以看作是在尝试使用第 1章和第2章中见过的网格搜索法去拟合一个平滑的未知函数，其中网 格的格点是<em>x</em>′，对于每个结，我们都有一个高斯函数，根据数据（通 过）增加或者降低这些高斯函数的权重。如果将所有的高斯函数都</p>
<p><img alt="" src="media/image2253.png" />{width=”0.10411417322834646in” height=”0.11453412073490814in”}加起来，那么就得到了一个平滑的曲线并拟合出了。这种解释称作 权重空间视角。在本章接下来的高斯过程一节中，我们将看到描述该 问题的另一种视角：函数空间视角。</p>
<p>现在，将前面所有的想法汇总在一起，并应用到一个简单的合成 数据集上，其中因变量是一个sin函数，自变量是一系列从均匀分布 中得到的点。代码实现如下：</p>
<p>np.random.seed(1)</p>
<p>x = np.random.uniform(0, 10, size=20)</p>
<p>y = np.sin(x)</p>
<p>plt.plot(x, y, ‘o’)</p>
<p>plt.xlabel(‘$x$’, fontsize=16)<br />
plt.ylabel(‘$f(x)$’, fontsize=16, rotation=0)</p>
<p>290</p>
<p><img alt="" src="media/image2259.png" />{width=”5.01833552055993in” height=”3.3527307524059493in”}</p>
<p>这里定义一个函数来计算模型中的高斯核函数，代码实现如下：</p>
<p>def gauss_kernel(x, n_knots=5, w=2):</p>
<p>“””</p>
<p>Simple Gaussian radial kernel</p>
<p>“””</p>
<p>knots = np.linspace(np.floor(x.min()), np.ceil(x.max()), n_knots) return np.array([np.exp(-(x-k)**2/w) for k in knots])</p>
<p><img alt="" src="media/image2264.png" />{width=”0.10411417322834646in” height=”0.12494641294838145in”}剩下的就是定义系数了，系数的个数应该与结的个数一致。注 意，这里系数决定了估计的曲线在结这里应该增加还是降低。</p>
<p>有时候将模型表示成线性回归更合理，对应的数学形式如下：</p>
<p><img alt="" src="media/image2267.png" />{width=”2.51957895888014in” height=”0.35401465441819774in”}</p>
<p>不过这里我们将省略截距<em>α</em>和斜率<em>β</em>，只保留最后一项。这里使用 柯西分布作为先验，稍后我们会讨论在使用核函数方法过程中选择先 验的一些关键点，现在先运行如下模型：</p>
<p><img alt="" src="media/image2268.png" />{width=”3.818897637795276e-2in” height=”0.8399300087489063in”}with pm.Model() as kernel_model:</p>
<p>gamma = pm.Cauchy(‘gamma’, alpha=0, beta=1, shape=n_knots) sd = pm.Uniform(‘sd’,0, 10)</p>
<p>mu = pm.math.dot(gamma, gauss_kernel(x, n_knots))</p>
<p>291 <img alt="" src="media/image2271.png" />{width=”2.7777777777777776e-2in” height=”2.7777777777777776e-2in”}</p>
<p><img alt="" src="media/image2273.png" />{width=”2.7777777777777776e-2in” height=”2.7777777777777776e-2in”} yl = pm.Normal(‘yl’, mu=mu, sd=sd, observed=y)<br />
kernel_trace = pm.sample(10000, step=pm.Metropolis()) chain = kernel_trace[5000:]</p>
<p>pm.traceplot(chain);</p>
<p><img alt="" src="media/image2277.png" />{width=”5.643024934383202in” height=”1.7492508748906386in”}</p>
<p>可以看到模型的结果与sin函数很接近，接下来对得到的模型进 行后验检查，代码实现如下：</p>
<p>ppc = pm.sample_ppc(chain, model=kernel_model, samples=100)</p>
<p>plt.plot(x, ppc[‘yl’].T, ‘ro’, alpha=0.1)</p>
<p>plt.plot(x, y, ‘bo’)</p>
<p>plt.xlabel(‘$x$’, fontsize=16)<br />
plt.ylabel(5$f(x)$’, fontsize=16, rotation=0)</p>
<p><img alt="" src="media/image2282.png" />{width=”5.01833552055993in” height=”3.342318460192476in”}</p>
<p>模型似乎能够很好地捕捉到数据，现在我们再用之前未使用的数</p>
<p>292</p>
<p>据检查模型的效果，代码实现如下：</p>
<p>new_x = np.linspace(np.floor(x.min()), np.ceil(x.max()), 100) k = gauss_kernel(new_x, n_knots)</p>
<p>gamma_pred = chain[‘gamma’]</p>
<p>for i in range(100):</p>
<p>idx = np.random.randint(0, len(gamma_pred))</p>
<p>y_pred = np.math.dot(gamma_pred[idx], k)<br />
plt.plot(new_x, y_pred, ‘r-‘, alpha=0.1)<br />
plt.xlabel(‘$x$’, fontsize=16)</p>
<p>plt.ylabel(‘$f(x)$’, fontsize=16, rotation=0)</p>
<p>plt.plot(x, y, ‘bo’);</p>
<p><img alt="" src="media/image2288.png" />{width=”5.01833552055993in” height=”3.373555336832896in”}</p>
<p>图中我们用蓝色的点表示数据，用红色的线表示拟合的曲线。你 可能会想要修改带宽和结的个数来看看模型的效果，本章的练习1中 有相关问题的描述，此外本章的练习2中还会探讨拟合另外一类函数 的效果。</p>
<p><strong>8.2.3</strong>　过拟合与先验</p>
<p>对于使用核函数方法的模型而言，人们常常关心的一点是如何选 择结的个数以及位置。一种方法是直接选择数据点作为结，即将高斯 分布放在数据点上，这里可以回想一下前面用过的KDE作图方法。</p>
<p>293</p>
<p>另外一种做法是，在建模过程中决定结的个数和位置。这种做法<br />
需要一些特殊的计算方法，不是很通用，或者至少不那么容易处理。</p>
<p><img alt="" src="media/image2290.png" />{width=”0.10411417322834646in” height=”0.13535761154855644in”}还有一种做法是应用变量选择，其思想是引入一个模型索引变<br />
量，该向量的大小与参数的大小保持一致，每个元素的值只有两种</p>
<p><img alt="" src="media/image2291.png" />{width=”0.10411417322834646in” height=”0.13535761154855644in”}可能——0或者1。采用这种方法，我们可以决定模型中对应系数的开 和关。不过这种方法的一个问题是，只对低维度的问题有效，因为可 能的索引组合是系数个数的2*^n^*倍。一种替代方案是使用正则先验，我 们希望先验集中在0附近，从而将系数拉向0，同时让具有长尾，从 而避免拉得太厉害了。一种正则先验是柯西分布，另外一种是拉普拉 斯分布。回忆一下第6章中我们讨论过的正则先验——岭回归和</p>
<p>LASSO回归。</p>
<p>294</p>
<p><strong>8.3</strong>　高斯过程</p>
<p>前面简要介绍了如何用核函数构建统计模型去描述任意函数。也 许核函数回归听起来好像有点带有技巧性，而且其中需要指定结的个 数以及分布，这似乎有点问题。现在，我们将从另外一个角度使用核 函数，直接在函数空间做推断，该方法基于高斯过程，在数学上和计 算量方面都更受欢迎。</p>
<p>在解释高斯过程之前，先想想什么是函数？函数可以看作是输入 到输出之间的映射关系。一种学习该映射的方法是将其限定为一条直 线，如我们在第4章做的那样，然后用贝叶斯的知识去推断出决定那</p>
<p>条直线的参数。不过假设我们不希望将其限定为一条直线，也可以是 任意可能的函数。通常在贝叶斯统计中，对于一个不知道的量，我们 先给其设置一个先验，因此，在我们不知道怎样的函数才是一个对数 据拟合得很好的模型时，我们需要对函数设置一个先验。有趣的是， 多元高斯就是这样一个先验（实际上是一个与之相似的东西，不过我 们暂且这么认为），可以用一个多元高斯从很宽泛的（但很有用的） 角度来描述一个函数。我们认为对于每个变量*x~i~*都存在一个高斯变量</p>
<p><em>y~i~</em>，其均值和标准差暂不清楚。这样，如果向量<em><strong>x</strong></em>的长度为<em>n</em>，那么就</p>
<p>得到一个<em>n</em>元高斯分布。</p>
<p>在真实世界中，对于一个实数范围内的映射函数，<em><strong>x</strong></em>和<em><strong>y</strong></em>实际上是 无限多的，因为两个点之间存在着无穷多个点。因此，理论上我们需 要一个无限元的高斯分布，这在数学上称为高斯过程，是一个参数化 的均值函数和一个协方差函数，此时我们有：</p>
<p><img alt="" src="media/image2295.png" />{width=”2.311349518810149in” height=”0.187419072615923in”}</p>
<p>295</p>
<p>关于高斯过程的一个正式定义是说，连续空间中的每个点都有一 个与之对应的正态分布的变量，而高斯过程就是这无限多个随机变量 的联合分布。其中均值函数是一个无限维向量的均值，协方差函数则 是一个无限维的协方差矩阵，我们将看到协方差矩阵可以有效地建</p>
<p>模<em><strong>x</strong></em>的变化量与<em><strong>y</strong></em>的变化量之间的关系。</p>
<p><img alt="" src="media/image2297.png" />{width=”0.45810476815398077in” height=”0.187419072615923in”}总结一下，前面几章中，我们学习了如何估计，比如，在<br />
线性回归中我们假设 ，其中，<em>f</em>是一个线性模型，然后估计</p>
<p><img alt="" src="media/image2299.png" />{width=”0.5726312335958005in” height=”0.187419072615923in”}线性模型的参数。也就是说，我们是一个线性模型，并估计出 了其中的参数，从而得到了，后面将会看到，我们仍然需要估 计参数，不过从概念上讲，我们是在直接处理函数，这样思考更容易 理解一些。</p>
<p><strong>8.3.1</strong>　构建协方差矩阵</p>
<p>在实践中，高斯过程的均值函数通常设为0（尽管这并非一定<br />
的），因而高斯过程的整个行为都受协方差矩阵的控制，所以这里重</p>
<p>点关注如何构建协方差函数。</p>
<p>从高斯过程先验中采样</p>
<p>高斯过程的概念有点像是脚手架，在实际使用中我们通常不直接 使用该无限维的对象，相反，我们将无限维的高斯过程收缩到一个有 限维的多元高斯。数学上，这是通过对模型中剩余的（无限的）未观 测维度进行边缘化得到的，这样做之后就可以得到一个多元高斯分</p>
<p>布。这么做遵循高斯过程的定义，即作为一系列随机变量，其中任意 有限的子集都有一个联合高斯分布，于是得到的多元高斯分布的维度 与我们现有数据的个数相同！这样，对于一个零均值函数的高斯过</p>
<p>程，我们有：</p>
<p>296</p>
<p><img alt="" src="media/image2302.png" />{width=”3.6960772090988625in” height=”0.19783136482939634in”}</p>
<p><img alt="" src="media/image2303.png" />{width=”0.6246883202099738in” height=”0.187419072615923in”}现在我们搞定了无限维这个棘手的问题，接下来继续处理协方差 矩阵，注意，我们将协方差矩阵写成了，这里故意写成了前面 核函数回归例子中的定义，因为我们实际上是用核函数在构建协方差 函数。协方差函数描述的是，当<em><strong>x</strong></em>变化时，<em><strong>y</strong></em>是如何随之变化的。前面 核函数回归中，我们已经知道了使用高斯核函数是一个不错的假设， 其等价的含义是说，一个小的*x~i~<em>扰动会导致较小的</em>y~i~*变化，而一个较</p>
<p>大的<em>x~i~<em>变化则会导致</em>y~i~</em>（平均来看）较大的变化。</p>
<p>为了从直观上了解高斯过程先验是什么样的，我们将对其采样并 画出来。下图根据高斯过程先验画出了6个任意的函数（或者叫实 现）。注意，这里我们将实现画成了连续函数，实际上，我们只有每 个<em>x</em>~*~以及与之对应的<em>f</em> (<em>x</em>~*~)，不过根据平滑性假设以及无法计算出无限</p>
<p>多个点这一事实，将实现中的点转换成连续的函数也是合理的。只要 测试点足够多，对应的结果就能准确反映出函数的真实形状。</p>
<p>np.random.seed(1)</p>
<p>test_points = np.linspace(0, 10, 100)</p>
<p>cov = np.exp(-squared_distance(test_points, test_points))<br />
plt.plot(test_points, stats.multivariate_normal.rvs(cov=cov, size=6). T)</p>
<p>plt.xlabel(‘$x$’, fontsize=16)<br />
plt.ylabel(‘$f(x)$’, fontsize=16, rotation=0)</p>
<p>297</p>
<p><img alt="" src="media/image2309.png" />{width=”5.01833552055993in” height=”3.3943799212598424in”}</p>
<p>从这张图可以看出，高斯过程先验（以及一个高斯核函数）是一 系列在0附近的平滑函数。</p>
<p>使用参数化的核函数</p>
<p>为了从数据中学到关于未知函数的信息，我们用一个参数化的核 函数定义了协方差矩阵。我们将这里的参数称为超参。原因有如下两 点：</p>
<p>这些参数是高斯过程的先验；<br />
这个名字强调了我们使用的是非参方法。</p>
<p>通过学习高斯过程的超参数，我们希望拟合出未知函数。</p>
<p>前面我们已经提到了，核函数有很多种选择，其中最常见的是高 斯核函数。前面已经看到了高斯核函数的一个版本（即带宽）。现 在，我们介绍另外一个版本，用到了其他两个参数，我们可以将其写 成如下形式：</p>
<p>298</p>
<p><img alt="" src="media/image2313.png" />{width=”1.9677701224846895in” height=”0.5310225284339457in”}</p>
<p><img alt="" src="media/image2314.png" />{width=”0.7079800962379702in” height=”0.20824365704286965in”}其中，<em>D</em>是SED，即； 是一个控制垂直尺度的参数，用 于让协方差矩阵建模<em>f</em> (<em><strong>x</strong></em>)更大或者更小的值；参数<em>ρ</em>是带宽，前面已 经知道了，<em>ρ</em>控制的是函数的平滑程度；参数<em>σ</em>控制的是数据中的噪 声。</p>
<p>让我们一起讨论下<em>σ</em>以及为什么当<em>i</em>和<em>j</em>不同时使用另外一个表达<br />
式。在一些场景中，比如进行插值的时候，我们希望模型对于每个观</p>
<p>测值<em>x~i~<em>都返回对应的观测量</em>f</em> (<em>x~i~</em>)而没有任何不确定性，而在另外一些 场景中，比如本书涉及的例子中，我们希望得到<em>f</em> (<em>x~i~</em>)估计值的不确定</p>
<p>性，因此希望得到一个接近于观测值但却又不是完全一样的值。于是 有：</p>
<p><img alt="" src="media/image2233.png" />{width=”1.0932053805774278in” height=”0.187419072615923in”}</p>
<p><img alt="" src="media/image2317.png" />{width=”0.8641524496937882in” height=”0.187419072615923in”}其中，误差项通过 建模。</p>
<p>因此协方差矩阵的构成需要考虑到有噪声的数据，于是有：</p>
<p><img alt="" src="media/image2318.png" />{width=”2.2801148293963256in” height=”0.20824365704286965in”}</p>
<p>其中：</p>
<p><img alt="" src="media/image2319.png" />{width=”1.1973206474190725in” height=”0.5101979440069991in”}</p>
<p><img alt="" src="media/image2320.png" />{width=”0.176994750656168in” height=”0.1770067804024497in”}称作delta分布，也就是说，我们通过将协方差矩阵的对角线上 的值设置为不是正好等于1的值来对噪声建模，事实上，我们是直接 从数据中估计出对角线上的值。这么做相当于给模型加入了一个扰动 项，之所以加入该项是因为，在核函数所附带的假设中，两个输入越 接近，对应的输出也越接近，极限情况下，两个输入完全相等，它们</p>
<p>299</p>
<p>的输出也应该完全相等，而增加扰动项则有利于捕捉到观测值的不确 定性。</p>
<p>为了更好地理解超参的含义，我们可以把扩展后的高斯核函数画<br />
出来，你也可以随意选一些不同的超参值自己试试，代码实现如下：</p>
<p>np.random.seed(1) eta = 1.5</p>
<p>rho = 0.2</p>
<p>sigma = 0.007</p>
<p>D = squared_distance(test_points, test_points)</p>
<p>cov = eta * np.exp(-rho * D) diag = eta + sigma</p>
<p>np.fill_diagonal(cov, diag)</p>
<p>for i in range(6):</p>
<p>plt.plot(test_points, stats.multivariate_normal.rvs(cov=cov)) plt.xlabel(‘$x$’, fontsize=16)</p>
<p>plt.ylabel(‘$f(x)$’, fontsize=16, rotation=0)</p>
<p><img alt="" src="media/image2326.png" />{width=”5.01833552055993in” height=”3.4047911198600174in”}</p>
<p><strong>8.3.2</strong>　根据高斯过程做预测</p>
<p>300</p>
<p>接下来学习如何用高斯过程做预测。高斯过程的一个优点就是结 果可以从数学分析上直接推导。如果将高斯过程先验与高斯似然组合 在一起，我们得到的是一个高斯过程后验。也就是说，如果在一些测 试点上应用条件高斯，我们可以得到如下后验预测密度的表达式：</p>
<p><img alt="" src="media/image2328.png" />{width=”2.623694225721785in” height=”0.8954494750656168in”}</p>
<p>在给定数据<em><strong>X</strong></em>、<em><strong>y</strong></em>以及一些测试点<em><strong>x</strong></em>~*~条件下，计算未知函数在（暂 时）未知的点上的值<em>f</em> (<em><strong>x</strong></em>~*~)。注意，这里我们使用符号*来表示所有测</p>
<p>试点上的运算，其中：</p>
<p><em><strong>K</strong></em>=<em>K</em>(<em><strong>X</strong></em>,<em><strong>X</strong></em>)</p>
<p><em><strong>K</strong></em>~**~=<em>K</em>(<em><strong>X</strong></em>~*~,<em><strong>X</strong></em>~*~)</p>
<p><em><strong>K</strong></em>~*~=<em>K</em>(<em><strong>X</strong></em>,<em><strong>X</strong></em>~*~)</p>
<p>这个式子初看可能有点吓人，稍后我们将看到如何用代码描述上 面的式子（但愿这样理解起来清晰一些），不过现在我们先从直观上 理解这些式子。</p>
<p><img alt="" src="media/image2329.png" />{width=”0.15617125984251967in” height=”0.187419072615923in”}<em><strong>X</strong></em>~**~是<em><strong>x</strong></em>~*~自己的协方差，其实就是<em><strong>x</strong></em>~*~的方差。也就是说，测试点<br />
的方差其实就是先验的方差，注意，这里等价于<em><strong>K</strong></em>~**~减去一个量，</p>
<p>这个式子的含义是说，我们可以根据数据将先验的方差降低这么多。 此外，对于一个距离数据点<em>x~i~<em>较远的测试点</em></em><em>x</em>**~*~而言，核函数返回的值</p>
<p>接近于0，因而这个量也接近于0，结果就是先验的方差也不会降低多 少。换句话说，评估远离数据点的函数并不会降低不确定性。因此， 推断基于的是数据在测试点附近的局部特性。</p>
<p>301</p>
<p>如果你想知道更多有关多元高斯的数学特性及前面的表达式是如 何推导的，可以参考本章的阅读更多部分，我在那里添加了一些你可 能感兴趣的参考资料。针对当前的讨论内容，我们暂且假设前面的后 验预测的表达式是已知的。有一点需要注意的是，我们可以从高斯过 程后验中进行采样。我们得到的这个描述未知函数的表达式非常有</p>
<p>用，不过有一个问题是，计算过程中涉及求解矩阵的逆，而计算逆矩 阵的复杂度是<em>O</em>(<em>n</em>^3^)，如果你不知道这个表达式的含义，可以理解为<br />
该操作很慢，因此实际使用中，我们无法将高斯过程应用到超过几千 个数据点的场景，不过对于这种情况有一些近似的方法来加速计算， 这里暂不深入讨论。此外，在实践中，直接求逆矩阵可能会有一些数 值问题，而且不稳定，因此更倾向于使用一些替代方案，比如用<br />
<strong>Cholesky</strong>分解来计算均值和协方差后验函数。Cholesky分解有点像我 们熟悉的对标量求平方根，不过其对象是矩阵。</p>
<p>在深入Cholesky分解和直接求逆矩阵的例子之前，我们先观察<br />
下。如果求逆矩阵会有问题，那么为什么还要将高斯过程表示成协方</p>
<p>差矩阵而不是直接用协方差矩阵的逆呢？原因是，当我们使用协方差 矩阵的时候，对其子集的计算是独立于其余部分的计算的，因而它是 独立于未观测到的点的计算。不过对于逆协方差矩阵而言，数据子集 的计算会依赖于我们是否观测到了其余点。只有使用协方差矩阵（而 不是它的逆），我们才能得出高斯过程基于一系列随机变量一致的定 义。</p>
<p>总结一下，为了完全从贝叶斯的角度使用高斯过程去近似一个函 数，我们需要：</p>
<p>选择一个核函数构建一个多元分布的协方差矩阵； 用贝叶斯统计的方法推断出核函数的参数；</p>
<p>302</p>
<p>计算出每个测试点的均值和标准差。</p>
<p>注意，实际上我们并没有真正地计算出高斯过程，只是借用了这 个数学概念来确保我们所做的是合理的，在实践中，所有的计算都是 通过多元高斯完成的。</p>
<p>读完前面理论方面的长篇大论之后，终于到了大家期待的时刻， 我们将把这些想法转化成代码。首先，假设已经知道了核函数的参 数，然后用代码描述后验的表达式。用到的超参和test_points的值</p>
<p>与前面定义的相同，数据也与前面核函数回归例子中用到的一样，代 码实现如下：</p>
<p>np.random.seed(1)</p>
<p>K_oo = eta * np.exp(-rho * D)</p>
<p>D_x = squared_distance(x, x) K = eta * np.exp(-rho * D_x) diag_x = eta + sigma</p>
<p>np.fill_diagonal(K, diag_x)</p>
<p>D_off_diag = squared_distance(x, test_points) K_o = eta * np.exp(-rho * D_off_diag)</p>
<p>mu_post = np.math.dot(np.math.dot(K_o, np.linalg.inv(K)), y)</p>
<p>SIGMA_post = K_oo – np.math.dot(np.math.dot(K_o, np.linalg.inv(K)), K_o .T)</p>
<p>for i in range(100):</p>
<p>fx = stats.multivariate_normal.rvs(mean=mu_post, cov=SIGMA_post) plt.plot(test_points, fx, ‘r-‘, alpha=0.1)</p>
<p>plt.plot(x, y, ‘o’)</p>
<p>plt.xlabel(‘$x$’, fontsize=16)<br />
plt.ylabel(‘$f(x)$’, fontsize=16, rotation=0)</p>
<p>303</p>
<p><img alt="" src="media/image2340.png" />{width=”5.01833552055993in” height=”3.425615704286964in”}</p>
<p>这里通过叠在一起的一些红线（从高斯过程中得到的实例）来表 示不确定性。从图中可以看到，在数据点附近的不确定性要小一些， 而在最后两个数据点之间的不确定性要大一些，在最右边没有数据点 的地方（<em>x</em>～9）不确定性要更大一些。</p>
<p>接下来，我们将重新实现前面的计算过程，不过这次用的是<br />
Cholesky分解。下面的代码是根据 Nando de Freitas讲授机器学习课程</p>
<p>中的实例修改后得到的，在代码中，<em>N</em>表示数据点的个数，<em>n</em>表示测<br />
试点的个数。</p>
<p><img alt="" src="media/image2341.png" />{width=”3.8190069991251095e-2in” height=”2.839073709536308in”}np.random.seed(1) eta = 1</p>
<p>rho = 0.5</p>
<p>sigma = 0.03</p>
<p>f = lambda x: np.sin(x).flatten()</p>
<p>def kernel(a, b):</p>
<p>“”” GP squared exponential kernel “””</p>
<p>sqdist = np.sum(a**2,1).reshape(-1,1) + np.sum(b**2,1) - 2*np.dot(a , b.T)</p>
<p>return eta * np.exp(- rho * sqdist)</p>
<p>N = 20</p>
<p>304</p>
<p><img alt="" src="media/image2345.png" />{width=”2.7777777777777776e-2in” height=”2.7777777777777776e-2in”}n = 100</p>
<p>X = np.random.uniform(0, 10, size=(N,1)) y = f(X) + sigma * np.random.randn(N)</p>
<p>K = kernel(X, X)</p>
<p>L = np.linalg.cholesky(K + sigma * np.eye(N))</p>
<p>test_points = np.linspace(0, 10, n).reshape(-1,1)</p>
<p>Lk = np.linalg.solve(L, kernel(X, test_points)) mu = np.dot(Lk.T, np.linalg.solve(L, y))</p>
<p>K_ = kernel(Xtest, Xtest)</p>
<p>sd_pred = (np.diag(K_) - np.sum(Lk**2, axis=0))**0.5</p>
<p>plt.fill_between(test_points.flat, mu-2*s, mu+2*s, color=”r”, alpha=0.2</p>
<p>)</p>
<p>plt.plot(test_points, mu, ‘r’, lw=2)<br />
plt.plot(x, y, ‘o’)</p>
<p>plt.xlabel(‘$x$’, fontsize=16)<br />
plt.ylabel(‘$f(x)$’, fontsize=16, rotation=0)</p>
<p><img alt="" src="media/image2349.png" />{width=”5.01833552055993in” height=”3.3319061679790027in”}</p>
<p>这里我们将数据点用蓝色的点来表示，均值函数是一条红线，不 确定性用半透明的区域来表示。</p>
<p><strong>8.3.3</strong>　用<strong>PyMC3</strong>实现高斯过程</p>
<p>305</p>
<p>总结一下，我们有如下高斯先验：</p>
<p><img alt="" src="media/image2351.png" />{width=”2.946450131233596in” height=”0.19783136482939634in”}</p>
<p>一个高斯似然：</p>
<p><img alt="" src="media/image2352.png" />{width=”2.196823053368329in” height=”0.2498928258967629in”}</p>
<p>以及一个高斯过程后验：</p>
<p><img alt="" src="media/image2353.png" />{width=”2.675751312335958in” height=”0.20824365704286965in”}</p>
<p>记住，在实践中，我们使用的是多元高斯，因为在一个有限的数 据集上，高斯过程就是一个多元高斯。</p>
<p>我们将使用贝叶斯相关的原理来学习协方差矩阵的超参。你会看 到，尽管使用PyMC3很简单，但是现在代码量会增加一些，我们需 要手动求解矩阵的逆（或者计算Cholesky分解）。</p>
<p>很可能在不久的未来，PyMC3中会出现一个高斯过程模块，让<br />
高斯过程模型的构造更简单一些。说不定在你阅读本书的此刻，高斯</p>
<p>过程模型已经有了！^[1]^</p>
<p>下面的模型是从Chris Fonnesbeck写的Stan代码中改造的：</p>
<p><img alt="" src="media/image2354.png" />{width=”3.819444444444445e-2in” height=”2.7349507874015746in”}with pm.Model() as GP:</p>
<p>mu = np.zeros(N)</p>
<p>eta = pm.HalfCauchy(‘eta’, 5)<br />
rho = pm.HalfCauchy(‘rho’, 5)<br />
sigma = pm.HalfCauchy(‘sigma’, 5)</p>
<p>D = squared_distance(x, x)</p>
<p>K = tt.fill_diagonal(eta * pm.math.exp(-rho * D), eta + sigma)</p>
<p>obs = pm.MvNormal(‘obs’, mu, tt.nlinalg.matrix_inverse(K), observed =y)</p>
<p>test_points = np.linspace(0, 10, 100)</p>
<p>306</p>
<p><img alt="" src="media/image2358.png" />{width=”2.7777777777777776e-2in” height=”2.7777777777777776e-2in”} D_pred = squared_distance(test_points, test_points) D_off_diag = squared_distance(x, test_points)</p>
<p>K_oo = eta * pm.math.exp(-rho * D_pred)<br />
K_o = eta * pm.math.exp(-rho * D_off_diag)</p>
<p>mu_post = pm.Deterministic(‘mu_post’, pm.math.dot(pm.math.dot(K_o, tt.nlinalg.matrix_inverse(K)), y))</p>
<p>SIGMA_post = pm.Deterministic(‘SIGMA_post’, K_oo – pm.math.dot(pm.m ath.dot(K_o, tt.nlinalg.matrix_inverse(K)), K_o.T))</p>
<p>start = pm.find_MAP()</p>
<p>trace = pm.sample(1000, start=start)</p>
<p>varnames = [‘eta’, ‘rho’, ‘sigma’] chain = trace[100:]<br />
pm.traceplot(chain, varnames)</p>
<p><img alt="" src="media/image2362.png" />{width=”5.643024934383202in” height=”2.696761811023622in”}</p>
<p>如果留心的话，你会注意到估计出来的参数（<em>η</em>，<em>ρ</em>，<em>σ</em>）的均值 就是我们前面例子中用到的值。这也解释了为什么拟合的效果这么 好，这些超参可不是从我们脑袋里想出来的！</p>
<p>pm.df_summary(chain, varnames).round(4)</p>
<p>+——-+——–+——–+———-+———+———-+
|       |  mean | sd     | mc_error | hpd_2.5 | hpd_97.5 |
+=======+========+========+==========+=========+==========+
|  eta | 2.5798 | 2.5296 | 0.1587   | 0.1757  | 6.3445   |
+——-+——–+——–+———-+———+———-+</p>
<p><img alt="" src="media/image2367.png" />{width=”3.818678915135608e-2in” height=”0.25684601924759404in”}</p>
<p><img alt="" src="media/image2379.png" />{width=”2.7777777777777776e-2in” height=”2.7777777777777776e-2in”}307</p>
<p><img alt="" src="media/image2386.png" />{width=”2.7777777777777776e-2in” height=”2.7777777777777776e-2in”}rho 0.1288 0.0485 0.0027 0.0589 0.2290</p>
<p>+———+——–+——–+——–+——–+——–+
|  sigma | 0.0006 | 0.0003 | 0.0000 | 0.0002 | 0.0012 |
+———+——–+——–+——–+——–+——–+</p>
<p>后验预测检查</p>
<p>现在我们把原始数据和从高斯过程后验中得到的实例都画出来， 注意，我们还画出了超参的不确定性，而不只是它们的均值，代码实 现如下：</p>
<p>y_pred = [np.random.multivariate_normal(m, S) for m,S in zip(chain[‘mu_ post’][::5], chain[‘SIGMA_post’][::5])]</p>
<p>for yp in y_pred:</p>
<p>plt.plot(test_points, yp, ‘r-‘, alpha=0.1)</p>
<p>plt.plot(x, y, ‘bo’)</p>
<p>plt.xlabel(‘$x$’, fontsize=16)<br />
plt.ylabel(‘$f(x)$’, fontsize=16, rotation=0)</p>
<p><img alt="" src="media/image2408.png" />{width=”5.01833552055993in” height=”3.279844706911636in”}</p>
<p>周期核函数</p>
<p>前一幅图中，你可能注意到了，我们可以很好地拟合sin函数，</p>
<p>308</p>
<p>不过模型在9到10之间不确定性非常大（该区间没有数据点）。该模 型的一个问题是，原始的数据是通过一个周期函数生成的，但是我们 的核函数并没有做出周期性的假设。某些情况下，当我们知道数据可 能是周期性的时候，应该使用一个周期性的核函数。一个典型的周期 性核函数如下：</p>
<p><img alt="" src="media/image2410.png" />{width=”2.6445166229221346in” height=”0.5726706036745407in”}</p>
<p>注意，这个核函数与高斯核函数的主要区别是包含了一个sin函<br />
数。我们可以复用前面的代码，唯一的区别是现在需要定义一个周期</p>
<p>函数而不是squared_distance，代码实现如下：</p>
<p>periodic = lambda x, y: np.array([[np.sin((x[i] - y[j])/2)**2 for i in range(len(x))] for j in range(len(y))])</p>
<p>在这个模型中，我们需要将squared_distance替换成这个周期 函数。 运行模型之后，你应该会得到类似下面的图：</p>
<p><img alt="" src="media/image2415.png" />{width=”5.01833552055993in” height=”3.279844706911636in”}</p>
<p>309</p>
<p><strong>8.4</strong>　总结</p>
<p>本章一开始，我们学习了贝叶斯框架下的非参统计，以及如何用 核函数表示统计学中的问题，例如，我们用一种采用核函数的线性回 归去建模了非线性输出，随后继续讨论了另外一种构建和理解核函数 方法的方式——高斯过程。</p>
<p>高斯过程是多元高斯分布扩展到无限多维时的一种一般形式，可 以用一个均值函数和一个协方差函数来描述。由于从概念上我们可以 将函数看作无限长的向量，因而可以将高斯过程作为函数先验。实践 中，我们处理的是维度和数据点个数相同的多元高斯分布。为了定义 与之对应的协方差函数，我们使用了参数化的核函数，通过学习超参 数，最终可以拟合出任意复杂的未知函数。</p>
<p>这一章中，我们简要介绍了高斯过程，还有许多与之相关的主题 需要学习（比如构建一个半参数化模型，将线性模型作为均值函 数），或者是将两个或者多个核函数组合在一起来描述未知函数，或 者是如何将高斯过程用于分类任务，或者是如何将高斯过程与统计学 或者机器学习中的其他模型联系起来。不管怎么说，我希望本章对高 斯过程的介绍以及本书中一些其他主题的介绍能够激励你阅读、使用 和进一步学习贝叶斯统计。</p>
<p>310</p>
<p><strong>8.5</strong>　深入阅读</p>
<p>Carl Edward Rasmussen和 Christopher K. I. Williams写的<br />
《Gaussian Processes for Machine Learning》一书。</p>
<p>Kevin Murhpy的《Machine Learning a Probabilistic Perspective》 中的第4章和第15章。</p>
<p>《Statistical Rethinking》中的第11章。</p>
<p>《Bayesian Data Analysis, Third Edition》中的第22章。</p>
<p>311</p>
<p><strong>8.6</strong>　练习</p>
<p>1．在核回归的例子中，尝试修改结的个数以及带宽（一次修改 一个），这些改变有什么效果？尝试只使用一个结，你观察到了什 么？</p>
<p><img alt="" src="media/image2426.png" />{width=”1.9990048118985126in” height=”0.187419072615923in”}2．用核回归拟合其他函数，比如 或者<em>y</em> = <em>x</em>。尝试像练习1中那样修改数据和参数的个数。</p>
<p>3．在前面从高斯过程先验中采样的例子里，增加实例的个数，<br />
将plt.plot(test_points,<br />
stats.multivariate_normal.rvs(cov=cov, size=6).T)替换 成plt.plot(test_points,</p>
<p>stats.multivariate_normal.rvs(cov=cov, size=1000).T,<br />
alpha=0.05, color=’b’)。高斯过程的先验是什么样子的？你是否 看出<em>f</em> (<em>x</em>)分布得像均值为0、标准差为1的高斯分布？</p>
<p>4．对于一个使用高斯核的高斯过程后验，尝试将测试点定义在 区间[0,10]之外，区间外的点有怎样的结果？这告诉我们在外推 （extrapolating）时需要注意什么（特别是非线性函数）？</p>
<p>5．重复练习4，这次换成周期性核，现在你的结论又是什么？</p>
<p>[1]　 在最新版的PyMC3中，已经有该模块了，具体请参考<br />
&lt;<a class="reference external" href="https://pymc-devs.github.io/">https://pymc-devs.github.io/</a> pymc3/examples.html#gaussian-processes。 ——译者注</p>
<p>312</p>
<p>欢迎来到异步社区！</p>
<p>异步社区的来历</p>
<p>异步社区(<a class="reference external" href="http://www.epubit.com.cn">www.epubit.com.cn</a>)是人民邮电出版社旗下IT专业图书 旗舰社区，于2015年8月上线运营。</p>
<p>异步社区依托于人民邮电出版社20余年的IT专业优质出版资源和 编辑策划团队，打造传统出版与电子出版和自出版结合、纸质书与电 子书结合、传统印刷与POD按需印刷结合的出版平台，提供最新技术 资讯，为作者和读者打造交流互动的平台。</p>
<p>313</p>
<p><img alt="" src="media/image2437.png" />{width=”6.267714348206474in” height=”5.320637576552931in”}</p>
<p>314</p>
<p>社区里都有什么？</p>
<p>购买图书</p>
<p>我们出版的图书涵盖主流IT技术，在编程语言、Web技术、数据 科学等领域有众多经典畅销图书。社区现已上线图书1000余种，电子 书400多种，部分新书实现纸书、电子书同步出版。我们还会定期发</p>
<p>布新书书讯。</p>
<p>下载资源</p>
<p>社区内提供随书附赠的资源，如书中的案例或程序源代码。</p>
<p>另外，社区还提供了大量的免费电子书，只要注册成为社区用户 就可以免费下载。</p>
<p>与作译者互动</p>
<p>很多图书的作译者已经入驻社区，您可以关注他们，咨询技术问 题；可以阅读不断更新的技术文章，听作译者和编辑畅聊好书背后有 趣的故事；还可以参与社区的作者访谈栏目，向您关注的作者提出采 访题目。</p>
<p>315</p>
<p>灵活优惠的购书</p>
<p>您可以方便地下单购买纸质图书或电子图书，纸质图书直接从人 民邮电出版社书库发货，电子书提供多种阅读格式。</p>
<p>对于重磅新书，社区提供预售和新书首发服务，用户可以第一时 间买到心仪的新书。</p>
<p><img alt="" src="media/image2442.png" />{width=”1.0411482939632546in” height=”0.15618219597550306in”}用户帐户中的积分可以用于购书优惠。100积分=1元，购买图书 时，在 里填入可使用的积分数值，即可扣减相应金额。</p>
<p>特别优惠</p>
<p>购买本电子书的读者专享异步社区优惠券。 使用方法：注册成为社区用户，在下单购 书时输入”<strong>57AWG</strong>”，然后点击”使用优惠码”，即可享受电子书8折优惠（本优惠券只可使 用一次）。</p>
<p>纸电图书组合购买</p>
<p>社区独家提供纸质图书和电子书组合购买方式，价格优惠，一次 购买，多种阅读选择。</p>
<p>316</p>
<p><img alt="" src="media/image2456.png" />{width=”6.267714348206474in” height=”4.144058398950131in”}</p>
<p>317</p>
<p>社区里还可以做什么？</p>
<p>提交勘误</p>
<p>您可以在图书页面下方提交勘误，每条勘误被确认后可以获得<br />
100积分。热心勘误的读者还有机会参与书稿的审校和翻译工作。</p>
<p>写作</p>
<p>社区提供基于Markdown的写作环境，喜欢写作的您可以在此一<br />
试身手，在社区里分享您的技术心得和读书体会，更可以体验自出版</p>
<p>的乐趣，轻松实现出版的梦想。</p>
<p>如果成为社区认证作译者，还可以享受异步社区提供的作者专享 特色服务。</p>
<p>会议活动早知道</p>
<p>您可以掌握IT圈的技术会议资讯，更有机会免费获赠大会门票。</p>
<p>318</p>
<p>加入异步</p>
<p>扫描任意二维码都能找到我们：</p>
<p><img alt="" src="media/image2461.png" />{width=”2.8111012685914263in” height=”2.8217082239720037in”}</p>
<p>异步社区</p>
<p><img alt="" src="media/image2462.png" />{width=”3.133857174103237in” height=”3.134073709536308in”}</p>
<p>微信订阅号</p>
<p>319</p>
<p><img alt="" src="media/image2464.png" />{width=”3.133857174103237in” height=”3.134073709536308in”}</p>
<p>微信服务号</p>
<p><img alt="" src="media/image2465.png" />{width=”2.8111012685914263in” height=”2.8217082239720037in”}</p>
<p>官方微博</p>
<p>320</p>
<p><img alt="" src="media/image2467.png" />{width=”3.133857174103237in” height=”3.092425634295713in”}</p>
<p>QQ群：436746675</p>
<p>社区网址：<a class="reference external" href="http://www.epubit.com.cn">www.epubit.com.cn</a></p>
<p>官方微信：异步社区</p>
<p>官方微博：&#64;人邮异步社区，&#64;人民邮电出版社-信息技术分社</p>
<p>投稿**&amp;**咨询：contact&#64;epubit.com.cn</p>
<p>321</p>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="chapter06-MixtureModels.html" title="previous page">第6章　混合模型</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Guoliang PU<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>