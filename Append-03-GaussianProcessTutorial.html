
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>附录 C： 高斯过程 &#8212; Python贝叶斯分析(中文)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="附录 D：贝叶斯神经网络概述" href="Append-04-BayesianNN_Tutorial.html" />
    <link rel="prev" title="附录 B： 变分法确定性推断" href="Append-02-VariationalInference_Tutorial.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Python贝叶斯分析(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   封面
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  书籍正文
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter01-ThinkingProbabilistically.html">
   第 1 章 概率思维
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter02-ProgrammingProbabilistically.html">
   第 2 章 概率编程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter03-ModellingwithLinearRegression.html">
   第 3 章 线性回归模型的贝叶斯视角
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter04-GeneralizedLinearRegression.html">
   第 4 章 广义线性回归模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter05-ModelComparison.html">
   第 5 章 模型比较
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter06-MixtureModels.html">
   第 6 章 混合模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter07-GaussianProcesses.html">
   第 7 章 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter08-InterefenceEngine.html">
   第 8 章 推断引擎
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter09-WheretoGoNext.html">
   第 9 章 下一步去哪儿？
  </a>
 </li>
</ul>
<p class="caption" role="heading">
 <span class="caption-text">
  文献阅读
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Append-01-MCMC_Tutorial.html">
   附录 A： MCMC 随机性推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-02-VariationalInference_Tutorial.html">
   附录 B： 变分法确定性推断
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   附录 C： 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-04-BayesianNN_Tutorial.html">
   附录 D：贝叶斯神经网络概述
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-05-BayesianDeepLearning_Tutorial.html">
   附录 E：贝叶斯深度学习综述
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-06-BayesianDeepLearningPymc3.html">
   附录 F：贝叶斯深度学习编程初步
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Append-07-ModelSelectAndAveraging.html">
   附录 G：模型选择与模型平均
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/Append-03-GaussianProcessTutorial.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/Append-03-GaussianProcessTutorial.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/BayesianAnalysiswithPython2nd"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/BayesianAnalysiswithPython2nd/issues/new?title=Issue%20on%20page%20%2FAppend-03-GaussianProcessTutorial.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/xishansnow/BayesianAnalysiswithPython2nd/master?urlpath=lab/tree/Append-03-GaussianProcessTutorial.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        <a class="jupyterhub-button" href="https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/xishansnow/BayesianAnalysiswithPython2nd&urlpath=lab/tree/BayesianAnalysiswithPython2nd/Append-03-GaussianProcessTutorial.ipynb&branch=master"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-1">
   C.1 一元高斯分布
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-2">
   C.2 多元高斯分布
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-3">
   C.3 无限元高斯分布
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-4">
   C.4 核函数（协方差函数）
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-5">
   C.5 高斯过程可视化
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-6">
   C.6 简单高斯过程回归实现
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-7">
   C.7 超参数优化
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-8">
   C.8 多维输入
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#c-9">
   C.9 高斯过程回归的优缺点
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   参考资料
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="c">
<h1>附录 C： 高斯过程<a class="headerlink" href="#c" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>高斯过程 <a class="reference external" href="https://en.wikipedia.org/wiki/Gaussian_process">Gaussian Processes</a> 是概率论和数理统计中随机过程的一种，是多元高斯分布的扩展，被应用于机器学习、信号处理等领域。本文对高斯过程进行公式推导、原理阐述、可视化以及代码实现，介绍了以高斯过程为基础的高斯过程回归 Gaussian Process Regression 基本原理、超参优化、高维输入等问题。</p>
<br>
<div class="section" id="c-1">
<h2>C.1 一元高斯分布<a class="headerlink" href="#c-1" title="Permalink to this headline">¶</a></h2>
<p>我们从最简单最常见的一元高斯分布开始，其概率密度函数为</p>
<div class="math notranslate nohighlight">
\[
p(x) = \frac{1}{\sigma\sqrt{2\pi}}\exp (-\frac{(x-\mu)^2}{2\sigma^2}) \tag{1}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\sigma\)</span> 分别表示均值和方差，这个概率密度函数曲线画出来就是我们熟悉的钟形曲线，均值和方差唯一地决定了曲线的形状。</p>
<br>
</div>
<div class="section" id="c-2">
<h2>C.2 多元高斯分布<a class="headerlink" href="#c-2" title="Permalink to this headline">¶</a></h2>
<p>从一元高斯分布推广到多元高斯分布，假设各维度之间相互独立，则有联合分布：</p>
<div class="math notranslate nohighlight">
\[
p(\boldsymbol{x})=p(x_1, x_2, ..., x_n) = \prod_{i=1}^{n}p(x_i)=\frac{1}{(2\pi)^{\frac{n}{2}}\sigma_1\sigma_2...\sigma_n}\exp \left(-\frac{1}{2}\left [\frac{(x_1-\mu_1)^2}{\sigma_1^2} + \frac{(x_2-\mu_2)^2}{\sigma_2^2} + ... + \frac{(x_n-\mu_n)^2}{\sigma_n^2}\right]\right) \tag{2}
\]</div>
<br>
<p>其中 <span class="math notranslate nohighlight">\(\mu_1, \mu_2, \cdots\)</span> 和 <span class="math notranslate nohighlight">\(\sigma_1, \sigma_2, \cdots\)</span> 分别是第 1 维、第 2 维… 的均值和方差。可以将上式表示为向量和矩阵形式，令：
<br>
$<span class="math notranslate nohighlight">\(
\boldsymbol{x - \mu}=[x_1-\mu_1, \ x_2-\mu_2,\ … \ x_n-\mu_n]^T \\
K = \begin{bmatrix}
    \sigma_1^2 &amp; 0 &amp; \cdots &amp; 0\\
    0 &amp; \sigma_2^2 &amp; \cdots &amp; 0\\
    \vdots &amp; \vdots &amp; \ddots &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; \sigma_n^2
    \end{bmatrix}
\)</span>$</p>
<p>则有：
$<span class="math notranslate nohighlight">\(\sigma_1\sigma_2...\sigma_n = |K|^{\frac{1}{2}} \)</span>$</p>
<div class="math notranslate nohighlight">
\[\frac{(x_1-\mu_1)^2}{\sigma_1^2} + \frac{(x_2-\mu_2)^2}{\sigma_2^2} + ... + \frac{(x_n-\mu_n)^2}{\sigma_n^2}=(\boldsymbol{x-\mu})^TK^{-1}(\boldsymbol{x-\mu}) \]</div>
<p>代入式（2）得到向量和矩阵形式：
<br>
$<span class="math notranslate nohighlight">\(
p(\boldsymbol{x}) = (2\pi)^{-\frac{n}{2}}|K|^{-\frac{1}{2}}\exp \left( -\frac{1}{2}(\boldsymbol{x-\mu})^TK^{-1}(\boldsymbol{x-\mu}) \right) \tag{3}
\)</span>$</p>
<p>其中 <span class="math notranslate nohighlight">\( \boldsymbol{\mu} \in \mathbb{R}^n \)</span> 是均值向量， <span class="math notranslate nohighlight">\( K \in \mathbb{R}^{n \times n} \)</span> 为协方差矩阵。</p>
<p>注意：式中假设了各维度之间相互独立，因此 <span class="math notranslate nohighlight">\( K \)</span> 是对角矩阵。如果各维度变量存在相关，则上述形式仍然成立，但此时协方差矩阵 <span class="math notranslate nohighlight">\( K \)</span> 不再是对角矩阵，而是半正定的对称矩阵。</p>
<p>式（3）通常也简写为：
$<span class="math notranslate nohighlight">\(
x \sim \mathcal{N}(\boldsymbol{\mu}, K)
\)</span>$</p>
<br>
</div>
<div class="section" id="c-3">
<h2>C.3 无限元高斯分布<a class="headerlink" href="#c-3" title="Permalink to this headline">¶</a></h2>
<p>在多元高斯分布基础上进一步扩展，假设有无限多个维度的随机变量呢？我们用一个例子来展示从一元高斯分布到无限元高斯分布的扩展过程（来源：<a class="reference external" href="http://www.columbia.edu/~jwp2128/Teaching/E6892/papers/mlss2012_cunningham_gaussian_processes.pdf">MLSS 2012: J. Cunningham - Gaussian Processes for Machine Learning</a>）。</p>
<p>（1）一元高斯分布图解</p>
<p>假设在周一到周四的每天 7:00 测 1 次心率，共测得下图所示 4 个点，其可能的高斯分布如图中高瘦的曲线所示。这是一元高斯分布，只有每天 7: 00 心率这一个维度。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/GP-20211010-14-04-29-efff.webp" /></p>
<p>（2）二元以及多元高斯分布</p>
<p>现在不仅考虑在每天 7: 00 测一次心率，也在 8:00 时也做一次测量，这时就问题就由原来一个维度的随机变量，变成两个维度的随机变量。此时如果考察两个时间心率的联合概率分布，就会如下图的二元高斯分布图：</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/GP-20211010-13-20-42-413b.webp" /></p>
<p>上述过程可以进一步扩展到每天每个整点均测量一次心率，则每天当中的整点测量值一起，构成了一个 24 维的高斯概率分布（此时已经很难利用计算机做可视化）。此时，周一到周四中每天的 24 个测量值，作为一个整体（可以视为一个离散函数），可以视为对该 24 维概率分布的一次采样。</p>
<p>（3）无限元的高斯分布</p>
<p>如果我们在每天的无限个时间点都做一次心率测量，则问题变成了下图所示的情况。注意图中的横轴是测量时间，不同颜色的每个线条均代表了在无限元的高斯分布上做的一次采样（或理解为一次采样产生了无限多个位置不重复的样本点），这种由无限多个不重复的样本点构成的采样结果看起来具有函数的表现形式，因此通常也将无限元的高斯分布称为<strong>函数的分布</strong>（即无限元的高斯分布的一次采样对应了一个函数）。这个无限元的高斯分布就是“高斯过程”。按照字面意思可以理解为：一元、二元和多元高斯分布对应于“点的分布” ，而无限元的高斯过程对应于<strong>函数的分布</strong>。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/GP-20211010-14-09-22-27de.webp" /></p>
<p>高斯过程正式地定义为：</p>
<p>对于所有可能的  <span class="math notranslate nohighlight">\( \boldsymbol{x} = [x_1, x_2, \cdots, x_n] \)</span>，如果 <span class="math notranslate nohighlight">\( f(\boldsymbol{t})=[f(x_1), f(x_2), \cdots, f(x_n)] \)</span> 都服从多元高斯分布，则称 <span class="math notranslate nohighlight">\(f\)</span> 是一个高斯过程，表示为：</p>
<div class="math notranslate nohighlight">
\[
f(\boldsymbol{x}) \sim \mathcal{N}(\boldsymbol{\mu}(\boldsymbol{x}), \kappa(\boldsymbol{x},\boldsymbol{x})) \tag{4}
\]</div>
<p>（1） <span class="math notranslate nohighlight">\(\boldsymbol{\mu}(\boldsymbol{x}): \mathbb{R^{n}} \rightarrow \mathbb{R^{n}} \)</span> 被称为均值函数（Mean function），返回各维度的均值。 均值函数可以是 <span class="math notranslate nohighlight">\(\mathbb{R} \rightarrow  \mathbb{R}\)</span> 或 <span class="math notranslate nohighlight">\(\mathbb{R}^D \rightarrow \mathbb{R}\)</span>  的任何函数；通常取  <span class="math notranslate nohighlight">\(\boldsymbol{\mu}(\boldsymbol{x})=0, \forall x\)</span> 。</p>
<p>（2） <span class="math notranslate nohighlight">\( \kappa(\boldsymbol{x},\boldsymbol{x}) : \mathbb{R^{n}} \times \mathbb{R^{n}} \rightarrow \mathbb{R^{n\times n}} \)</span> 被称为协方差函数（ Covariance Function）或核函数（Kernel Function），返回各维度随机变量两两之间的协方差，多个变量一起构成协方差矩阵。 <span class="math notranslate nohighlight">\(\kappa\)</span> 可以是任何有效的 <span class="math notranslate nohighlight">\(\mathbb{R}^D \times \mathbb{R}^D  \rightarrow \mathbb{R}\)</span> 的 Mercer 核。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Mercer 定理： <strong>任何半正定的函数都可以作为核函数。</strong></p>
<p>半正定函数的定义：在训练数据集 <span class="math notranslate nohighlight">\((x_1,x_2,...x_n)\)</span> 基础上， 定义一个元素值为 <span class="math notranslate nohighlight">\(a_{ij} = f(x_i,x_j)\)</span> 的 <span class="math notranslate nohighlight">\(n \times n\)</span> 矩阵，如果该矩阵为半正定，那么函数 <span class="math notranslate nohighlight">\(f(x_i,x_j)\)</span> 就被称为半正定函数。</p>
<p>Mercer 定理核函数的充分条件，而非必要条件。也就是说还有不满足 Mercer 定理的函数也可以是核函数。常见的核函数有高斯核、多项式核等，基于这些核函数，可以通过核函数的性质（如对称性等）进一步构造出新的核函数。支持向量机（SVM） 是目前核方法应用的经典模型。</p>
</div>
<p>根据上述定义，一个高斯过程由一个均值函数和一个协方差函数唯一地定义，并且 <strong>一个高斯过程的有限维度的子集都服从多元高斯分布</strong>  。</p>
<br>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>一元高斯： 单个实数值变量上的分布
多元高斯： 多个实数值变量上的联合分布
高斯过程： 无限个实数值变量构成的函数上的分布</p>
</div>
<br>
</div>
<div class="section" id="c-4">
<h2>C.4 核函数（协方差函数）<a class="headerlink" href="#c-4" title="Permalink to this headline">¶</a></h2>
<p>核函数是一个高斯过程的核心，核函数决定了一个高斯过程的性质。核函数在高斯过程中主要用于衡量任意两个点（在高斯过程语境中，指任意两个随机变量）之间的相似性程度，其离散化表现形式为协方差矩阵（或相关系数矩阵）。目前最常用的核函数是二次高斯核函数，也称为径向基函数（ <a class="reference external" href="https://en.wikipedia.org/wiki/Radial_basis_function_kernel">RBF</a>）。其基本形式如下。其中 <span class="math notranslate nohighlight">\( \sigma \)</span> 和 <span class="math notranslate nohighlight">\( l \)</span> 是高斯核的超参数。</p>
<div class="math notranslate nohighlight">
\[
K(x_i,x_j)=\sigma^2\exp \left( -\frac{\left \|x_i-x_j\right \|_2^2}{2l^2}\right)
\]</div>
<p>以上高斯核函数的 <code class="docutils literal notranslate"><span class="pre">Python</span></code> 实现如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="k">def</span> <span class="nf">gaussian_kernel</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Easy to understand but inefficient.&quot;&quot;&quot;</span>
    <span class="n">m</span><span class="p">,</span> <span class="n">n</span> <span class="o">=</span> <span class="n">x1</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">x2</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">dist_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">m</span><span class="p">,</span> <span class="n">n</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">float</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="n">dist_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">x2</span><span class="p">[</span><span class="n">j</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma_f</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span> <span class="mf">0.5</span> <span class="o">/</span> <span class="n">l</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dist_matrix</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">gaussian_kernel_vectorization</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">sigma_f</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;More efficient approach.&quot;&quot;&quot;</span>
    <span class="n">dist_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">sigma_f</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">/</span> <span class="n">l</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dist_matrix</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">700</span><span class="p">,</span> <span class="mi">800</span><span class="p">,</span> <span class="mi">1029</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">gaussian_kernel_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">sigma</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
</pre></div>
</div>
<p>输出的协方差矩阵为：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mf">100.</span>    <span class="mf">98.02</span>  <span class="mf">80.53</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">98.02</span> <span class="mf">100.</span>    <span class="mf">90.04</span><span class="p">]</span>
 <span class="p">[</span> <span class="mf">80.53</span>  <span class="mf">90.04</span> <span class="mf">100.</span>  <span class="p">]]</span>
</pre></div>
</div>
<br>
</div>
<div class="section" id="c-5">
<h2>C.5 高斯过程可视化<a class="headerlink" href="#c-5" title="Permalink to this headline">¶</a></h2>
<p>下图是高斯过程的可视化，其中蓝线是高斯过程的均值，浅蓝色区域 95% 置信区间（即每个随机变量的方差，由协方差矩阵的对角线元素确定），每条虚线（函数）代表高斯过程的一次采样（此处用 100 维模拟连续的无限维）。左上角第一幅图是零均值高斯过程先验的四个样本，分别用四种不同颜色的曲线表示。后面几幅图分别展示了观测到新数据点时，不同高斯过程样本的更新过程（即更新高斯过程后验的均值函数和协方差函数）的过程。从图中可以看到：<strong>随着数据点（随机变量）数量的增加，所有高斯过程先验的样本最终都会收敛至真实值附近</strong>。 可以做出更进一步的设想： <strong>即使设置不同的高斯过程先验，随着数据点（随机变量）数量的增加，所有高斯过程先验的样本最终也都会收敛至真实值附近</strong>。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/GP-20211010-16-15-26-f373.webp" /></p>
<p>接下来用公式推导上图的过程。</p>
<p><strong>（1）高斯过程先验的设置</strong></p>
<p>将高斯过程先验表示为 <span class="math notranslate nohighlight">\(f(\boldsymbol{x}) \sim \mathcal{N}(\boldsymbol{\mu}_{f}, K_{ff}) \)</span>，对应左上角第一幅图。</p>
<p><strong>（2）高斯过程后验的更新</strong></p>
<p>现在观测到一些数据 <span class="math notranslate nohighlight">\( (\boldsymbol{x^{*}}, \boldsymbol{y^{*}}) \)</span> ，则根据模型假设， <span class="math notranslate nohighlight">\( \boldsymbol{y^{*}} \)</span> 与 <span class="math notranslate nohighlight">\(f(\boldsymbol{x})\)</span> 服从联合高斯分布：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix}
    f(\boldsymbol{x})\\
    \boldsymbol{y^{*}}\\
    \end{bmatrix} \sim \mathcal{N} \left(
    \begin{bmatrix}
    \boldsymbol{\mu_f}\\
    \boldsymbol{\mu_y}\\
    \end{bmatrix},
    \begin{bmatrix}
    K_{ff} &amp; K_{fy}\\
    K_{fy}^T &amp; K_{yy}\\
    \end{bmatrix}
    \right)
\end{split}\]</div>
<p>其中 <span class="math notranslate nohighlight">\(K_{ff} = \kappa(\boldsymbol{x}, \boldsymbol{x}) \)</span> ，<span class="math notranslate nohighlight">\( K_{fy}=\kappa(\boldsymbol{x}, \boldsymbol{x^{*}}) \)</span>，<span class="math notranslate nohighlight">\( K_{yy} = \kappa(\boldsymbol{x^{*}},\boldsymbol{x^{*}})\)</span> ，则有：</p>
<div class="math notranslate nohighlight">
\[
f \sim \mathcal{N}(K_{fy}^{T}K_{ff}^{-1}\boldsymbol{y}+\boldsymbol{\mu_f},K_{yy}-K_{fy}^{T}K_{ff}^{-1}K_{fy}) \tag{5}
\]</div>
<p>公式（5）表明：给定数据 <span class="math notranslate nohighlight">\((\boldsymbol{x^{*}}, \boldsymbol{y^{*}})\)</span> 之后，函数 <span class="math notranslate nohighlight">\( f \)</span> 仍然服从高斯过程分布，具体推导可见 <a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">Gaussian Processes for Machine Learning</a>。</p>
<p>从公式（5）可以看出一些有趣的性质：</p>
<ul class="simple">
<li><p><strong>后验的均值函数 <span class="math notranslate nohighlight">\(K_{fy}^{T}K_{ff}^{-1}\boldsymbol{y}+\boldsymbol{\mu_f}\)</span> 实际上是观测变量 <span class="math notranslate nohighlight">\( \boldsymbol{y^*} \)</span> 的一个线性函数</strong> ；</p></li>
<li><p><strong>后验的协方差函数</strong> <span class="math notranslate nohighlight">\(K_{yy}-K_{fy}^{T}K_{ff}^{-1}K_{fy}\)</span> 的第一部分 <span class="math notranslate nohighlight">\( K_{yy} \)</span> 是先验的协方差，<strong>减掉的那一项表示观测到数据后函数分布不确定性的减少</strong>，当第二项接近于 0时，说明观测数据后不确定性几乎不变，当第二项非常大时，说明不确定性降低了很多。</p></li>
</ul>
<p><strong>（3）概率视角的解释</strong></p>
<p>公式（5）其实就是高斯过程回归的基本公式，从贝叶斯视角来看，首先设置一个高斯过程先验  <span class="math notranslate nohighlight">\(f(\boldsymbol{x}) \sim \mathcal{N}(\boldsymbol{\mu}_{f}, K_{ff}) \)</span>，然后在观测到一些数据后，基于先验和似然（此处的似然假设为 “<strong><span class="math notranslate nohighlight">\( \boldsymbol{y^{*}} \)</span> 与 <span class="math notranslate nohighlight">\(f(\boldsymbol{x})\)</span> 服从联合高斯分布</strong>”），计算得到高斯过程后验（即更新均值函数和协方差函数）。</p>
<br>
</div>
<div class="section" id="c-6">
<h2>C.6 简单高斯过程回归实现<a class="headerlink" href="#c-6" title="Permalink to this headline">¶</a></h2>
<p>本节用代码实现一个简单的高斯过程回归模型。由于高斯过程回归是一种非参数化的方法，每次推断（inference）需要利用所有训练数据进行计算，因此没有显式的训练模型参数的过程，所以拟合（ fit）过程只需要将训练数据记录下来，实际的推断过程在预测（ predict）过程中进行。Python 代码如下</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注：参数模型的贝叶斯过程通常先为模型参数设置先验（或超先验），然后通过拟合学习（推断）出模型参数的后验分布，而后利用最大后验或贝叶斯模型平均得到预测值。而非参数模型没有确定数量的模型参数，其模型参数大多是根据训练数据动态生成的，因此非参数模型的学习过程和预测过程很多时候是纠缠在一起的。</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>


<span class="k">class</span> <span class="nc">GPR</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fit</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_y</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;l&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;sigma_f&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span> <span class="o">=</span> <span class="n">optimize</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># store train data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fit</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_fit</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;GPR Model not fit yet.&quot;</span><span class="p">)</span>
            <span class="k">return</span>

        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">Kff</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">)</span>  <span class="c1"># (N, N)</span>
        <span class="n">Kyy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>  <span class="c1"># (k, k)</span>
        <span class="n">Kfy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>  <span class="c1"># (N, k)</span>
        <span class="n">Kff_inv</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Kff</span> <span class="o">+</span> <span class="mf">1e-8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">)))</span>  <span class="c1"># (N, N)</span>

        <span class="n">mu</span> <span class="o">=</span> <span class="n">Kfy</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Kff_inv</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_y</span><span class="p">)</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="n">Kyy</span> <span class="o">-</span> <span class="n">Kfy</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Kff_inv</span><span class="p">)</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Kfy</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mu</span><span class="p">,</span> <span class="n">cov</span>

    <span class="k">def</span> <span class="nf">kernel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="p">):</span>
        <span class="n">dist_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x1</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">x2</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x1</span><span class="p">,</span> <span class="n">x2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;sigma_f&quot;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mf">0.5</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;l&quot;</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">dist_matrix</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">y</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>

<span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">9</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>
<span class="n">test_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">gpr</span> <span class="o">=</span> <span class="n">GPR</span><span class="p">()</span>
<span class="n">gpr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">cov</span> <span class="o">=</span> <span class="n">gpr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">mu</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">uncertainty</span> <span class="o">=</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;l=</span><span class="si">%.2f</span><span class="s2"> sigma_f=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">gpr</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;l&quot;</span><span class="p">],</span> <span class="n">gpr</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;sigma_f&quot;</span><span class="p">]))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">test_X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">test_y</span> <span class="o">+</span> <span class="n">uncertainty</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">-</span> <span class="n">uncertainty</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<p>结果如下图，红点是训练数据，蓝线是预测值，浅蓝色区域是 95% 置信区间。真实的函数是一个 cosine 函数，可以看到在训练数据点较为密集的地方，模型预测的不确定性较低，而在训练数据点比较稀疏的区域，模型预测不确定性较高。</p>
<p><img alt="https://pic4.zhimg.com/80/v2-5237ebe25306a4a8a241be23893c96a7_720w.jpg" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/GP-20211010-13-21-09-42dc.webp" /></p>
<br>
</div>
<div class="section" id="c-7">
<h2>C.7 超参数优化<a class="headerlink" href="#c-7" title="Permalink to this headline">¶</a></h2>
<p>上文提到高斯过程是一种非参数模型，没有训练模型参数的过程，一旦核函数、训练数据给定，模型就被唯一地确定下来。但注意到核函数本身是有参数的（比如高斯核的参数  <span class="math notranslate nohighlight">\( \sigma \)</span> 和 <span class="math notranslate nohighlight">\( l \)</span>  ），我们称为这种参数为模型的超参数（类似于 k-NN 模型中 k 的取值）。</p>
<p>核函数本质上决定了样本点相似性的度量方法，进而影响到整个函数的概率分布的形状。上面的高斯过程回归的例子中使用了 <span class="math notranslate nohighlight">\( \sigma=0.2 \)</span> 和 <span class="math notranslate nohighlight">\( l=0.5 \)</span> 的超参数，现在可以选取不同的超参数看看回归出来的效果有何不同。</p>
<p><img alt="https://pic2.zhimg.com/80/v2-209ee2a5fefd5f1efdc91878328966c1_720w.jpg" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/GP-20211010-13-21-35-0d2d.webp" /></p>
<p>从上图可以看出，<span class="math notranslate nohighlight">\( l \)</span> 越大函数更加平滑，同时训练数据点之间的预测方差更小，反之 <span class="math notranslate nohighlight">\( l \)</span> 越小则函数倾向于更加“曲折”，训练数据点之间的预测方差更大；<span class="math notranslate nohighlight">\( \sigma \)</span> 则直接控制方差大小，<span class="math notranslate nohighlight">\( \sigma \)</span> 越大方差越大，反之亦然。</p>
<p>如何选择最优的核函数参数 <span class="math notranslate nohighlight">\( \sigma \)</span> 和 <span class="math notranslate nohighlight">\( l \)</span> 呢？答案是：最大化在这两个超参数下 <span class="math notranslate nohighlight">\( \boldsymbol{y} \)</span> 出现的概率，通过 <strong>最大化边缘对数似然（Marginal Log-likelihood）来找到最优的参数</strong>，边缘对数似然表示为：</p>
<div class="math notranslate nohighlight">
\[
\mathrm{log}\ p(\boldsymbol{y}|\sigma, l) = \mathrm{log} \ \mathcal{N}(\boldsymbol{0}, K_{yy}(\sigma, l)) = -\frac{1}{2}\boldsymbol{y}^T K_{yy}^{-1}\boldsymbol{y} - \frac{1}{2}\mathrm{log}\ |K_{yy}| - \frac{N}{2}\mathrm{log} \ (2\pi) \tag{6}
\]</div>
<p>具体实现中，在拟合（fit）方法中增加超参数优化这部分代码，最小化负边缘对数似然。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.optimize</span> <span class="kn">import</span> <span class="n">minimize</span>


<span class="k">class</span> <span class="nc">GPR</span><span class="p">:</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_fit</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_y</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;l&quot;</span><span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s2">&quot;sigma_f&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">}</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span> <span class="o">=</span> <span class="n">optimize</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="c1"># store train data</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

         <span class="c1"># hyper parameters optimization</span>
        <span class="k">def</span> <span class="nf">negative_log_likelihood_loss</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;l&quot;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;sigma_f&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="n">Kyy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">))</span>
            <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">train_y</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">Kyy</span><span class="p">))</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_y</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">slogdet</span><span class="p">(</span><span class="n">Kyy</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">train_X</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimize</span><span class="p">:</span>
            <span class="n">res</span> <span class="o">=</span> <span class="n">minimize</span><span class="p">(</span><span class="n">negative_log_likelihood_loss</span><span class="p">,</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;l&quot;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;sigma_f&quot;</span><span class="p">]],</span>
                   <span class="n">bounds</span><span class="o">=</span><span class="p">((</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">),</span> <span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">)),</span>
                   <span class="n">method</span><span class="o">=</span><span class="s1">&#39;L-BFGS-B&#39;</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;l&quot;</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;sigma_f&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">is_fit</span> <span class="o">=</span> <span class="kc">True</span>
</pre></div>
</div>
<p>将训练、优化得到的超参数、预测结果可视化如下图，可以看到最优的 <span class="math notranslate nohighlight">\( l=1.2 \)</span>，<span class="math notranslate nohighlight">\( \sigma_f=0.8 \)</span> 。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/GP-20211010-13-21-44-6d38.webp" /></p>
<p>这里用 scikit-learn 的 <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor">GaussianProcessRegressor</a> 接口进行对比：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.gaussian_process</span> <span class="kn">import</span> <span class="n">GaussianProcessRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.gaussian_process.kernels</span> <span class="kn">import</span> <span class="n">ConstantKernel</span><span class="p">,</span> <span class="n">RBF</span>

<span class="c1"># fit GPR</span>
<span class="n">kernel</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="n">constant_value</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">constant_value_bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">))</span> <span class="o">*</span> <span class="n">RBF</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">length_scale_bounds</span><span class="o">=</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e4</span><span class="p">))</span>
<span class="n">gpr</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">,</span> <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">gpr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">cov</span> <span class="o">=</span> <span class="n">gpr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">return_cov</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">test_y</span> <span class="o">=</span> <span class="n">mu</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="n">uncertainty</span> <span class="o">=</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov</span><span class="p">))</span>

<span class="c1"># plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;l=</span><span class="si">%.1f</span><span class="s2"> sigma_f=</span><span class="si">%.1f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">gpr</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">k2</span><span class="o">.</span><span class="n">length_scale</span><span class="p">,</span> <span class="n">gpr</span><span class="o">.</span><span class="n">kernel_</span><span class="o">.</span><span class="n">k1</span><span class="o">.</span><span class="n">constant_value</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">test_X</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">test_y</span> <span class="o">+</span> <span class="n">uncertainty</span><span class="p">,</span> <span class="n">test_y</span> <span class="o">-</span> <span class="n">uncertainty</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">test_X</span><span class="p">,</span> <span class="n">test_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;predict&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<p>scikit-learn 得到结果为 <span class="math notranslate nohighlight">\( l=1.2, \sigma_f=0.6 \)</span>，与我们优化的超参数有些许不同，可能是实现细节有所不同导致。</p>
<p><img alt="https://pic2.zhimg.com/80/v2-5771db0caae19ef0f196f77a2fa33151_720w.jpg" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/GP-20211010-13-21-50-6f25.webp" /></p>
<br>
</div>
<div class="section" id="c-8">
<h2>C.8 多维输入<a class="headerlink" href="#c-8" title="Permalink to this headline">¶</a></h2>
<p>我们上面讨论的训练数据的输入都是一维（即每一个变量 <span class="math notranslate nohighlight">\(x_i\)</span> 都是标量，主要用于时间序列分析 ）的，不过高斯过程可以直接扩展到多维输入（ <span class="math notranslate nohighlight">\(x_i\)</span> 是 D 维向量，主要用于多变量影响分析）的情况，只需直接将输入维度增加即可。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">y_2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="o">=</span><span class="mf">0.0</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">y</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">y</span>

<span class="n">train_X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
<span class="n">train_y</span> <span class="o">=</span> <span class="n">y_2d</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">noise_sigma</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">)</span>

<span class="n">test_d1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">test_d2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">test_d1</span><span class="p">,</span> <span class="n">test_d2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">test_d1</span><span class="p">,</span> <span class="n">test_d2</span><span class="p">)</span>
<span class="n">test_X</span> <span class="o">=</span> <span class="p">[[</span><span class="n">d1</span><span class="p">,</span> <span class="n">d2</span><span class="p">]</span> <span class="k">for</span> <span class="n">d1</span><span class="p">,</span> <span class="n">d2</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">test_d1</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">test_d2</span><span class="o">.</span><span class="n">ravel</span><span class="p">())]</span>

<span class="n">gpr</span> <span class="o">=</span> <span class="n">GPR</span><span class="p">(</span><span class="n">optimize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">gpr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">train_y</span><span class="p">)</span>
<span class="n">mu</span><span class="p">,</span> <span class="n">cov</span> <span class="o">=</span> <span class="n">gpr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">mu</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">test_d1</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">Axes3D</span><span class="p">(</span><span class="n">fig</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot_surface</span><span class="p">(</span><span class="n">test_d1</span><span class="p">,</span> <span class="n">test_d2</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">antialiased</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">train_X</span><span class="p">)[:,</span><span class="mi">0</span><span class="p">],</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">train_X</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">],</span> <span class="n">train_y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">train_y</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">test_d1</span><span class="p">,</span> <span class="n">test_d2</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">zdir</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">,</span> <span class="n">offset</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cm</span><span class="o">.</span><span class="n">coolwarm</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;l=</span><span class="si">%.2f</span><span class="s2"> sigma_f=</span><span class="si">%.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">gpr</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;l&quot;</span><span class="p">],</span> <span class="n">gpr</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="s2">&quot;sigma_f&quot;</span><span class="p">]))</span>
</pre></div>
</div>
<p>下面是一个二维输入数据的高斯过程回归，左图是没有经过超参优化的拟合效果，右图是经过超参优化的拟合效果。</p>
<p><img alt="https://pic4.zhimg.com/80/v2-ce3895df03430d8b18b9b640243ab25f_720w.jpg" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/GP-20211010-13-21-55-bb7a.webp" /></p>
<p>以上完整的的代码放在 <a class="reference external" href="https://github.com/borgwang/toys/tree/master/math-GP">toys/GP</a>。</p>
<br>
</div>
<div class="section" id="c-9">
<h2>C.9 高斯过程回归的优缺点<a class="headerlink" href="#c-9" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>优点</p>
<ul>
<li><p>（采用 RBF 作为协方差函数）具有平滑性质，能够拟合非线性数据；</p></li>
<li><p>高斯过程回归天然支持获得<strong>模型关于预测的不确定性</strong>，可以直接输出预测点值的概率分布；</p></li>
<li><p>通过最大化边缘似然的方式，高斯过程回归可以在不需要交叉验证的情况下给出比较好的正则化效果。</p></li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li><p>高斯过程是非参数模型，每次的推断都需要对所有的数据点参与计算（矩阵求逆）。对于没有经过任何优化的高斯过程回归，<span class="math notranslate nohighlight">\(n\)</span> 个样本点时间复杂度大概是 <span class="math notranslate nohighlight">\( \mathcal{O}(n^3) \)</span>，空间复杂度是 <span class="math notranslate nohighlight">\( \mathcal{O}(n^2) \)</span>，在数据量大的时候高斯过程变得 intractable；</p></li>
<li><p>高斯过程回归中，先验是一个高斯过程分布，似然是高斯分布，根据高斯的共轭特性，其后验仍是一个高斯过程分布。在似然不服从高斯分布的问题中（如分类），需要对得到的后验进行近似使其仍可以是一个高斯过程；</p></li>
<li><p>径向基函数（RBF）是最常用的协方差函数，但实际中通常需要根据问题和数据性质选择恰当的协方差函数。</p></li>
</ul>
</li>
</ul>
<br>
</div>
<div class="section" id="id1">
<h2>参考资料<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">Carl Edward Rasmussen - Gaussian Processes for Machine Learning</a></p></li>
<li><p><a class="reference external" href="http://www.columbia.edu/~jwp2128/Teaching/E6892/papers/mlss2012_cunningham_gaussian_processes.pdf">MLSS 2012 J. Cunningham - Gaussian Processes for Machine Learning</a></p></li>
<li><p><a class="reference external" href="http://krasserm.github.io/2018/03/19/gaussian-processes/">Martin Krasser’s blog- Gaussian Processes</a></p></li>
<li><p><a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html">scikit-learn GaussianProcessRegressor</a></p></li>
</ul>
<p><br><br></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="Append-02-VariationalInference_Tutorial.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">附录 B： 变分法确定性推断</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="Append-04-BayesianNN_Tutorial.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">附录 D：贝叶斯神经网络概述</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Osvaldo Martin<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>