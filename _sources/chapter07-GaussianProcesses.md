 

#  第7章　高斯过程



在上一章中，我们学习了Dirichlet过程，它是Dirichlet分布的无限维推广，可用于设置未知连续分布的先验。在本章中，我们将学习高斯过程，这是高斯分布的无限维推广，可用于设置未知函数的先验。在贝叶斯统计中，Dirichlet过程和Gauss过程都被用来建立灵活的模型，其中允许参数的数量随着数据的大小而增加。

本章我们将学习：

-  函数作为概率对象
- 核
- 具有高斯似然的高斯过程
- 具有非高斯似然的高斯过程

## 7.1 线性模型和非线性数据

**7.1**　非参统计

非参统计通常用来描述一类不依赖于参数化概率分布的统计工\
 具/模型。根据这个定义，贝叶斯统计似乎不可能是非参的，因为前

 面我们学到过，贝叶斯统计的第一步就是在概率模型中准确地将概率 分布组合在一起。第1章中说过，概率分布是构建概率模型的基石。 在贝叶斯框架中，非参模型是指包含有无限多参数的模型，因此，我 们将参数可以随着数据大小而变化的模型称作非参数化模型。对于非 参数化模型而言，理论上其参数个数是无限的，实际使用中会根据数 据将其收缩到一个有限的值，从而让数据本身来决定参数的个数。

 286

 **7.2**　基于核函数的模型

 核函数方法相关的研究是一个非常高产而且活跃的领域，有许多 书讨论这个主题，其流行也是因为核函数具有一些有趣的数学特点。 这里只需要知道，核函数可以作为非线性模型的基础，并且相对来说 比较容易计算。比较流行的核函数方法有两种：支持向量机

 （Support Vector Machine，SVM）和高斯过程。后者是一种概率方\
法，也正是本章将要介绍的主题，前者不是概率的方法，这里不做深 入讨论，你可以阅读Jake Vanderplas写的《Python Data Science\
 Handbook》和Sebastian Raschka写的《Python Machine Learning\
 Bayesian Analysis with Python》这两本书了解更多。在继续深入之\
前，我们先了解一下什么是核以及如何使用。

你可能会发现，在统计学中，核函数的定义不止一种，并且根据 定义的不同，核函数的数学特性也有所不同。结合本章讨论的目的， 这里将核函数看作是一个对称函数，接受两个输入并返回一个永远为 正数的值，从而可以将核函数的输出看作是两个输入变量之间的相似 度。

核函数有许多种，其中某些经过特殊改造后能很好地用于解决图 像识别、文档分析等问题，还有些核函数适用于对周期性的函数建 模。有一个核函数在统计学和机器学习中非常流行，叫做高斯核，也 称作高斯径向基函数。

**7.2.1**　高斯核函数

高斯核函数的定义如下：

 其中称为欧氏距离的平方（Squared Euclidean Distance， SED），对于一个*n*维的空间，我们有：

 

 注意，如果计算欧几里得距离的话需要求平方根。SED并不满足 三角不等式，因而并不是真实的距离，不过在许多常见的问题中，我 们只需要对不同的SED进行比较，例如，计算一些点中的最小欧氏距 离等价于找到最小的SED。

  

有一点不太明显，高斯核函数与高斯分布的数学形式其实有一些 相似之处，将常数项去掉之后，可以得到，其中，*w*控制着核 函数的宽度，在这里正比于方差，因而有时候也称作带宽。

 **7.2.2**　核线性回归

 前面我们学习了线性回归模型的基本形式：

![](media/image2233.png){width="1.0932053805774278in" height="0.187419072615923in"}

 ![](media/image2234.png){width="0.10411417322834646in" height="0.10412182852143483in"}其中， 是噪声或者误差，通常是高斯分布。

![](media/image2235.png){width="1.6762489063867017in" height="0.3331900699912511in"}

 这里我们用*f* (.)来表示线性函数（没有噪声），如果使用了其他 逆连结函数（比如第5章中的逻辑函数），我们就将它包含在*f* (.)内。

 ![](media/image2236.png){width="0.10411417322834646in" height="0.12494641294838145in"}这里向量表示一个系数向量，通常是一个系数和一个或多个斜率。

 在第4章中，我们介绍了多项式回归的概念，同时提到了，多项 式回归的唯一实际用途可能就是作为统计学的教学科研工具（至少对

 288

 于2次或3次以上的多项式回归）。此外我们还学习了如何将多项式回 归用于非线性数据的建模过程中。

 这里我们将多项式回归表示成如下形式：

![](media/image2237.png){width="1.2597889326334208in" height="0.2707163167104112in"}

 其中，*φ*函数表示一系列阶数逐渐增加的多项式，将向量***x***转成矩 阵后，矩阵的每一列都是向量***x***的幂次（逐渐增加），从而有效地将 数据映射到了更高维度的空间中，然后再从高维空间中找到一条直线 去拟合数据，最后将该直线投影回原始的低维空间时，不一定仍然是 直线，而有可能是一条曲线。整个过程通常称作将输入投影到特征空 间。

 函数*φ*不一定是多项式，还有其他形式的函数将输入向量映射到 （高维的）特征空间中，在此条件下，除了使用*φ*函数之外，我们还 可以将其替换成一个核函数。尽管在数学形式上是等价的，使用核函

 数会让计算更容易一些，这通常称作核函数技巧。这也是为什么核函 数是许多统计学方法和机器学习方法中的核心概念的主要原因，而特 征空间的概念在实际中反而不那么重要了（尽管在学习核函数方法的 时候还是能提供一些直观的解释）。

 继续我们的讨论，这里不过多深入其中的数学细节，我们将φ替 换成一个核函数*K*，这里用*K*来表示高斯核函数，于是得到：

![](media/image2239.png){width="1.7178947944007in" height="0.35401465441819774in"}

 注意，这里用*x*来表示数据，此外还有一个向量叫做*x*′，后者也 是一个向量，通常称作结或者中心点，它均匀地分布在数据的范围 内。一个特例是*x* = *x*′，换句话说，我们完全可以将数据点作为结。

 289

 ![](media/image2241.png){width="0.5518077427821523in" height="0.21865594925634296in"}我们为什么要增加这些点呢？在直接回答这个问题之前，我们先 看一下如何使用结。注意，如果结距离原数据越近，核函数的返回值 越大。为了简化分析，假设*w*=1，然后如果，那么有

 ![](media/image2242.png){width="0.166583552055993in" height="0.21865594925634296in"}；如果和相隔较远，那么 。换句话说，由\
 于高斯核函数的输出衡量的是相似性，因此我们可以说，如果和 比较接近，那么函数这些点的均值也很接近，即。如果对改 动一点点，那么 也会相应地变化，*x*的变化越大，那么 的变化也就 越大。现在思考一下，我们可以将其看作是模型的一种特性，经验告 诉我们，有很多函数都有类似的表现，事实上，这类函数有一个名 字，称作平滑函数。尽管有些特例，不过许多问题都可以用平滑函数 来近似。

 ![](media/image2252.png){width="0.10411417322834646in" height="0.12494641294838145in"}进一步解释一下我们在做的事情，我们可以看作是在尝试使用第 1章和第2章中见过的网格搜索法去拟合一个平滑的未知函数，其中网 格的格点是*x*′，对于每个结，我们都有一个高斯函数，根据数据（通 过）增加或者降低这些高斯函数的权重。如果将所有的高斯函数都

 ![](media/image2253.png){width="0.10411417322834646in" height="0.11453412073490814in"}加起来，那么就得到了一个平滑的曲线并拟合出了。这种解释称作 权重空间视角。在本章接下来的高斯过程一节中，我们将看到描述该 问题的另一种视角：函数空间视角。

 现在，将前面所有的想法汇总在一起，并应用到一个简单的合成 数据集上，其中因变量是一个sin函数，自变量是一系列从均匀分布 中得到的点。代码实现如下：

 np.random.seed(1)

 x = np.random.uniform(0, 10, size=20)

 y = np.sin(x)

 plt.plot(x, y, \'o\')

 plt.xlabel(\'\$x\$\', fontsize=16)\
 plt.ylabel(\'\$f(x)\$\', fontsize=16, rotation=0)

 290

![](media/image2259.png){width="5.01833552055993in" height="3.3527307524059493in"}

 这里定义一个函数来计算模型中的高斯核函数，代码实现如下：

 def gauss_kernel(x, n_knots=5, w=2):

 \"\"\"

 Simple Gaussian radial kernel

 \"\"\"

 knots = np.linspace(np.floor(x.min()), np.ceil(x.max()), n_knots) return np.array(\[np.exp(-(x-k)\*\*2/w) for k in knots\])

 ![](media/image2264.png){width="0.10411417322834646in" height="0.12494641294838145in"}剩下的就是定义系数了，系数的个数应该与结的个数一致。注 意，这里系数决定了估计的曲线在结这里应该增加还是降低。

 有时候将模型表示成线性回归更合理，对应的数学形式如下：

![](media/image2267.png){width="2.51957895888014in" height="0.35401465441819774in"}

 不过这里我们将省略截距*α*和斜率*β*，只保留最后一项。这里使用 柯西分布作为先验，稍后我们会讨论在使用核函数方法过程中选择先 验的一些关键点，现在先运行如下模型：

 ![](media/image2267.png){width="3.818897637795276e-2in" height="0.8399300087489063in"}with pm.Model() as kernel_model:

 gamma = pm.Cauchy(\'gamma\', alpha=0, beta=1, shape=n_knots) sd = pm.Uniform(\'sd\',0, 10)

 mu = pm.math.dot(gamma, gauss_kernel(x, n_knots))

 291 ![](media/image2271.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}

 ![](media/image2273.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"} yl = pm.Normal(\'yl\', mu=mu, sd=sd, observed=y)\
 kernel_trace = pm.sample(10000, step=pm.Metropolis()) chain = kernel_trace\[5000:\]

 pm.traceplot(chain);

![](media/image2277.png){width="5.643024934383202in" height="1.7492508748906386in"}

 可以看到模型的结果与sin函数很接近，接下来对得到的模型进 行后验检查，代码实现如下：

 ppc = pm.sample_ppc(chain, model=kernel_model, samples=100)

 plt.plot(x, ppc\[\'yl\'\].T, \'ro\', alpha=0.1)

 plt.plot(x, y, \'bo\')

 plt.xlabel(\'\$x\$\', fontsize=16)\
 plt.ylabel(5\$f(x)\$\', fontsize=16, rotation=0)

![](media/image2282.png){width="5.01833552055993in" height="3.342318460192476in"}

 模型似乎能够很好地捕捉到数据，现在我们再用之前未使用的数

 292

 据检查模型的效果，代码实现如下：

 new_x = np.linspace(np.floor(x.min()), np.ceil(x.max()), 100) k = gauss_kernel(new_x, n_knots)

 gamma_pred = chain\[\'gamma\'\]

 for i in range(100):

 idx = np.random.randint(0, len(gamma_pred))

 y_pred = np.math.dot(gamma_pred\[idx\], k)\
 plt.plot(new_x, y_pred, \'r-\', alpha=0.1)\
 plt.xlabel(\'\$x\$\', fontsize=16)

 plt.ylabel(\'\$f(x)\$\', fontsize=16, rotation=0)

 plt.plot(x, y, \'bo\');

![](media/image2287.png){width="5.01833552055993in" height="3.373555336832896in"}

 图中我们用蓝色的点表示数据，用红色的线表示拟合的曲线。你 可能会想要修改带宽和结的个数来看看模型的效果，本章的练习1中 有相关问题的描述，此外本章的练习2中还会探讨拟合另外一类函数 的效果。

 **7.2.3**　过拟合与先验

 对于使用核函数方法的模型而言，人们常常关心的一点是如何选 择结的个数以及位置。一种方法是直接选择数据点作为结，即将高斯 分布放在数据点上，这里可以回想一下前面用过的KDE作图方法。

 293

 另外一种做法是，在建模过程中决定结的个数和位置。这种做法\
 需要一些特殊的计算方法，不是很通用，或者至少不那么容易处理。

 ![](media/image2290.png){width="0.10411417322834646in" height="0.13535761154855644in"}还有一种做法是应用变量选择，其思想是引入一个模型索引变\
 量，该向量的大小与参数的大小保持一致，每个元素的值只有两种

 ![](media/image2291.png){width="0.10411417322834646in" height="0.13535761154855644in"}可能------0或者1。采用这种方法，我们可以决定模型中对应系数的开 和关。不过这种方法的一个问题是，只对低维度的问题有效，因为可 能的索引组合是系数个数的2*^n^*倍。一种替代方案是使用正则先验，我 们希望先验集中在0附近，从而将系数拉向0，同时让具有长尾，从 而避免拉得太厉害了。一种正则先验是柯西分布，另外一种是拉普拉 斯分布。回忆一下第6章中我们讨论过的正则先验------岭回归和

 LASSO回归。

 294

 **7.3**　高斯过程

 前面简要介绍了如何用核函数构建统计模型去描述任意函数。也 许核函数回归听起来好像有点带有技巧性，而且其中需要指定结的个 数以及分布，这似乎有点问题。现在，我们将从另外一个角度使用核 函数，直接在函数空间做推断，该方法基于高斯过程，在数学上和计 算量方面都更受欢迎。

 在解释高斯过程之前，先想想什么是函数？函数可以看作是输入 到输出之间的映射关系。一种学习该映射的方法是将其限定为一条直 线，如我们在第4章做的那样，然后用贝叶斯的知识去推断出决定那

 条直线的参数。不过假设我们不希望将其限定为一条直线，也可以是 任意可能的函数。通常在贝叶斯统计中，对于一个不知道的量，我们 先给其设置一个先验，因此，在我们不知道怎样的函数才是一个对数 据拟合得很好的模型时，我们需要对函数设置一个先验。有趣的是， 多元高斯就是这样一个先验（实际上是一个与之相似的东西，不过我 们暂且这么认为），可以用一个多元高斯从很宽泛的（但很有用的） 角度来描述一个函数。我们认为对于每个变量*x~i~*都存在一个高斯变量

 *y~i~*，其均值和标准差暂不清楚。这样，如果向量***x***的长度为*n*，那么就

 得到一个*n*元高斯分布。

 在真实世界中，对于一个实数范围内的映射函数，***x***和***y***实际上是 无限多的，因为两个点之间存在着无穷多个点。因此，理论上我们需 要一个无限元的高斯分布，这在数学上称为高斯过程，是一个参数化 的均值函数和一个协方差函数，此时我们有：

![](media/image2295.png){width="2.311349518810149in" height="0.187419072615923in"}

 295

 关于高斯过程的一个正式定义是说，连续空间中的每个点都有一 个与之对应的正态分布的变量，而高斯过程就是这无限多个随机变量 的联合分布。其中均值函数是一个无限维向量的均值，协方差函数则 是一个无限维的协方差矩阵，我们将看到协方差矩阵可以有效地建

 模***x***的变化量与***y***的变化量之间的关系。

 ![](media/image2297.png){width="0.45810476815398077in" height="0.187419072615923in"}总结一下，前面几章中，我们学习了如何估计，比如，在\
 线性回归中我们假设 ，其中，*f*是一个线性模型，然后估计

 ![](media/image2299.png){width="0.5726312335958005in" height="0.187419072615923in"}线性模型的参数。也就是说，我们是一个线性模型，并估计出 了其中的参数，从而得到了，后面将会看到，我们仍然需要估 计参数，不过从概念上讲，我们是在直接处理函数，这样思考更容易 理解一些。

 **7.3.1**　构建协方差矩阵

 在实践中，高斯过程的均值函数通常设为0（尽管这并非一定\
 的），因而高斯过程的整个行为都受协方差矩阵的控制，所以这里重

 点关注如何构建协方差函数。

 从高斯过程先验中采样

 高斯过程的概念有点像是脚手架，在实际使用中我们通常不直接 使用该无限维的对象，相反，我们将无限维的高斯过程收缩到一个有 限维的多元高斯。数学上，这是通过对模型中剩余的（无限的）未观 测维度进行边缘化得到的，这样做之后就可以得到一个多元高斯分

 布。这么做遵循高斯过程的定义，即作为一系列随机变量，其中任意 有限的子集都有一个联合高斯分布，于是得到的多元高斯分布的维度 与我们现有数据的个数相同！这样，对于一个零均值函数的高斯过

 程，我们有：

 296

![](media/image2302.png){width="3.6960772090988625in" height="0.19783136482939634in"}

 ![](media/image2303.png){width="0.6246883202099738in" height="0.187419072615923in"}现在我们搞定了无限维这个棘手的问题，接下来继续处理协方差 矩阵，注意，我们将协方差矩阵写成了，这里故意写成了前面 核函数回归例子中的定义，因为我们实际上是用核函数在构建协方差 函数。协方差函数描述的是，当***x***变化时，***y***是如何随之变化的。前面 核函数回归中，我们已经知道了使用高斯核函数是一个不错的假设， 其等价的含义是说，一个小的*x~i~*扰动会导致较小的*y~i~*变化，而一个较

 大的*x~i~*变化则会导致*y~i~*（平均来看）较大的变化。

 为了从直观上了解高斯过程先验是什么样的，我们将对其采样并 画出来。下图根据高斯过程先验画出了6个任意的函数（或者叫实 现）。注意，这里我们将实现画成了连续函数，实际上，我们只有每 个*x*~\*~以及与之对应的*f* (*x*~\*~)，不过根据平滑性假设以及无法计算出无限

 多个点这一事实，将实现中的点转换成连续的函数也是合理的。只要 测试点足够多，对应的结果就能准确反映出函数的真实形状。

 np.random.seed(1)

 test_points = np.linspace(0, 10, 100)

 cov = np.exp(-squared_distance(test_points, test_points))\
 plt.plot(test_points, stats.multivariate_normal.rvs(cov=cov, size=6). T)

 plt.xlabel(\'\$x\$\', fontsize=16)\
 plt.ylabel(\'\$f(x)\$\', fontsize=16, rotation=0)

 297

![](media/image2309.png){width="5.01833552055993in" height="3.3943799212598424in"}

 从这张图可以看出，高斯过程先验（以及一个高斯核函数）是一 系列在0附近的平滑函数。

 使用参数化的核函数

 为了从数据中学到关于未知函数的信息，我们用一个参数化的核 函数定义了协方差矩阵。我们将这里的参数称为超参。原因有如下两 点：

 这些参数是高斯过程的先验；\
 这个名字强调了我们使用的是非参方法。

 通过学习高斯过程的超参数，我们希望拟合出未知函数。

 前面我们已经提到了，核函数有很多种选择，其中最常见的是高 斯核函数。前面已经看到了高斯核函数的一个版本（即带宽）。现 在，我们介绍另外一个版本，用到了其他两个参数，我们可以将其写 成如下形式：

 298

![](media/image2313.png){width="1.9677701224846895in" height="0.5310225284339457in"}

 ![](media/image2314.png){width="0.7079800962379702in" height="0.20824365704286965in"}其中，*D*是SED，即； 是一个控制垂直尺度的参数，用 于让协方差矩阵建模*f* (***x***)更大或者更小的值；参数*ρ*是带宽，前面已 经知道了，*ρ*控制的是函数的平滑程度；参数*σ*控制的是数据中的噪 声。

 让我们一起讨论下*σ*以及为什么当*i*和*j*不同时使用另外一个表达\
 式。在一些场景中，比如进行插值的时候，我们希望模型对于每个观

 测值*x~i~*都返回对应的观测量*f* (*x~i~*)而没有任何不确定性，而在另外一些 场景中，比如本书涉及的例子中，我们希望得到*f* (*x~i~*)估计值的不确定

 性，因此希望得到一个接近于观测值但却又不是完全一样的值。于是 有：

![](media/image2233.png){width="1.0932053805774278in" height="0.187419072615923in"}

 ![](media/image2317.png){width="0.8641524496937882in" height="0.187419072615923in"}其中，误差项通过 建模。

 因此协方差矩阵的构成需要考虑到有噪声的数据，于是有：

![](media/image2317.png){width="2.2801148293963256in" height="0.20824365704286965in"}

 其中：

![](media/image2319.png){width="1.1973206474190725in" height="0.5101979440069991in"}

 ![](media/image2320.png){width="0.176994750656168in" height="0.1770067804024497in"}称作delta分布，也就是说，我们通过将协方差矩阵的对角线上 的值设置为不是正好等于1的值来对噪声建模，事实上，我们是直接 从数据中估计出对角线上的值。这么做相当于给模型加入了一个扰动 项，之所以加入该项是因为，在核函数所附带的假设中，两个输入越 接近，对应的输出也越接近，极限情况下，两个输入完全相等，它们

 299

 的输出也应该完全相等，而增加扰动项则有利于捕捉到观测值的不确 定性。

 为了更好地理解超参的含义，我们可以把扩展后的高斯核函数画\
 出来，你也可以随意选一些不同的超参值自己试试，代码实现如下：

 np.random.seed(1) eta = 1.5

 rho = 0.2

 sigma = 0.007

 D = squared_distance(test_points, test_points)

 cov = eta \* np.exp(-rho \* D) diag = eta + sigma

 np.fill_diagonal(cov, diag)

 for i in range(6):

 plt.plot(test_points, stats.multivariate_normal.rvs(cov=cov)) plt.xlabel(\'\$x\$\', fontsize=16)

 plt.ylabel(\'\$f(x)\$\', fontsize=16, rotation=0)

![](media/image2326.png){width="5.01833552055993in" height="3.4047911198600174in"}

 **7.3.2**　根据高斯过程做预测

 300

 接下来学习如何用高斯过程做预测。高斯过程的一个优点就是结 果可以从数学分析上直接推导。如果将高斯过程先验与高斯似然组合 在一起，我们得到的是一个高斯过程后验。也就是说，如果在一些测 试点上应用条件高斯，我们可以得到如下后验预测密度的表达式：

![](media/image2327.png){width="2.623694225721785in" height="0.8954494750656168in"}

 在给定数据***X***、***y***以及一些测试点***x***~\*~条件下，计算未知函数在（暂 时）未知的点上的值*f* (***x***~\*~)。注意，这里我们使用符号\*来表示所有测

 试点上的运算，其中：

 ***K***=*K*(***X***,***X***)

 ***K***~\*\*~=*K*(***X***~\*~,***X***~\*~)

 ***K***~\*~=*K*(***X***,***X***~\*~)

 这个式子初看可能有点吓人，稍后我们将看到如何用代码描述上 面的式子（但愿这样理解起来清晰一些），不过现在我们先从直观上 理解这些式子。

 ![](media/image2329.png){width="0.15617125984251967in" height="0.187419072615923in"}***X***~\*\*~是***x***~\*~自己的协方差，其实就是***x***~\*~的方差。也就是说，测试点\
 的方差其实就是先验的方差，注意，这里等价于***K***~\*\*~减去一个量，

 这个式子的含义是说，我们可以根据数据将先验的方差降低这么多。 此外，对于一个距离数据点*x~i~*较远的测试点***x***~\*~而言，核函数返回的值

 接近于0，因而这个量也接近于0，结果就是先验的方差也不会降低多 少。换句话说，评估远离数据点的函数并不会降低不确定性。因此， 推断基于的是数据在测试点附近的局部特性。

 301

 如果你想知道更多有关多元高斯的数学特性及前面的表达式是如 何推导的，可以参考本章的阅读更多部分，我在那里添加了一些你可 能感兴趣的参考资料。针对当前的讨论内容，我们暂且假设前面的后 验预测的表达式是已知的。有一点需要注意的是，我们可以从高斯过 程后验中进行采样。我们得到的这个描述未知函数的表达式非常有

 用，不过有一个问题是，计算过程中涉及求解矩阵的逆，而计算逆矩 阵的复杂度是*O*(*n*^3^)，如果你不知道这个表达式的含义，可以理解为\
 该操作很慢，因此实际使用中，我们无法将高斯过程应用到超过几千 个数据点的场景，不过对于这种情况有一些近似的方法来加速计算， 这里暂不深入讨论。此外，在实践中，直接求逆矩阵可能会有一些数 值问题，而且不稳定，因此更倾向于使用一些替代方案，比如用\
 **Cholesky**分解来计算均值和协方差后验函数。Cholesky分解有点像我 们熟悉的对标量求平方根，不过其对象是矩阵。

 在深入Cholesky分解和直接求逆矩阵的例子之前，我们先观察\
 下。如果求逆矩阵会有问题，那么为什么还要将高斯过程表示成协方

 差矩阵而不是直接用协方差矩阵的逆呢？原因是，当我们使用协方差 矩阵的时候，对其子集的计算是独立于其余部分的计算的，因而它是 独立于未观测到的点的计算。不过对于逆协方差矩阵而言，数据子集 的计算会依赖于我们是否观测到了其余点。只有使用协方差矩阵（而 不是它的逆），我们才能得出高斯过程基于一系列随机变量一致的定 义。

 总结一下，为了完全从贝叶斯的角度使用高斯过程去近似一个函 数，我们需要：

 选择一个核函数构建一个多元分布的协方差矩阵； 用贝叶斯统计的方法推断出核函数的参数；

 302

 计算出每个测试点的均值和标准差。

 注意，实际上我们并没有真正地计算出高斯过程，只是借用了这 个数学概念来确保我们所做的是合理的，在实践中，所有的计算都是 通过多元高斯完成的。

 读完前面理论方面的长篇大论之后，终于到了大家期待的时刻， 我们将把这些想法转化成代码。首先，假设已经知道了核函数的参 数，然后用代码描述后验的表达式。用到的超参和test_points的值

 与前面定义的相同，数据也与前面核函数回归例子中用到的一样，代 码实现如下：

 np.random.seed(1)

 K_oo = eta \* np.exp(-rho \* D)

 D_x = squared_distance(x, x) K = eta \* np.exp(-rho \* D_x) diag_x = eta + sigma

 np.fill_diagonal(K, diag_x)

 D_off_diag = squared_distance(x, test_points) K_o = eta \* np.exp(-rho \* D_off_diag)

 mu_post = np.math.dot(np.math.dot(K_o, np.linalg.inv(K)), y)

 SIGMA_post = K_oo -- np.math.dot(np.math.dot(K_o, np.linalg.inv(K)), K_o .T)

 for i in range(100):

 fx = stats.multivariate_normal.rvs(mean=mu_post, cov=SIGMA_post) plt.plot(test_points, fx, \'r-\', alpha=0.1)

 plt.plot(x, y, \'o\')

 plt.xlabel(\'\$x\$\', fontsize=16)\
 plt.ylabel(\'\$f(x)\$\', fontsize=16, rotation=0)

 303

![](media/image2340.png){width="5.01833552055993in" height="3.425615704286964in"}

 这里通过叠在一起的一些红线（从高斯过程中得到的实例）来表 示不确定性。从图中可以看到，在数据点附近的不确定性要小一些， 而在最后两个数据点之间的不确定性要大一些，在最右边没有数据点 的地方（*x*～9）不确定性要更大一些。

 接下来，我们将重新实现前面的计算过程，不过这次用的是\
 Cholesky分解。下面的代码是根据 Nando de Freitas讲授机器学习课程

 中的实例修改后得到的，在代码中，*N*表示数据点的个数，*n*表示测\
 试点的个数。

 ![](media/image2341.png){width="3.8190069991251095e-2in" height="2.839073709536308in"}np.random.seed(1) eta = 1

 rho = 0.5

 sigma = 0.03

 f = lambda x: np.sin(x).flatten()

 def kernel(a, b):

 \"\"\" GP squared exponential kernel \"\"\"

 sqdist = np.sum(a\*\*2,1).reshape(-1,1) + np.sum(b\*\*2,1) - 2\*np.dot(a , b.T)

 return eta \* np.exp(- rho \* sqdist)

 N = 20

 304

 ![](media/image2345.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}n = 100

 X = np.random.uniform(0, 10, size=(N,1)) y = f(X) + sigma \* np.random.randn(N)

 K = kernel(X, X)

 L = np.linalg.cholesky(K + sigma \* np.eye(N))

 test_points = np.linspace(0, 10, n).reshape(-1,1)

 Lk = np.linalg.solve(L, kernel(X, test_points)) mu = np.dot(Lk.T, np.linalg.solve(L, y))

 K\_ = kernel(Xtest, Xtest)

 sd_pred = (np.diag(K\_) - np.sum(Lk\*\*2, axis=0))\*\*0.5

 plt.fill_between(test_points.flat, mu-2\*s, mu+2\*s, color=\"r\", alpha=0.2

 )

 plt.plot(test_points, mu, \'r\', lw=2)\
 plt.plot(x, y, \'o\')

 plt.xlabel(\'\$x\$\', fontsize=16)\
 plt.ylabel(\'\$f(x)\$\', fontsize=16, rotation=0)

![](media/image2349.png){width="5.01833552055993in" height="3.3319061679790027in"}

 这里我们将数据点用蓝色的点来表示，均值函数是一条红线，不 确定性用半透明的区域来表示。

 **7.3.3**　用**PyMC3**实现高斯过程

 305

 总结一下，我们有如下高斯先验：

![](media/image2351.png){width="2.946450131233596in" height="0.19783136482939634in"}

 一个高斯似然：

![](media/image2352.png){width="2.196823053368329in" height="0.2498928258967629in"}

 以及一个高斯过程后验：

![](media/image2353.png){width="2.675751312335958in" height="0.20824365704286965in"}

 记住，在实践中，我们使用的是多元高斯，因为在一个有限的数 据集上，高斯过程就是一个多元高斯。

 我们将使用贝叶斯相关的原理来学习协方差矩阵的超参。你会看 到，尽管使用PyMC3很简单，但是现在代码量会增加一些，我们需 要手动求解矩阵的逆（或者计算Cholesky分解）。

 很可能在不久的未来，PyMC3中会出现一个高斯过程模块，让\
 高斯过程模型的构造更简单一些。说不定在你阅读本书的此刻，高斯

 过程模型已经有了！^\[1\]^

 下面的模型是从Chris Fonnesbeck写的Stan代码中改造的：

 ![](media/image2354.png){width="3.819444444444445e-2in" height="2.7349507874015746in"}with pm.Model() as GP:

 mu = np.zeros(N)

 eta = pm.HalfCauchy(\'eta\', 5)\
 rho = pm.HalfCauchy(\'rho\', 5)\
 sigma = pm.HalfCauchy(\'sigma\', 5)

 D = squared_distance(x, x)

 K = tt.fill_diagonal(eta \* pm.math.exp(-rho \* D), eta + sigma)

 obs = pm.MvNormal(\'obs\', mu, tt.nlinalg.matrix_inverse(K), observed =y)

 test_points = np.linspace(0, 10, 100)

 306

 ![](media/image2357.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"} D_pred = squared_distance(test_points, test_points) D_off_diag = squared_distance(x, test_points)

 K_oo = eta \* pm.math.exp(-rho \* D_pred)\
 K_o = eta \* pm.math.exp(-rho \* D_off_diag)

 mu_post = pm.Deterministic(\'mu_post\', pm.math.dot(pm.math.dot(K_o, tt.nlinalg.matrix_inverse(K)), y))

 SIGMA_post = pm.Deterministic(\'SIGMA_post\', K_oo -- pm.math.dot(pm.m ath.dot(K_o, tt.nlinalg.matrix_inverse(K)), K_o.T))

 start = pm.find_MAP()

 trace = pm.sample(1000, start=start)

 varnames = \[\'eta\', \'rho\', \'sigma\'\] chain = trace\[100:\]\
 pm.traceplot(chain, varnames)

![](media/image2362.png){width="5.643024934383202in" height="2.696761811023622in"}

 如果留心的话，你会注意到估计出来的参数（*η*，*ρ*，*σ*）的均值 就是我们前面例子中用到的值。这也解释了为什么拟合的效果这么 好，这些超参可不是从我们脑袋里想出来的！

 pm.df_summary(chain, varnames).round(4)

+-------+--------+--------+----------+---------+----------+
|       |  mean | sd     | mc_error | hpd_2.5 | hpd_97.5 |
+=======+========+========+==========+=========+==========+
|  eta | 2.5798 | 2.5296 | 0.1587   | 0.1757  | 6.3445   |
+-------+--------+--------+----------+---------+----------+

![](media/image2367.png){width="3.818678915135608e-2in" height="0.25684601924759404in"}

 ![](media/image2379.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}307

 ![](media/image2386.png){width="2.7777777777777776e-2in" height="2.7777777777777776e-2in"}rho 0.1288 0.0485 0.0027 0.0589 0.2290

+---------+--------+--------+--------+--------+--------+
|  sigma | 0.0006 | 0.0003 | 0.0000 | 0.0002 | 0.0012 |
+---------+--------+--------+--------+--------+--------+

 后验预测检查

 现在我们把原始数据和从高斯过程后验中得到的实例都画出来， 注意，我们还画出了超参的不确定性，而不只是它们的均值，代码实 现如下：

 y_pred = \[np.random.multivariate_normal(m, S) for m,S in zip(chain\[\'mu\_ post\'\]\[::5\], chain\[\'SIGMA_post\'\]\[::5\])\]

 for yp in y_pred:

 plt.plot(test_points, yp, \'r-\', alpha=0.1)

 plt.plot(x, y, \'bo\')

 plt.xlabel(\'\$x\$\', fontsize=16)\
 plt.ylabel(\'\$f(x)\$\', fontsize=16, rotation=0)

![](media/image2407.png){width="5.01833552055993in" height="3.279844706911636in"}

 周期核函数

 前一幅图中，你可能注意到了，我们可以很好地拟合sin函数，

 308

 不过模型在9到10之间不确定性非常大（该区间没有数据点）。该模 型的一个问题是，原始的数据是通过一个周期函数生成的，但是我们 的核函数并没有做出周期性的假设。某些情况下，当我们知道数据可 能是周期性的时候，应该使用一个周期性的核函数。一个典型的周期 性核函数如下：

![](media/image2410.png){width="2.6445166229221346in" height="0.5726706036745407in"}

 注意，这个核函数与高斯核函数的主要区别是包含了一个sin函\
 数。我们可以复用前面的代码，唯一的区别是现在需要定义一个周期

 函数而不是squared_distance，代码实现如下：

 periodic = lambda x, y: np.array(\[\[np.sin((x\[i\] - y\[j\])/2)\*\*2 for i in range(len(x))\] for j in range(len(y))\])

 在这个模型中，我们需要将squared_distance替换成这个周期 函数。 运行模型之后，你应该会得到类似下面的图：

![](media/image2415.png){width="5.01833552055993in" height="3.279844706911636in"}

 309

 **7.4**　总结

 本章一开始，我们学习了贝叶斯框架下的非参统计，以及如何用 核函数表示统计学中的问题，例如，我们用一种采用核函数的线性回 归去建模了非线性输出，随后继续讨论了另外一种构建和理解核函数 方法的方式------高斯过程。

 高斯过程是多元高斯分布扩展到无限多维时的一种一般形式，可 以用一个均值函数和一个协方差函数来描述。由于从概念上我们可以 将函数看作无限长的向量，因而可以将高斯过程作为函数先验。实践 中，我们处理的是维度和数据点个数相同的多元高斯分布。为了定义 与之对应的协方差函数，我们使用了参数化的核函数，通过学习超参 数，最终可以拟合出任意复杂的未知函数。

 这一章中，我们简要介绍了高斯过程，还有许多与之相关的主题 需要学习（比如构建一个半参数化模型，将线性模型作为均值函 数），或者是将两个或者多个核函数组合在一起来描述未知函数，或 者是如何将高斯过程用于分类任务，或者是如何将高斯过程与统计学 或者机器学习中的其他模型联系起来。不管怎么说，我希望本章对高 斯过程的介绍以及本书中一些其他主题的介绍能够激励你阅读、使用 和进一步学习贝叶斯统计。

 310

 **7.5**　深入阅读

 Carl Edward Rasmussen和 Christopher K. I. Williams写的\
 《Gaussian Processes for Machine Learning》一书。

 Kevin Murhpy的《Machine Learning a Probabilistic Perspective》 中的第4章和第15章。

 《Statistical Rethinking》中的第11章。

 《Bayesian Data Analysis, Third Edition》中的第22章。

 311

 **7.6**　练习

 1．在核回归的例子中，尝试修改结的个数以及带宽（一次修改 一个），这些改变有什么效果？尝试只使用一个结，你观察到了什 么？

 ![](media/image2426.png){width="1.9990048118985126in" height="0.187419072615923in"}2．用核回归拟合其他函数，比如 或者*y* = *x*。尝试像练习1中那样修改数据和参数的个数。

 3．在前面从高斯过程先验中采样的例子里，增加实例的个数，\
 将plt.plot(test_points,\
 stats.multivariate_normal.rvs(cov=cov, size=6).T)替换 成plt.plot(test_points,

 stats.multivariate_normal.rvs(cov=cov, size=1000).T,\
 alpha=0.05, color=\'b\')。高斯过程的先验是什么样子的？你是否 看出*f* (*x*)分布得像均值为0、标准差为1的高斯分布？

 4．对于一个使用高斯核的高斯过程后验，尝试将测试点定义在 区间\[0,10\]之外，区间外的点有怎样的结果？这告诉我们在外推 （extrapolating）时需要注意什么（特别是非线性函数）？

 5．重复练习4，这次换成周期性核，现在你的结论又是什么？

 \[1\]　 在最新版的PyMC3中，已经有该模块了，具体请参考\
 <https://pymc-devs.github.io/ pymc3/examples.html\#gaussian-processes。 ------译者注

 312

 欢迎来到异步社区！

 异步社区的来历

 异步社区([www.epubit.com.cn](http://www.epubit.com.cn))是人民邮电出版社旗下IT专业图书 旗舰社区，于2015年8月上线运营。

 异步社区依托于人民邮电出版社20余年的IT专业优质出版资源和 编辑策划团队，打造传统出版与电子出版和自出版结合、纸质书与电 子书结合、传统印刷与POD按需印刷结合的出版平台，提供最新技术 资讯，为作者和读者打造交流互动的平台。

 313

![](media/image2437.png){width="6.267714348206474in" height="5.320637576552931in"}

 314

 社区里都有什么？

 购买图书

 我们出版的图书涵盖主流IT技术，在编程语言、Web技术、数据 科学等领域有众多经典畅销图书。社区现已上线图书1000余种，电子 书400多种，部分新书实现纸书、电子书同步出版。我们还会定期发

 布新书书讯。

 下载资源

 社区内提供随书附赠的资源，如书中的案例或程序源代码。

 另外，社区还提供了大量的免费电子书，只要注册成为社区用户 就可以免费下载。

 与作译者互动

 很多图书的作译者已经入驻社区，您可以关注他们，咨询技术问 题；可以阅读不断更新的技术文章，听作译者和编辑畅聊好书背后有 趣的故事；还可以参与社区的作者访谈栏目，向您关注的作者提出采 访题目。

 315

 灵活优惠的购书

 您可以方便地下单购买纸质图书或电子图书，纸质图书直接从人 民邮电出版社书库发货，电子书提供多种阅读格式。

 对于重磅新书，社区提供预售和新书首发服务，用户可以第一时 间买到心仪的新书。

 ![](media/image2442.png){width="1.0411482939632546in" height="0.15618219597550306in"}用户帐户中的积分可以用于购书优惠。100积分=1元，购买图书 时，在 里填入可使用的积分数值，即可扣减相应金额。

 特别优惠

 购买本电子书的读者专享异步社区优惠券。 使用方法：注册成为社区用户，在下单购 书时输入"**57AWG**"，然后点击"使用优惠码"，即可享受电子书8折优惠（本优惠券只可使 用一次）。

 纸电图书组合购买

 社区独家提供纸质图书和电子书组合购买方式，价格优惠，一次 购买，多种阅读选择。

 316

![](media/image2456.png){width="6.267714348206474in" height="4.144058398950131in"}

 317

 社区里还可以做什么？

 提交勘误

 您可以在图书页面下方提交勘误，每条勘误被确认后可以获得\
 100积分。热心勘误的读者还有机会参与书稿的审校和翻译工作。

 写作

 社区提供基于Markdown的写作环境，喜欢写作的您可以在此一\
 试身手，在社区里分享您的技术心得和读书体会，更可以体验自出版

 的乐趣，轻松实现出版的梦想。

 如果成为社区认证作译者，还可以享受异步社区提供的作者专享 特色服务。

 会议活动早知道

 您可以掌握IT圈的技术会议资讯，更有机会免费获赠大会门票。

