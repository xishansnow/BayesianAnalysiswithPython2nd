 #  第 8 章 推断引擎

<style>p{text-indent:2em;2}</style>

到目前为止，我们的重点是建立模型、解释结果和批判模型。依靠 `pm.sample` 函数的魔力计算后验分布。现在，我们将重点学习此函数背后的推理引擎。概率编程工具(如 `PyMC3` )的目的是使用户不用关心如何进行采样，但了解如何从后验获取样本对于全面理解推断过程很重要，还可以帮助我们了解这些方法何时会失败、为什么失败、如何处理。如果您对了解近似后验的方法原理不感兴趣，可以跳过本章的大部分内容，但强烈建议您至少阅读样本诊断一节，因为这一节提供了一些指导原则，可以帮助您检查后验样本是否可靠。

计算后验分布的方法有很多。本章将讨论一些基本思想，并将重点介绍在 `PyMC3` 中实现的最重要的方法。

在本章中，我们将学习：
- 变分方法
- Metropolis-Hastings
- 汉密尔顿蒙特卡罗
- 序贯蒙特卡罗
- 样本诊断
  
---

## 8.1 几类推断引擎

虽然概念上很简单，但贝叶斯方法在数学和数值上都可能是具有挑战性的。主要原因是，贝叶斯定理(见公式1.4)中的分母边际似然通常采用难以处理或计算昂贵的积分的形式来求解。为此，后验估计通常使用马尔可夫链蒙特卡罗(MCMC)家族的算法或最近的变分算法进行数值估计。这些方法有时被称为推理机，因为至少在原则上，它们能够近似任何概率模型的后验分布。即使在实践中推理并不总是那么好用，这种方法的存在也推动了概率编程语言(如PyMC3)的发展。

概率编程语言的目标是将建模过程与推理过程分开，以促进模型构建、评估和模型修改/扩展的迭代步骤(如第1章，概率思维和第2章，概率编程中所讨论的)。通过将推理过程(但不是模型构建过程)视为黑匣子，PyMC3等概率编程语言的用户可以自由地关注他们的具体问题，让PyMC3为他们处理计算细节。这正是我们到目前为止一直在做的事情。因此，你可能会有偏见地认为这是显而易见或自然的方法。但重要的是要注意到，在概率编程语言之前，做概率模型的人也习惯于编写自己的抽样方法，通常是根据他们的模型量身定做的，或者他们习惯于简化他们的模型，使它们适合于某些数学近似。事实上，在一些学术界，这仍然是正确的。这种量身定制的方法可以更优雅，甚至可以提供一种更有效的后验计算方法，但它也容易出错和耗时，即使对专家来说也是如此。此外，定制的方法不适合大多数对用概率模型解决问题感兴趣的从业者。像PyMC3这样的软件邀请来自非常广泛背景的人使用概率模型，从而降低了数学和计算的入门门槛。我个人认为这很棒，也是一种邀请，让我们更多地了解统计建模中的良好实践，因此我们试图避免自欺欺人。前面的章节主要是关于学习贝叶斯建模的基础知识；现在我们将在概念层面上学习如何实现自动推理，何时以及为什么失败，以及当失败时该怎么做。

有几种数值计算后验概率的方法。我把他们分成两大类：

非马尔可夫方法
- 网格计算
- 二次逼近
- 变分方法
- 集成嵌入拉普拉斯近似（INLA）方法
  
马尔可夫方法
- Metropolis-Hastings
- 哈密顿蒙特卡罗
- 序贯蒙特卡罗

如今，贝叶斯分析主要是通过马尔科夫链蒙特卡洛（Markov Chain Monte Carlo，MCMC）方法进行，同时变分方法也越来越流行，特别是在一些较大的数据集上。学习贝叶斯分析并不需要完全掌握这些方法，不过从概念层面上对它们的工作原理有一定了解会很有帮助，比如调试模型。

## 8.2 非马尔可夫方法

首先讨论基于非马尔科夫方法的推断引擎。对于某些问题，这类方法非常有用，而对另外一些问题，这类方法只能提供真实后验的粗略近似。

### 8.2.1 网格计算法

网格计算是一种暴力穷举的方法。即便你无法计算出整个后验，你也可以根据一些点计算出先验和似然。假设我们要计算某个单参数模型的后验，网格近似可以按照如下方式进行：

- 确定参数的一个合理区间（先验会给你点提示）；
- 在以上区间确定一些网格点（通常是等距离的）；
- 对于网格中的每个点计算先验和似然。
  
视情况，我们可能会对计算结果进行归一化（把每个点的计算结果除以所有点的计算结果之和）。很容易看出，选的点越多（网格越密）近似的结果就越好。事实上，如果使用无限多的点，我们可以得到准确的后验。网格计算的方法不能很好地适用于多参数（又或者称多维度）的场景，随着参数的增加，采样空间相比后验空间会急剧增加，换言之，我们花费了大量
时间计算后验值，但对于估计后验却几乎没有帮助，因而使得该方法对于大多数统计学和数据科学的问题都不太实用[1]。

下面的代码用网格计算的方法解决第一章中的抛硬币问题：

```python
def posterior_grid(grid_points=50, heads=6, tails=9):
    """
    A grid implementation for the coin-flipping problem
    """
    grid = np.linspace(0, 1, grid_points)
    prior = np.repeat(1/grid_points, grid_points)  # uniform prior
    likelihood = stats.binom.pmf(heads, heads+tails, grid)
    posterior = likelihood * prior
    posterior /= posterior.sum()
    return grid, posterior
```

假设我们抛硬币13次，观察到3个头：

```python
data = np.repeat([0, 1], (10, 3))
points = 10
h = data.sum()
t = len(data) - h
grid, posterior = posterior_grid(points, h, t)
plt.plot(grid, posterior, 'o-')
plt.title(f'heads = {h}, tails = {t}')
plt.yticks([])
plt.xlabel('θ');
```

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021060716063392.webp)

很容易注意到，点的数量越多(或者等效地：栅格的大小越小)，可以得到更好的近似值。事实上，在无限个点的限制下，我们会以增加计算资源为代价得到精确的后验结果。

网格方法最大的警告是，这种方法不能很好地随参数(也称为维度)的数量进行调整。我们可以通过一个简单的例子来了解这一点。假设我们想要采样一个单位间隔(参见图8.2)，就像抛硬币问题一样，我们使用四个等距点。这意味着分辨率为0.25个单位。现在假设我们有一个2D问题(图8.2中的正方形)，并且我们想使用具有相同分辨率的网格，我们将需要16个点，而对于3D问题，我们将需要64个点(参见图8.1中的立方体)。在本例中，从边1的立方体采样所需的资源是长度为1、分辨率为0.25的线的16倍。如果我们决定需要0.1个单位的分辨率，则必须对直线采样10个点，对立方体采样1000个点：

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210607160809ce.webp)

除了点数的增加，还有另一种现象，它不是网格法的属性，也不是任何其他方法的属性，而是高维空间的属性。随着参数数量的增加，与采样体积相比，大部分后方集中的参数空间区域变得越来越小。这是统计学和机器学习中的一种普遍现象，通常被称为维度的诅咒，或者数学家更喜欢将其称为度量的集中度。

名称中的维度诅咒用来谈论各种相关的现象，这些现象在低维空间中不存在，但在高维空间中存在。以下是这些现象的一些例子：

- 随着维数的增加，任意一对样本之间的欧几里得距离变得越来越近。也就是说，在高维空间中，大多数点彼此之间的距离基本相同。
- 对于超级立方体，大部分体积在其角落，而不是在中间。对于超球体，大部分体积在其表面，而不在中间。
- 在高维中，多变量高斯分布的大部分质量并不接近平均值(或模)，而是在其周围的壳层中，随着维数的增加，壳层从平均值移动到尾部。该外壳的名称为Typical Set。
  
有关其中一些事实的代码示例，请查看 [链接](https:/​/​gihub.​com/aloctavodia/​BAP)

在我们目前的讨论中，所有这些事实都意味着，如果我们不明智地选择在哪里评估后验，我们将花费大部分时间来计算对后验的贡献几乎为零的值，从而浪费宝贵的资源。网格法不是一种非常智能的方法来选择在哪里评估后验分布，因此作为高维问题的通用方法不是很有用。

### 8.2.2 拉普拉斯近似法

二次近似，也称为拉普拉斯方法或正态近似，包括用高斯分布近似后验。

此方法由两个步骤组成：

- 找出后验分布的模式。这将是一种手段。
- 计算Hessian矩阵。由此，我们可以计算出的标准偏差2。
  
第一步可以使用最优化方法进行数值计算，也就是找出函数的最大值或最小值。为此，有许多现成的方法。因为对于高斯分布，模和平均值是相等的，所以我们可以使用模作为近似分布的平均值。第二步并不那么透明。我们可以通过计算模式/平均值处的曲率来近似计算的标准偏差。这可以通过计算黑森矩阵的平方根的倒数来实现。黑森矩阵是函数的二阶导数的矩阵，其逆提供协方差矩阵。使用PyMC3，我们可以执行以下操作：

```python
with pm.Model() as normal_approximation:
     p = pm.Beta('p', 1., 1.)
     w = pm.Binomial('w',n=1, p=p, observed=data)
     mean_q = pm.find_MAP()
     std_q = ((1/pm.find_hessian(mean_q, vars=[p]))**0.5)[0] mean_q['p'], std_q
```

```{note}
如果您尝试在PyMC3中使用pm.find_map函数，您将收到一条警告消息。由于维数灾难，使用最大后验概率(MAP)来表示后验，甚至初始化采样方法通常不是一个好主意。
```

让我们看看β-二项式模型的二次近似是什么样子：

```python
# analytic calculation
x = np.linspace(0, 1, 100)
plt.plot(x, stats.beta.pdf(x , h+1, t+1),
         label='True posterior')
# quadratic approximation
plt.plot(x, stats.norm.pdf(x, mean_q['p'], std_q),label='Quadratic
         approximation')
plt.legend(loc=0, fontsize=13)
plt.title(f'heads = {h}, tails = {t}')
plt.xlabel('θ', fontsize=14)
plt.yticks([]);
```
![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021060716150097.webp)

图8.3显示，平方近似并没有那么差，至少在本例中是这样。严格地说，我们只能将拉普拉斯方法应用于无界变量，也就是居住在中的变量。这是因为高斯分布是一个无界分布，所以如果我们用它来模拟一个有界分布(比如贝塔分布)，我们最终会估计一个正密度，而实际上密度应该是零(在贝塔分布的[0，1]区间之外)。不过，如果我们首先将有界变量变换为无界变量，则可以使用拉普拉斯方法。例如，我们通常使用半正态分布来精确地模拟标准差，因为它被限制在[0，∞)区间内，我们可以通过取其对数来使半正态分布变量无界。

拉普拉斯方法是有限的，但对于某些模型可以很好地工作，并且可以用来获得近似后验的解析表达式。它也是一种名为积分嵌套拉普拉斯近似(INLA)的更高级方法的构建块之一。

在下一节中，我们将讨论变分方法，这些方法在某种程度上类似于拉普拉斯近似，但更灵活、更强大，其中一些方法可以自动应用于各种模型。

### 8.2.3 变分方法

大多数现代贝叶斯统计都是使用马尔可夫方法(见下一节)完成的，但对于某些问题，这些方法可能太慢了。变分方法是一种替代方法，对于大数据集(比如大数据)和/或计算成本太高的后验数据而言，它可能是更好的选择。

变分法的基本思想是用一种更简单的分布来近似后验分布，类似于拉普拉斯方法，但以更精细的方式。我们可以通过解决一个最优化问题来找到这个更简单的分布，这个最优化问题包括在某种度量接近程度的方法下找到与后方最接近的可能分布。衡量分布之间接近程度的一种常用方法是使用Kullback-Leibler(KL)散度(如第5章，模型比较中所讨论的)。使用KL散度，我们可以写道：

$$
D_{K L}(q(\theta) \| p(\theta \mid y))=\int q(\theta) \log \frac{q(\theta)}{p(\theta \mid y)} d(\theta)  \tag{8.1}
$$

其中是较简单的分布，我们用来近似后验分布，通常称为变分分布，通过使用最优化方法，我们试图找出参数(通常称为变分参数)，使其在KL散度方面尽可能接近后验分布。注意，我们写了，没有写；我们这样做是因为这导致了表达问题的更方便的方式和更好的解决方案，尽管我应该明确地指出，在另一个方向上写KL分歧也是有用的，实际上导致了另一组我们不在这里讨论的方法。


表达式8.1的问题是我们不知道后缀，所以不能直接使用它。我们需要找到另一种方式来表达我们的问题。以下步骤显示了如何做到这一点。如果您不关心中间步骤，请跳到公式8.7。

首先，我们用条件分布的定义替换条件分布(如果您不记得如何执行此操作，请参阅第1章，概率思维)：

$$
D_{K L}(q(\theta) \| p(\theta \mid y))=\int q(\theta) \log \frac{q(\theta)}{\underline{p(\theta, y)}} d(\theta) \tag{8.2}
$$

然后我们只需重排式8.2：

$$
=\int q(\theta) \log \frac{q(\theta)}{p(\theta, y)} p(y) d(\theta) \tag{8.3}
$$

根据对数的性质，我们得到这个方程：

$$
=\int q(\theta)\left(\log \frac{q(\theta)}{p(\theta, y)}+\log p(y)\right) d(\theta) \tag{8.4}
$$

重新排列：

$$
=\int q(\theta) \log \frac{q(\theta)}{p(\theta, y)} d(\theta)+\int q(\theta) \log p(y) d(\theta) \tag{8.5}
$$

的积分是1，我们可以移出积分，然后我们得到：

$$
=\int q(\theta) \log \frac{q(\theta)}{p(\theta, y)} d(\theta)+\log p(y) \tag{8.6}
$$

并利用对数的性质：

$$
D_{K L}(q(\theta)|| p(\theta \mid y))=\underbrace{-\int q(\theta) \log \frac{p(\theta, y)}{q(\theta)} d(\theta)}_{\text {evidence lower bound (ELBO) }}+\log p(y)) \tag{8.7}
$$

因为，然后，或者换句话说，证据(或边际可能性)总是等于或大于ELBO，这就是它的名字的原因。既然它是一个常量，我们可以只关注埃尔博。最大化ELBO的值相当于最小化KL发散。因此，使Elbo最大化是一种使其尽可能接近后方的方法。

请注意，到目前为止，我们还没有引入任何近似，我们只是在做一些代数。在我们选择的时刻引入近似值。原则上，可以是我们想要的任何东西，但实际上我们应该选择易于处理的发行版。一种解决方案是假设高维后验可以用独立的一维分布来描述；在数学上，这可以表示如下：

$$
q(\theta)=\prod_{j} q_{j}\left(\theta_{j}\right) \tag{8.8}
$$

这被称为平均场近似。平均场近似在物理学中很常见，在物理学中，它被用来将具有许多相互作用的部分的复杂系统建模为根本不相互作用的更简单的子系统的集合，或者在一般情况下，只有在平均情况下才考虑相互作用。

我们可以为每个参数选择不同的分布。通常，分布取自指数族，因为它们易于处理。指数族包括我们在本书中使用的许多分布，如正态分布、指数分布、贝塔分布、狄利克雷分布、伽马分布、泊松分布、绝对分布和伯努利分布。

有了所有这些元素，我们已经有效地将推理问题转化为优化问题；因此，至少在概念上，我们需要解决的所有问题就是使用一些现成的优化器方法并最大化ELBO。在实践中，事情稍微复杂一些，但我们已经涵盖了大体思想。

### 8.2.4 自动微分变分推断方法

我们刚才描述的平均场变分法的主要缺点是，我们必须为每个模型提出一个特定的算法。我们没有通用推理引擎的配方，而是生成需要用户干预的特定于模型的方法的配方。幸运的是，许多人已经注意到了这个问题，并提出了以变分方法自动化为重点的解决方案。最近提出的一种方法是自动微分变分推理(ADVI)(参见http：//​/​arxiv.org/​abs/​1603.​00788)。在概念层面上，ADVI采取的主要步骤是：

- 变换所有有界分布，使它们位于实线上，正如我们在拉普拉斯方法中所讨论的那样。
- 用高斯分布近似无界参数(这是公式8.8中的)；请注意，变换后的参数空间上的高斯在原始参数空间上是非高斯的。
- 使用自动区分来最大化ELBO。

PyMC3文档(例如：https:/​/​docs.​pymc.​io/​nb_​examples)提供了许多关于如何对PyMC3使用变分推理的示例。

## 8.2 马尔科夫方法

有一系列相关的方法，统称为MCMC方法。这些随机方法允许我们从真实的后验分布中获得样本，只要我们能够逐点计算似然和先验值。虽然这与网格方法所需的条件相同，但MCMC方法的性能优于网格近似方法。这是因为MCMC方法能够从高概率区域比低概率区域采集更多的样本。实际上，MCMC方法会根据参数空间的每个区域的相对概率来访问它们。如果区域A的概率是区域B的两倍，那么我们将从A获得的样本是从B获得的样本的两倍。因此，即使我们不能解析地计算出整个后验结果，我们也可以使用MCMC方法来从其中获取样本。

在最基本的层面上，我们在统计中关心的一切基本上都是关于计算预期的，比如：

$$
\mathbb{E}[f]=\int_{\theta} p(\theta) f(\theta) \mathrm{d} \theta \tag{8.9}
$$

下面是这个一般表达式的一些特殊例子：
- 后验，公式1.14
- 后验预测分布，公式1.17
- 给定模型的边缘似然，公式5.13

使用MCMC方法，我们使用有限样本近似公式8.9：

$$
\lim _{N \rightarrow \infty} \mathbb{E}_{\pi}[f]=\frac{1}{N} \sum_{n=1}^{N} f\left(\theta_{n}\right) \tag{8.10}
$$

等式8.10的一大问题是，等式仅渐近成立，也就是说，对于无限数量的样本！在实践中，我们总是有有限数量的样本，因此我们希望MCMC方法尽可能快地收敛到正确的答案-用尽可能少的样本(也称为DRAW)。

一般来说，委婉地说，要确定MCMC的特定样本已经收敛并非易事。因此，在实践中，我们必须依靠实证检验来确保我们有一个可靠的MCMC近似。我们将在诊断样本部分讨论MCMC样本的此类测试。重要的是要记住，其他近似(包括本章讨论的非马尔可夫方法)也需要实证检验，但我们不会讨论它们，因为本书的重点是MCMC方法。

对MCMC方法有一个概念性的了解可以帮助我们从这些方法中诊断样本。那么，让我问一下，名字有什么用呢？嗯，有时不多，有时很多。为了理解什么是MCMC方法，我们将把该方法分成两个MC部分：蒙特卡罗部分和马尔可夫链部分。

