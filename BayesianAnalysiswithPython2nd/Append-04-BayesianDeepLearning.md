---
jupytext:
  formats: ipynb,md:myst
  text_representation:
    extension: .md
    format_name: myst
    format_version: 0.13
    jupytext_version: 1.12.0
kernelspec:
  display_name: Python 3
  language: ipython3
  name: python3
---

# 附录 D：贝叶斯神经网络的实践 -- 面向深度学习用户的教程

[原文](https://arxiv.org/abs/2007.06823)

[作者]
- LAURENT VALENTIN JOSPIN,University of Western Australia
- WRAY BUNTINE,Monash University
- FARID BOUSSAID,University of Western Australia
- HAMID LAGA,Murdoch university
- MOHAMMED BENNAMOUN,University of Western Australia

[引用]
Laurent Valentin Jospin, Wray Buntine, Farid Boussaid, Hamid Laga, and Mohammed Bennamoun. 2020.Hands-on Bayesian Neural Networks - a Tutorial for Deep Learning Users.ACM Comput. Surv.1, 1 ( July 2020),35 pages

<style>p{text-indent:2em;2}</style>

现代深度学习方法已经为研究人员和工程师提供了令人难以置信的强大工具，以解决以前似乎不可能解决的问题。然而，由于深度学习方法是作为黑箱操作的，与他们的预测相关的不确定性往往是难以量化的。贝叶斯统计学提供了一个形式化的方法来理解和量化与深度神经网络预测相关的不确定性。本文为正在使用机器学习，特别是深度学习的研究人员和科学家提供了一个相关文献的概述和一个完整的工具集来设计、实现、训练、使用和评估贝叶斯神经网络。

## 1 简介

深度学习导致了机器学习的革命，为解决现实生活中复杂而具有挑战性的问题提供了解决方案。然而，深度学习模型容易过拟合，这对其泛化能力产生了不利影响。深度学习模型也倾向于对其预测结果过于自信（当他们提供一个置信区间时）。所有这些对于诸如自动驾驶汽车 [74]、医疗诊断 [38] 或交易和金融 [11] 等应用来说都是有问题的，因为无声的失败会导致戏剧性的结果。因此，人们提出了许多方法来减轻风险，特别是通过随机神经网络来估计模型预测的不确定性。贝叶斯范式为分析和训练随机神经网络提供了严格框架，并且更广泛地支持了学习算法的发展。

图 1.  本文所涉及的主题的思维导图。这些可以大致分为贝叶斯深度神经网络的概念、不同的（严格意义上的或近似于贝叶斯的）学习方法、评估方法，以及研究人员可用于实施的工具集。

## 2 

## 3 深度学习中贝叶斯方法的动机

一些用户认为定义模型参数的先验 $p(θ)$ 即便可能，也会很难。为简单模型定义先验通常是直观的，例如：明确地添加一个正则化项以支持低次多项式函数或平滑函数 [54]。但对深度学习中使用的多层模型来说，定义先验比较困难。

那么，既然在定义先验时很难理解深度神经网络的行为，为什么还要为其使用贝叶斯方法呢？

人工神经网络所编码的函数关系隐含代表了条件概率 $p(y|x,θ)$ 。贝叶斯公式是用来反转条件概率的合适工具，即使人们事先对 $p(θ)$ 没有什么信息。虽然有很强的理论原则和模式可以作为贝叶斯公式的基础 [76]，但本节重点讨论使用贝叶斯深度网络的一些实际好处。

（1）贝叶斯方法提供了一种方法来量化深度学习中的不确定性。贝叶斯神经网络通常比经典神经网络有更好的校准 [46,58,66]，即其预测的不确定性与观测误差更加一致。与非贝叶斯神经网络相比，它们既不过度自信，也不缺乏自信。使用贝叶斯神经网络可以区分认知不确定性（epistemic uncertainty）和偶然不确定性（aleatoric uncertainty）。前者是由于缺乏知识而产生的不确定性，用 $p(θ|D)$ 来测量，随着数据的增加该不确定性在减少；后者是由于数据的偶然性质而产生的不确定性，用 $p(y|x，θ)$ 来测量 [14,44] 。这使得贝叶斯神经网络具有非常高的数据效率：在学习时，它可以从一个小数据集开始，而不会产生过拟合；在预测时，超出训练集范围的数据只会产生认知不确定性。因此，贝叶斯神经网络成为主动学习（activate learning）的有趣工具 [19,88]，因为人们可以解释模型的预测结果，查看对于相同输入，不同（可能的）参数是否会导致不同预测结果。

（2）机器学习“没有免费的午餐定理” [94] 可以被解释为任何监督学习算法都包括隐含先验（虽然这种解释更多是哲学而非数学的），而贝叶斯方法则明确了先验。虽然并非不可能，但现在的黑盒工具确实在整合先验知识方面非常困难。而在贝叶斯深度学习中，先验被认为是一种软约束，类似于正则化。大多数用于点估计神经网络的正则化方法，基本都可以从贝叶斯角度理解为设置了某种先验（见第 5.3 节）。此外，当新数据出现时，以前学到的后验可循环使用，这使贝叶斯神经网络成为在线学习的重要工具 [64]。

（3）贝叶斯范式能够分析学习方法，并在它们之间建立联系。一些最初不被视为贝叶斯的方法可以被隐含地理解为近似贝叶斯方法，如正则化（见第 5.3 节）或集成概念（第 8.2.2 节）。这也解释了“为什么某些很好用的非贝叶斯算法，也仍然能给出贝叶斯理解？”。实践中大多数贝叶斯神经网络架构都依赖于近似或隐含的贝叶斯方法（见第 8 节），因为精确算法往往太昂贵了。贝叶斯范式还提供了一个系统框架来设计新的学习和正则化策略，即使模型是面向点估计的。

## 4 用于贝叶斯深度学习的随机模型

在设计贝叶斯神经网络时，我们需要选择一个深度网络架构（即功能模型），同时还要选择一个随机模型（即哪些变量被视为随机变量以及其先验分布）。本教程中不会涉及功能模型的设计，因为几乎所有用于点估计的网络模型都可用于贝叶斯深度学习，而且已经有关于功能模型的丰富文献 [71]。

此处将介绍概率图模型（Probabilistic Graphical Models, 概率图模型），一种用来表示随机变量及其条件依赖关系的工具。特别是贝叶斯统计中常使用的一种概率图模型：贝叶斯信念网络。然后展示如何从概率图中实现贝叶斯神经网络的随机模型。

### 4.1 概率图模型

概率图模型是统计学家用于表示多个随机变量之间的相互依赖性，并用图形方式来分解其概率分布的一种工具。概率图涵盖了大量模型，而本教程中只讨论其中的贝叶斯信念网络（有时也称为信念网络或贝叶斯网络）, 贝叶斯信念网络是用有向无环图表示的概率图模型。

```{note}
有关概率图模型的详细回顾，请参考 [9]。
```

虽然贝叶斯信念网络（随机模型）和贝叶斯神经网络（功能模型）都表示为有向无环图，但两者完全不是一回事。贝叶斯神经网络模型表示一组函数关系，如公式（2）所示，具有先验分布；而贝叶斯信念网络则表示模型中所考虑变量的联合概率分布。在构思贝叶斯神经网络时，相应的贝叶斯信念网络代表先验的基础结构，最终在使用变分推断时则代表了变分后验（见第 7.2 节）。

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021091716374092.webp)


在概率图模型中，图中的节点代表随机变量，用不同符号来区分所考虑的变量性质（图 5）。贝叶斯信念网络中仅允许有向链接，这意味着目标随机变量的概率分布是以源随机变量为条件来定义的（反过来不成立）。根据乘法法则，这种条件依赖性使得贝叶斯信念网络中所有变量 $v_i$ 的联合概率分布，能够被分解为局部随机变量某些概率分布的组合。

$$
p\left(\boldsymbol{v}_{1}, \ldots, \boldsymbol{v}_{n}\right)=\prod_{i=1}^{n} p\left(\boldsymbol{v}_{i} \mid \operatorname{parents}\left(\boldsymbol{v}_{i}\right)\right)
$$

为完成贝叶斯信念网络，必须定义所有的概率分布 $p(v_i | parents(v_i))$ 。所用分布类型取决于上下文。一旦定义了 $p(v_i|parents(v_i))$ ，则贝叶斯信念网络描述了一个数据生成过程。有向无环图的约束条件，使得父变量总是在其子孙变量前被抽样，而所有变量一起形成了联合概率分布 $p(v_1,...,v_n)$ 的一个样本。

模型通常基于同一分布中采样出的多个样本进行学习。为强调这一事实，引入了板（plate）符号（图 5e）。一个板表示其所封装的子图中所有变量 $(v_1,...,v_n)$ 会按照指定批次维度进行重复，这也意味着板种所有节点在批次之间存在独立性。这种独立性质可被用来计算某个批次 $B={(v_1,...,v_n)_b:b=1,...,|B|}$ 的联合概率。

$$
p(B)=\prod_{\left(\boldsymbol{v}_{1}, \ldots, \boldsymbol{v}_{n}\right) \in B} p\left(\boldsymbol{v}_{1}, \ldots, \boldsymbol{v}_{n}\right)
$$

在概率图模型中，需要区分观测变量和非观测变量，前者用灰色圆圈表示（图 5a），作为数据来处理；后者用白色圆圈表示（图 5b），作为假设来处理。从概率图模型得出的联合概率来看，使用贝叶斯公式可以直接定义给定观测变量的潜变量后验。

$$
p\left(\boldsymbol{v}_{\text {latent }} \mid \boldsymbol{v}_{o b s}\right)=\frac{p\left(\boldsymbol{v}_{o b s}, \boldsymbol{v}_{\text {latent }}\right)}{\int_{\boldsymbol{v}_{\text {latent }}} p\left(\boldsymbol{v}_{\text {obs }}, \boldsymbol{v}_{\text {latent }}\right) d \boldsymbol{v}_{\text {latent }}} \propto p\left(\boldsymbol{v}_{\text {obs }}, \boldsymbol{v}_{\text {latent }}\right)
$$

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_202109171653153e.webp)
### 4.2 根据概率图模型定义贝叶斯神经网络的随机模型

考虑图 6 中的两个模型，贝叶斯神经网络和相应的贝叶斯信念网络都被画出来了。将权重视为随机变量情况下的贝叶斯信念网络（图 6a）可以代表以下数据生成过程，假设神经网络是为了做回归。

$$
\begin{aligned}
&\theta \sim p(\theta)=\mathcal{N}(\mu, \Sigma) \\
&\boldsymbol{y} \sim p(\boldsymbol{y} \mid x, \theta)=\mathcal{N}\left(N N_{\theta}(x), \Sigma\right)
\end{aligned}
$$

模型中选择正态分布 $\mathcal{N}(\mu, \Sigma)$ 完全是随意的，不过在实践中比较常见。

如果神经网络是为了做分类，那么该模型将有一个类别分布 $Cat(pi)$ 来对预测进行采样，而不是正态分布。

$$
\begin{aligned}
&\theta \sim p(\theta)=\mathcal{N}(\mu, \Sigma) \\
&y \sim p(\boldsymbol{y} \mid x, \theta)=\operatorname{Cat}\left(N N_{\theta}(x)\right)
\end{aligned}
$$

可以利用训练集中数据点相互独立这一假设，即用图 6 中的“板”符号表示，将训练集的概率写成：

$$
p\left(D_{y} \mid D_{x}, \theta\right)=\prod_{(x, y) \in D} p(\boldsymbol{y} \mid x, \theta)
$$

图 6b 中所示的将激活视为随机变量的情况，数据生成过程可能变成：
$$
\begin{aligned}
&l_{0}=x \\
&l_{i} \sim p\left(l_{i} \mid l_{i-1}\right)=n l_{i}\left(\mathcal{N}\left(W_{i} l_{i-1}+b_{i}, \Sigma\right)\right) \quad \forall i \in[1, n] \\
&y=l_{n}
\end{aligned}
$$

贝叶斯公式的联合概率公式稍微复杂一些，因为必须考虑贝叶斯信念网络跨越多个潜在变量 $l_{[1, n-1]}$ 的链式依赖关系：

$$
p\left(D_{\boldsymbol{y}}, \boldsymbol{l}_{[1, n-1]} \mid D_{x}\right)=\prod_{\left(l_{0}, l_{n}\right) \in D}\left(\prod_{i=1}^{n} p\left(\boldsymbol{l}_{i} \mid \boldsymbol{l}_{i-1}\right)\right)
$$

定义 $p\left(\boldsymbol{l}_{i} \mid \boldsymbol{l}_{i-1}\right)$ 有时是可能的，而且通常是可取的。例如：图 6a 和图 6b 中所描述的贝叶斯信念网络可以被认为是等效的，例如，以下 $l$ 的采样：

$$
\begin{aligned}
&W \sim \mathcal{N}\left(\mu_{W}, \Sigma_{W}\right) \\
&b \sim \mathcal{N}\left(\mu_{b}, \Sigma_{b}\right) \\
&l=n l\left(W l_{-1}+b\right)
\end{aligned}
$$

等价于如下对 $l$ 的采样：

$$
l \sim n l\left(\mathcal{N}\left(\mu_{W} l_{-1}+\mu_{b},\left(I \otimes l_{-1}\right)^{\top} \Sigma_{W}\left(I \otimes l_{-1}\right)+\Sigma_{b}\right)\right)
$$

其中 $⊗$ 表示克罗内克积。

图 6a 中描绘的贝叶斯回归架构在实践中更为常见。有时会使用图 6b 中的替代公式，因为它有助于在使用变分推断时压缩参数的数量 [92]。这为定义先验提供了不同的选择。

## 5 设置先验

为小型的、因果的概率模型先验非常直接。但对于深层神经网络来说，情况不一样，设置一个好的先验往往是一项繁琐而不直观的任务。主要问题是：对于具有大量参数和非微观结构的模型，如人工神经网络，如何对给定的参数进行归纳，并不十分明确 [98]。

本节将介绍与人工神经网络的 “统计不可辨识性” 有关的常见做法和相关问题。然后，在第 5.3 节，介绍贝叶斯深度学习中的先验如何与点估计方法的正则化相关。在那里会展示传统正则化方法如何帮助我们选择合适的先验，以及贝叶斯分析如何帮助传统方法设计新的目标函数。

### 5.1 良好的缺省先验

对于基本架构，如图 6a 中所示的带有人工神经网络的贝叶斯回归，标准程序是对网络系数采用均值为 $0$ 、对角线协方差为 $σ_I$ 的正态先验：

$$
P(θ) = \mathcal{N}(0，σI)
$$

正如在第 5.3 节中将证明的，在训练点估计网络时，该方法等同于权重为 $1/σ$ 的加权 ℓ2 正则化。概率编程语言 `Stan[10]` 提供了一些在知道所考虑参数预期规模的情况下，如何选择 $σ$ 的例子 [21]。

但是，虽然该方法在实践中被普遍使用，但并没有理论上的依据表明其优于任何其他表示 [80]。正态分布因其数学特性和其对数的简单表示而受到青睐，因为大多数学习算法中都使用了概率分布的对数。

## 5.2 解决贝叶斯神经网络中的不可辨识性问题

贝叶斯深度学习的主要问题之一是，深度神经网络可能是一个过度参数化的模型，即其中许多参数之间存在等价关系[59]。这被称为统计学上的不可辨识性问题，即推断不会产生唯一的结果。在训练贝叶斯神经网络时，这会导致很难取样和近似的复杂多模态后验。有两种解决方案来处理该问题：一是调整功能模型的参数化形式；二是约束先验的支持度以消除不可辨识性。

人工神经网络中最常见的两类非唯一性是：权重空间对称性和缩放对称性。两者都不是点估计神经网络的关注点，但对于贝叶斯神经网络来说则可能是。

（1）权重空间对称性

权重空间对称性意味着我们可以通过改变其中一个隐藏层的权重 $W_i$（不考虑偏差 $b_i$ ）中的两行和下面一层权重矩阵 $W_{i+1}$ 中的相应列来建立一个至少有一个隐藏层的人工神经网络的等效参数化。这意味着，随着隐层和隐层中单元数量的增加，等价表征的数量也会以阶乘形式增长，这些表征大致对应于后验分布中的众数。一种缓解策略是强制每层的偏置向量按升序或降序排列。然而，其实际效果未知，在优化的早期阶段，权重空间对称性可能隐含着对参数空间探索的支持。

（2）缩放对称性 

缩放对称性是在使用具有 $nl(αx)=αnl(x)$ 性质的非线因子时产生的不可辨识性问题，典型如：RELU 和 Leaky-RELU 这两个现代机器学习中最受欢迎的非线性因子。在这种情况下，给层 $l$ 和 $l+1$ 赋于权重 $W_l,W_{l+1}$ 就严格等同于赋 $αWl,1/αWl+1$。这可能会降低点估计神经网络的收敛速度，该问题在实践中可通过各种激活的归一化技术来解决 [1]。对于贝叶斯神经网络来说，问题稍微复杂一些，因为缩放对称性会影响后验，使得它更难近似其真实形状。一些作者提议使用 `Givens 变换`（有时也称 `Givens 旋转`）来约束隐藏层的范数 [70]，以解决缩放对称问题。

在实践中，使用高斯先验已经减少了缩放对称问题，因为它将有利于每个层上具有相同 `Frobenius 范数` 的权重。如第 5.4 节所述，激活归一化的软版本也可以通过使用一致性条件来实现。从计算复杂度角度来看，在受限空间中对网络参数进行采样的额外复杂性是不值得的。

### 5.3 正则化和先验之间的联系

点估计神经网络的通常学习程序是找到使某些损失函数最小的参数集 $θ$ ，该参数集是用训练集数据学习得到的。

$$
\hat{\theta}=\underset{\theta}{\arg \min } \operatorname{loss}_{D_{\boldsymbol{x}}, D_{y}}(\theta)
$$

假设损失为减去对数似然函数（总是这样的，最多是一个加法常数），问题可以重写为：

$$
\hat{\boldsymbol{\theta}}=\underset{\boldsymbol{\theta}}{\arg \max } p\left(D_{\boldsymbol{y}} \mid D_{\boldsymbol{x}}, \boldsymbol{\theta}\right)
$$

根据贝叶斯范式，这是模型的前一半。现在假设我们也有一个关于 $θ$ 的先验，并且想从后验中找到最可能的点估计。那么问题就变成了:

$$
\hat{\theta}=\underset{\theta}{\arg \max } p\left(D_{\boldsymbol{y}} \mid D_{\boldsymbol{x}}, \boldsymbol{\theta}\right) p(\theta)
$$

接下来，由于更容易优化，再次转换回对数似然的公式：

$$
\hat{\theta}=\underset{\theta}{\arg \min } \operatorname{loss}_{D_{x}, D_{y}}(\theta)+\operatorname{reg}(\theta)
$$

如果这个公式看起来很熟悉，那并不奇怪。这正是正则化在机器学习和许多其他领域的应用方式。这里暗含以下思想：

作为先验，我们有

$$
p(\theta) \propto e^{-r e g(\theta)+c s t}
$$

对于某些在实践中使用的正则化，这可能是一个不理想的分布，但一般的想法是存在的。另一个不太正式的论点是，正则化作为搜索空间的软约束，与先验对后验的作用相同。

### 5.4 满足一致性条件的先验

使用方程 25 中的表述，在某些情况下可以使用函数模型的预期行为来扩展先验。为此，人们通常定义一个一致性条件 $C(θ,x)$ 来评估在输入 $x$ 和参数集 $θ$ 下的预测的相对对数似然。例如，$C$ 可以被设置为有利于稀疏或有规律的预测，鼓励预测与某些输入变量的同调性（例如：得流感的概率随年龄的增长而增加），或者在进行半监督学习时有利于低密度区域的决策边界（第 6 节）。

$$
C(\theta)=\int_{\boldsymbol{x}} C(\theta, x) p(x) d x
$$

在实践中，$p(x)$ 是未知的，$C(\theta)$ 是从训练集的特征中近似估计出来的： 
 
$$
C(\theta) \approx \frac{1}{\left|D_{x}\right|} \sum_{\boldsymbol{x} \in\left|D_{x}\right|} C(\theta, x)
$$
 
 现在可以写一个与包含一致性条件的先验成比例的函数：
 
$$
p\left(\theta \mid D_{\boldsymbol{x}}\right) \propto p(\theta) \exp \left(-\frac{1}{\left|D_{x}\right|} \sum_{\boldsymbol{x} \in\left|D_{\boldsymbol{x}}\right|} C(\theta, \boldsymbol{x})\right)
$$
 
 其中 $p(θ)$ 是没有一致性条件的先验。 
 
 ## 5 监督的程度和先验知识的替代形式
 
 到目前为止介绍的架构主要集中在贝叶斯神经网络在监督学习环境中的使用。然而，在现实世界的应用中，获得地面真实标签可能是昂贵的，因此应该采用新的学习策略 [72]。我们现在介绍如何使贝叶斯神经网络适应不同程度的监督。在这样做的同时，我们还展示了贝叶斯后验的表述，它来自于下面介绍的不同的贝叶斯后验（图 8、9 和 10），可以用来（第 5.3 节）获得一个合适的最大后验估计器的损失函数，在这种情况下，一个点估计神经网络足以满足所考虑的使用情况。

 is approximated from the features in the t

write a function proportional to the prior with the consistency con

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210917132235fa.webp)