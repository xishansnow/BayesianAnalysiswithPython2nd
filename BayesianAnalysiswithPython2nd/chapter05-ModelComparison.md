# 第 5 章 模型比较

<style>p{text-indent:2em;2}</style>

```{note}
地图不是它所代表的领土，但如果正确的话，它的结构与该领土相似。---阿尔弗雷德·科日布斯基
```

模型应该被设计成帮助我们理解特定问题或某类相关问题的近似值。模型并不是真实世界的翻版，因此所有模型都是错误的，就像地图不是领土一样。即使在先验情况下，每个模型也都是错误的。但每个模型的错误可能不同：一些模型比其他模型更好地描述给定的问题。前面章节将注意力集中在推断问题上，即如何从数据中学习参数值。本章将重点讨论一个互补问题：如何比较用于解释相同数据的两个或多个模型。这是数据分析需要解决的核心问题之一。

本章将讨论以下内容：

1. 后验预测性检查
2. 奥卡姆剃刀---简单性和准确性
3. 过拟合和欠拟合
4. 信息准则
5. 贝叶斯因子
6. 正则化先验

---


## 5.1 后验预测性检查

第一章“概率思维”介绍了后验预测性检查的概念，本章将用它来评估拟合出的模型对相同数据的解释程度。后验预测性检查的目的并非断定某个模型是否错误，而是通过后验预测性检查更好地把握模型的局限性，并做出适当改进。模型不会再现所有问题，但这并不是问题，因为构建模型都有特定目的，后验预测性检查是在该目的背景下评估模型的一种方式；因此，如果考虑了多个模型，可以使用后验预测性检查来对它们进行比较。

让我们读取并绘制一个简单的数据集：

```python
dummy_data = np.loadtxt('../data/dummy.csv')
x_1 = dummy_data[:, 0]
y_1 = dummy_data[:, 1]
order = 2
x_1p = np.vstack([x_1**i for i in range(1, order+1)])
x_1s = (x_1p - x_1p.mean(axis=1, keepdims=True)) / x_1p.std(axis=1, keepdims=True)
y_1s = (y_1 - y_1.mean()) / y_1.std()
plt.scatter(x_1s[0], y_1s)
plt.xlabel('x')
plt.ylabel('y')
```

<center>

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510111807b3.webp)
图 5.1 
</center>

现在，将用两个略有不同的模型来拟合数据，一个是线性模型，另一个是二阶多项式模型：

```python
with pm.Model() as model_l:
    α = pm.Normal('α', mu=0, sd=1)
    β = pm.Normal('β', mu=0, sd=10)
    ϵ = pm.HalfNormal('ϵ', 5)
    μ = α + β * x_1s[0]
    y_pred = pm.Normal('y_pred', mu=μ, sd=ϵ, observed=y_1s)
    trace_l = pm.sample(2000)

with pm.Model() as model_p:
    α = pm.Normal('α', mu=0, sd=1)
    β = pm.Normal('β', mu=0, sd=10, shape=order)
    ϵ = pm.HalfNormal('ϵ', 5)
    μ = α + pm.math.dot(β, x_1s)
    y_pred = pm.Normal('y_pred', mu=μ, sd=ϵ, observed=y_1s)
    trace_p = pm.sample(2000)
```

现在绘制这两个模型的平均拟合曲线：

```python
x_new = np.linspace(x_1s[0].min(), x_1s[0].max(), 100)
α_l_post = trace_l['α'].mean()
β_l_post = trace_l['β'].mean(axis=0)
y_l_post = α_l_post + β_l_post *x_new
plt.plot(x_new, y_l_post, 'C1', label='linear model')
α_p_post = trace_p['α'].mean()
β_p_post = trace_p['β'].mean(axis=0)
idx = np.argsort(x_1s[0])
y_p_post = α_p_post + np.dot(β_p_post, x_1s)
plt.plot(x_1s[0][idx], y_p_post[idx], 'C2', label=f'model order {order}')
α_p_post = trace_p['α'].mean()
β_p_post = trace_p['β'].mean(axis=0)
x_new_p = np.vstack([x_new**i for i in range(1, order+1)])
y_p_post = α_p_post + np.dot(β_p_post, x_new_p)
plt.scatter(x_1s[0], y_1s, c='C0', marker='.')
plt.legend()
```

<center>

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510112129a4.webp)
图 5.2 
</center>

二阶模型似乎做得更好，但线性模型也并没有那么糟糕。使用 PyMC3 可获得两个模型的后验预测样本：

```python
y_l = pm.sample_posterior_predictive(trace_l, 2000, model=model_l)['y_pred']
y_p = pm.sample_posterior_predictive(trace_p, 2000, model=model_p)['y_pred']
```

正如已经看到的，后验预测性检查通常使用可视化方式来执行，如下例所示：

```python
plt.figure(figsize=(8, 3))
data = [y_1s, y_l, y_p]
labels = ['data', 'linear model', 'order 2']
for i, d in enumerate(data):
    mean = d.mean()
    err = np.percentile(d, [25, 75])
    plt.errorbar(mean, -i, xerr=[[-err[0]], [err[1]]], fmt='o')
    plt.text(mean, -i+0.2, labels[i], ha='center', fontsize=14)
    plt.ylim([-i-0.5, 0.5])
    plt.yticks([])
```

<center>

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510112319ed.webp)

图 5.3
</center>

图 5.3 显示了数据、线性模型和二次模型的均值和四分位数范围 (IQR) 。该图对各模型的后验预测样本做了平均，可以看到，两个模型的均值都复现得很好，分位数范围也不是很差，但在实际问题中，有一些小差异可能是值得注意的。你可以尝试做更多不同曲线图来探索后验预测分布。例如，绘制均值和四分位数间相对于数据真实值的离散度。下图就是一个例子：

```python
fig, ax = plt.subplots(1, 2, figsize=(10, 3), constrained_layout=True)
def iqr(x, a=0):
return np.subtract(*np.percentile(x, [75, 25], axis=a))
for idx, func in enumerate([np.mean, iqr]):
    T_obs = func(y_1s)
    ax[idx].axvline(T_obs, 0, 1, color='k', ls='--')
    for d_sim, c in zip([y_l, y_p], ['C1', 'C2']):
    T_sim = func(d_sim, 1)
    p_value = np.mean(T_sim >= T_obs)
    az.plot_kde(T_sim, plot_kwargs={'color': c},
    label=f'p-value {p_value:.2f}', ax=ax[idx])
    ax[idx].set_title(func.__name__)
    ax[idx].set_yticks([])
    ax[idx].legend()
```

<center>

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051011245094.webp)
图 5.4
</center>

图 5.4 中黑色虚线表示根据数据计算的平均值和四分位数。因为只有一个数据集，所以只有一个统计值，而不是分布。图中曲线（与图 5.3 相同颜色代码）表示根据后验预测样本计算得出的平均值分布（左图）或四分位数范围分布（右图）。图 5.4 还包括标记为 `p-value` 的值，该值来自于预测数据与实际数据的比较和计算。对于两个预测数据集合，我们计算了其平均值和四分位数范围，然后计算了两个统计量等于或大于根据实际数据统计量的比例。**一般而言，如果数据和预测结果一致，预期的 `p-value` 值在 0.5 左右，否则将处于有偏的后验预测分布**。

```{tip}
贝叶斯 p 值只是一种衡量后验预测性检查拟合度的数字方法。
```

贝叶斯 `p-value` 与频率派的 `p-value` 名字相似，定义基本上也相同：

$$
\text{Bayesian p-value}\triangleq p\left(T_{s i m} \geq T_{o b s} \mid y \right)
$$

可以解释为：从模拟数据中获得与观测数据相同或更高统计量值的概率。$T$ 几乎可以是数据的任意统计量。在图 5.4 中，统计量是左侧的平均值和右侧的四分位数范围。$T$ 应该在最初定义推断任务时就选择好。

这些 `p-value` 是贝叶斯的，因为其采样自后验预测分布。需要注意的是：贝叶斯的 `p-value` 不需要频率主义的任何零假设作为条件；事实上，我们拥有基于观测数据的整个后验分布。此外，贝叶斯也没有使用类似置信度的任何预定义阈值来声明统计显著性，当然也没有执行假设检验。这里只是试图计算一个数字来评估后验预测分布与数据集的拟合度。

无论使用曲线图还是数字摘要（如贝叶斯 `p-value` ），或者两者组合，后验预测性检查都是非常灵活的。该概念可让分析师思考不同方法来探索后验预测分布，并使用合适的方法来讲述一个数据驱动的故事，包括但不限于模型比较。在接下来几节中，我们将探索一些其他模型比较的方法。


## 5.2 奥卡姆剃刀 --- 简约性与准确性

假如对同一个问题（或数据）有两个模型，二者对数据解释得同样好，应该选哪个模型呢？有一个基本准则叫做**奥卡姆剃刀**：如果对同一现象有两种不同假说，应选用比较简单的那一种。关于奥卡姆剃刀的论证有很多，其中之一与波普尔的可证伪性有关，还有一种说法是从实用角度提出的，因为简单模型相比复杂模型更容易理解，另外还有一种论证是基于贝叶斯统计。这里不深入讨论细节，只将该准则当做一个有用而合理的常识。

在比较模型时，需要同时考虑**模型准确性**，即模型对数据拟合得怎么样。之前已见过一些衡量准确性的指标，如： $R^2$ 系数可视为线性回归中可解释方差的比例。但如果有两个模型，其中一个模型对数据的解释比另一个更准确，是否应该选更准确率的模型呢？

直觉上，似乎最好选择准确度高且简单的模型。但如果简单模型准确度最差，该怎么办？如何才能平衡这两种要素呢？为简化问题，此处引入一个例子来帮助理解如何平衡准确性与复杂性，实现从感性认识到理论的证明。该例中将使用一系列逐渐复杂的多项式来拟合一个简单数据集，为方便理解，此处未采用贝叶斯方法，而是采用最小二乘估计来拟合线性模型。当然，最小二乘估计其实可转化成一个带均匀先验的贝叶斯模型，因此，将其理解成贝叶斯方法也没问题。

```python
x = np.array([4., 5., 6., 9., 12, 14.])
y = np.array([4.2, 6., 6., 9., 10, 10.])
plt.figure(figsize=(10, 5))
order = [0, 1, 2, 5]
plt.plot(x, y, 'o')
for i in order:
    x_n = np.linspace(x.min(), x.max(), 100)
    coeffs = np.polyfit(x, y, deg=i)
    ffit = np.polyval(coeffs, x_n)
    p = np.poly1d(coeffs)
    yhat = p(x)
    ybar = np.mean(y)
    ssreg = np.sum((yhat-ybar)**2)
    sstot = np.sum((y - ybar)**2)
    r2 = ssreg / sstot
    plt.plot(x_n, ffit, label=f'order {i}, $R^2$= {r2:.2f}')
plt.legend(loc=2)
plt.xlabel('x')
plt.ylabel('y', rotation=0)
```
<center>

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210512104113_44.webp)

图 5.5 
</center>

### 5.2.1 参数太多导致过拟合

从图 5.5 可看出，模型复杂度增加时，对应的 $R^2$ 系数在上升。当多项式为 5 阶时，模型完美拟合了数据。前面章节中讨论过，用多项式去解决实际问题并不是一个特别好的办法。为什么 5 阶多项式能完美拟合所有数据呢？原因是模型中参数的数量与样本数量相同，此时都是 6，也就是说，模型只是用另一种方式对数据进行了编码，模型并没有从数据中学到任何内容，只是记住了全部数据而已。此外，如果使用这几种不同的模型做预测，5 阶多项式模型对数据的预测看起来会非常奇怪。

假设我们收集了更多数据点。例如，收集到点 [(10，9)，(7，7)] （参见图 5.6)。与 1 阶或 2 阶模型相比，5 阶模型对这些点的解释效果如何？不是很好，对吧？5 阶模型没有在数据中学习任何有趣的模式；相反，它只是记住了一些东西，因此 5 阶模型在泛化到未来数据方面做得非常糟糕：

<center>

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210512104753_be.webp)

图 5.6
</center>

当一个模型与最初用于学习其参数的数据集非常吻合，但在拟合其他数据集却非常差时，被称为过拟合。过拟合是统计学和机器学习中一个普遍问题。描述过拟合问题的一个有效方法是将数据集视为由`信号`和`噪声`两部分组成。信号是想要从数据中了解到的任何东西，如果使用某个数据集，那是因为我们认为该数据集中有一个信号，否则训练毫无意义；而噪声是数据中无用的部分，往往是测量误差、数据生成方式限制、数据损坏等因素的产物。当一个模型过于灵活，甚至能够学到噪声，而隐藏信号时，该模型就会变得过拟合。避免过拟合是奥卡姆剃刀的实际理由之一。上例表明，如果仅关注模型对数据的解释能力，很可能会被过拟合误导，因为理论上总是可以通过增加模型参数数量来提高准确率。


### 5.2.2 参数太少导致欠拟合

继续同样的例子，不过重点放在 0 阶模型上。在 0 阶模型中，所有的 $\beta$ 参数都为 0，因而两个变量间的线性关系变成了只是描述结果变量的一个高斯模型。注意对于 0 阶模型来说，预测变量对模型不再有任何影响，模型只能捕捉到结果变量的均值。换句话说，模型认为数据能够通过结果变量的均值以及一些高斯噪声来解释。我们称这种模型是欠拟合的，因为它实在太简单了，以至于不能从数据中获取有意义的模式。通常，一个参数很少的模型容易出现欠拟合。


### 5.2.3 简约性与准确性之间的平衡

经常与奥卡姆剃刀准则一起提到的是爱因斯坦的一句名言“事情应该尽可能简单，但不必过于简单”。这就像健康饮食，我们在建模时也需要保持某种平衡。理想状态下，模型既不过拟合也不欠拟合，因此，通常需要优化或者调整模型来权衡二者。

机器学习领域中，通常从`方差（variance）`和 `偏差（bias）`两个角度来讨论和权衡二者：

- 高偏差（ `bias` ）是模型适应数据的能力不足导致的。高偏差可能使模型不能捕捉到数据中一些关键的模式，因而导致欠拟合。
- 高方差（ `variance`）是模型对数据中细节过于敏感导致的。高方差会导致模型捕捉到数据中的噪声，因而可能导致过拟合。

在图 5.5 中，0 阶模型具有较高的偏差（和较低的方差），因为它偏向于在变量 $y$ 的平均值处返回一条平坦直线，而与 $x$ 值无关。5 阶模型具有较高的方差（和较低的偏差），你可以采用差别很大的方式设置六个点，会发现曲线将完美拟合其中的大多数点。

具有高偏差的模型是具有更多偏见或惯性的模型，而具有高方差的模型是一个思想更开放的模型。太有偏见的问题是没有能力容纳新证据；太开放的问题是最终会相信荒唐的东西。总体来说，如果提升其中一个方面，就会导致另外一方面的下降，这也是为什么人们称之 `偏差-方差平衡`，而我们最希望得到二者平衡的模型。


## 5.3 预测准确度的度量

在上例中，很容易看出 0 阶模型非常简单，而 5 阶模型相对数据过于复杂，但其他两个模型呢？要回答该问题，我们需要一种原则性的方式，在考虑准确性同时，兼顾考虑简单性。要做到这一点，需要引入几个新概念：

- **样本内精度**：基于拟合模型的样本数据测量得到的模型精度。
- **样本外精度**：用拟合模型的样本数据以外的数据测量得到的模型精度（也称为 `预测精度`）。

对于数据和模型的任意组合，样本内精度平均将小于样本外精度。使用样本内精确度会使我们认为拥有一个比实际更好的模型。样本外测量比样本内测量更可取，但也存在问题。因此，一个合理的做法是放弃一部分样本数据，这部分数据不参与拟合模型，而是用于测试模型。但对大多数分析师来说，仅将花大成本得到的数据用作测试，似乎过于奢侈。为避免该问题，人们花了很多精力用于获得使用样本内数据来估计样本外精度的方法。其中两种方法包括：

- 交叉验证：这是一种经验性策略，将数据分为多个子集，并轮流将其中一个子集作为测试集，剩余子集作为训练集进行评估。
- 信息准则：这是几个相对简单的表达式的总称，可认为这些表达式能够近似执行交叉验证后获得的结果。
### 5.3.1 交叉验证（Cross-validation）

交叉验证是一种简单且在有效的解决方案，可在不遗漏数据的情况下评估模型。此过程的示意见下图。通常把数据分成大致相等的 $K$ 份，使用其中 $K-1$ 份训练模型 $A_1$，剩下的 1 份用来测试模型；然后，从训练集中重新选择不同的 $K-1$ 份用于训练模型 $A_2$，并用剩余的 1 份测试模型；如此直到完成所有 $K$ 轮，得到模型 $A_K$；然后对结果 $A$ 求平均。

上述交叉验证过程被称为 `K-折交叉验证` 。当 $K$ 与数据的数量相同时（即 $ K = N$ 时），就是所谓的 `留一法交叉验证 (LOOCV)`。在执行留一法交叉验证时，如果数据数量太多，有时会出现轮数少于数据总数的情况。

<center>

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210512112629_82.webp)

图 5.7
</center>

交叉验证是机器学习从业者的谋生之本，有关更多细节，可以阅读 Sebastian Raschka 的《Python Machine Learning》一书，或 Jack Vanderplas 的《Python Data Science Handbook》。

交叉验证是一个简单而强大的想法，不过对于某些模型或者量很大的数据而言，交叉验证的计算量可能超出可接受范围。因此，许多人尝试提出了一些更容易计算的量，来得到近似交叉验证的效果，或者应用到不能直接使用交叉验证的情况，其中比较出名的是**信息量准则**。

### 5.3.2 信息量准则

信息量准则是一系列用来比较模型对数据拟合程度的方法，这类方法引入了一个惩罚项来平衡模型的复杂度。换句话说，信息量准则形式化地表示了我们在本章开头建立的一些直觉，用一种合适的方式平衡模型对数据的解释能力和模型的复杂程度。这些衡量方式的推导过程与信息论相关，不过这超出了本书的范围，我们只从实用的角度去理解这些概念。

#### **（1）Log 似然与偏差**

一种衡量模型对数据的拟合程度的方法是计算模型预测结果与真实数据之间的均方差：

$$
\frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\mathrm{E}\left(y_{i} \mid \theta\right)\right)^{2} 
$$

其中，$E(y_i|\theta)$ 是根据预估参数值得到的预测值。

可以看到基本上就是观察值和预测值之间的平均差异，求平方是为保证误差为正，不会相互抵消，此外相比其他的衡量指标（比如绝对值误差），该衡量标准更强调较大的误差。更通用的一种方法是计算 log 似然：

$$
\sum_{i=1}^{n} \log p\left(y_{i} \mid \theta\right) 
$$

当似然为正态分布时，这与二次均方误差成正比。由于历史原因，实践中人们通常不直接使用 log 似然，而是使用一个称作偏差平方和的量：

$$
-2 \sum_{i=1}^{n} \log p\left(y_{i} \mid \theta\right) 
$$

偏差平方和在贝叶斯方法和非贝叶斯方法中类似，区别在于，贝叶斯框架中 $θ$ 是来自后验的抽样。而在非贝叶斯方法中，$θ$ 是一个点估计。在使用偏差平方和的时候，需要注意以下两点：

- 偏差平方和越小，log 似然值越大，模型的预测结果与数据越吻合。因此我们**希望偏差平方和越小越好**。
- 偏差平方和衡量的是样本内的模型准确率，因而复杂模型通常会比简单模型的偏差平方和小，此时需要给复杂模型加入惩罚项。

下面我们将学习几个不同的信息量准则方法，它们的共同点是都使用了偏差平方和及正则项，区别在于偏差平方和和惩罚项的计算方式不同。

#### **（2）AIC 信息量准则**

AIC 信息量准则（Akaike Information Criterion）是一个广泛应用的信息量准则，其定义如下：

$$
A I C=-2 \sum_{i=1}^{n} \log p\left(y_{i} \mid \hat{\theta}_{m l e}\right)+2 p A I C 
$$

其中，$pAIC$ 表示参数的个数， $\hat{\theta}_{m l e}$ 时 $\theta$ 的最大似然估计。最大似然估计在非贝叶斯方法中经常用到，等价于使用贝叶斯方法中的均匀先验的最大后验估计。注意这里 $\hat{\theta}_{mle}$ 是点估计而不是分布。

同样，这里的 −2 是出于历史原因。从实用角度来看，上式中的第 1 项考虑的是模型对数据的拟合效果，第 2 项衡量的是模型复杂度。因此，如果两个模型对数据的解释能力相同，但是其中一个比另一个的参数更多的话，AIC 会告诉我们应该选择参数更少的那个。

AIC 对于非贝叶斯方法来说很有用，但是对于贝叶斯方法可能会有些问题。原因之一是 AIC 没有使用后验，因而将估计中的不确定信息丢失了，此外假设用到的是均匀先验，因而该准则对于使用非均匀先验的模型来说不太合适。在使用非均匀先验时，不能简单地计算模型中参数的个数。合理使用非均匀先验相当于对模型使用了正则，因而会降低过拟合的可能，也就是说带正则模型的有效参数个数比真实参数个数要少。类似的情况在多层模型中同样会出现，毕竟多层模型可以看作是从数据中学习先验的有效方式。

#### **（3）WAIC 通用信息量准则**

`通用信息量准则（WidelyAvailableInformationCriterion，WAIC）` 是 `AIC` 的完全贝叶斯版本。与 `AIC` 一样，`WAIC` 有两个项：一项衡量模型对数据的拟合效果；另外一项衡量模型的复杂程度。
$$
\text{WAIC}=-2 l p p d+2 p_{W A I C} 
$$

如果您想更好地理解这两个术语是什么，请阅读深入部分。从实际的角度来看，您只需要知道我们更喜欢较低的值。



#### **（4）帕累托平滑重要性采样留一交叉验证**

`帕累托平滑重要性抽样留一交叉验证` 是一种用于近似 `LOOCV`  结果但不实际执行 K 次迭代的方法。该方法不是一个信息准则，但提供的结果与 `WAIC` 非常相似。在某些通用条件下，`WAIC` 和留一法都是渐近收敛的。本方法的主要思想是通过对似然适当重新加权来近似 `LOOCV` ，这可以使用统计学中的重要性抽样来实现。该方法的问题是结果不稳定，为解决不稳定性问题，引入了一种新方法。这种新方法使用一种称为 `帕累托平滑重要性采样(PSIS)` 的技术，用来计算更可靠的留一法估计值。该方法结果与 `AIC` 和 `WAIC` 类似，数值越低，模型估计预测的精度就越高。因此，通常更倾向于选择数值较低的模型。

#### **（5）DIC 与 BIC 准则**

另一种常见的信息量准则是 `差分信息准则（DIC）` 。但无论在理论上还是在经验上，`WAIC` 都被证明比 `DIC` 更有用，因此推荐使用 `WAIC` 而不是 `DIC`。

另一个信息准则是 `贝叶斯信息准则（BIC）`，它类似于 Logistic 回归。 `BIC` 的提出是为了纠正 `AIC` 的一些问题，作者建议采用贝叶斯纠正。但 `BIC` 并不是真正的贝叶斯，实际上它与 `AIC` 非常相似。它还假设平坦的先验，并使用最大似然估计。更重要的是，BIC 不同于 AIC 和 WAIC，而更多涉及贝叶斯因子的概念，这点将在本章后面讨论。



## 5.4 使用 PyMC3 做模型比较

采用 `ArviZ` 进行模型比较想像起来容易得多！

```
waic_l = az.waic(trace_l)
waic_l
```

![image-20210524231647287](C:\Users\89877\AppData\Roaming\Typora\typora-user-images\image-20210524231647287.png)

如果你想计算 `LOO` 而不是 `WAIC` ，你必须用 `az.loo` 。

对于 `WAIC` 和 `LOO`，`PyMC3` 报告了四个值：

- 一个点估计
- 点估计的标准误差（这是通过假设正态分布计算的，因此在样本量较低时可能不太可靠）
- 参数的有效数量
- 一个警告（有关更多详细信息，请阅读 `WAIC` 和 `LOO` 计算部分的可靠性说明）



```{note}
在计算 WAIC 或 LOO 时，可能会收到一条警告消息，指出这两种计算的结果可能都不可靠。此警告是根据经验确定的分界值提出的(有关参考，请参阅持续阅读部分)。虽然这不一定是问题，但它可能表明这些度量的计算有问题。WAIC 和 LOO 相对较新，可能还需要开发更好的方法来获得它们的可靠性。无论如何，如果这种情况发生在你身上，首先，确保有足够的样本，并且你有一个混合良好、可靠的样本(参见第8章，推理引擎)。如果你仍然得到这些信息，LOO 方法的作者建议使用更健壮的模型，比如使用学生 t 分布而不是高斯分布。如果这些建议都不起作用，那么您可能需要考虑使用另一种方法，例如直接执行K-折交叉验证。更广泛地说，WAIC 和 LOO 只能帮助你在一组给定的模型中进行选择，但不能帮助你决定一个模型是否真的是解决我们特定问题的好方法。因此，WAIC 和 LOO 应该得到后验预测性检查的补充，以及任何其他信息和测试，这些信息和测试可以帮助我们根据试图解决的特定问题和领域知识来设置模型和数据。
```



## 5.5 模型平均

模型选择因其简单性而吸引人，但我们正在抛弃有关模型中不确定性的信息。这在某种程度上类似于计算完整的后验，然后只保持后验的平均值；我们可能会对自己真正知道的东西过于自信。一种替代方案是执行模型选择，但是报告和讨论不同的模型，以及计算的信息准则值、它们的标准误差值，并且可能还报告和讨论后验预测检查。把所有这些数字和测试放在我们问题的上下文中是很重要的，这样我们和我们的观众才能更好地感受到模型可能存在的局限性和缺点。如果你在学术界，你可以使用这种方法在论文、演示文稿、论文等的讨论部分添加元素。

另一种方法是充分利用模型比较中的不确定性，并进行模型平均。现在的想法是使用每个模型的加权平均值来生成元模型(和元预测)。计算这些权重的一种方法是应用以下公式


$$
w_{i}=\frac{e^{\frac{1}{2} d E_{i}}}{\sum_{j}^{M} e^{-\frac{1}{2} d E_{j}}}
$$
这里是I-ESIM模型中 `WAIC`的值与 `WAIC`最低的模型之间的差值。除了 `WAIC`，你可以使用任何你想要的信息准则，比如AIC或其他衡量标准，比如洗手间。此公式是根据 `WAIC`值(或其他类似度量)计算每个模型(给定一组固定模型)的相对概率的启发式方法。看看分母是如何只是一个规格化术语，以确保权重总和为1。您可能还记得第4章“推广线性模型”中的这个表达式，因为它只是Softmax函数。使用前面公式中的权重对模型进行平均称为伪贝叶斯建模平均。真正的贝叶斯建模平均将使用边际概率，而不是 `WAIC`或LOO。然而，即使使用边际可能性在理论上听起来很有吸引力，在模型比较和模型平均中，也有理论和经验上的理由选择 `WAIC`或Loo而不是边际可能性。您可以在贝叶斯因素一节中找到关于这一点的更多详细信息。

使用PyMC3，您可以通过将method=‘seudo-bma’(伪贝叶斯建模平均)参数传递给az.Compare函数来计算前面公式中表示的权重。此公式的一个警告是，它在计算的值时没有考虑到的不确定性。假设是高斯近似，我们可以计算每一个的标准误差。这些是函数az.waic、az.loo和函数az.Compare在传递method=‘伪bma’参数时返回的错误。我们还可以使用贝叶斯自举来估计不确定性。这是一种比假设正态更可靠的方法。如果您将method=‘bb-seudo-bma’传递给az.Compare函数，PyMC3可以为您计算这一点。

另一种计算平均模型权重的方法被称为预测分布的堆叠，或者仅仅是堆叠。这在PyMC3中是通过将method=‘STACKING’传递给az.Compare来实现的。其基本思想是通过最小化元模型和真实生成模型之间的差异，将多个模型组合到一个元模型中。使用对数计分规则时，这相当于以下内容：


$$
\max _{n} \frac{1}{n} \sum_{i=1}^{n} \log \sum_{k=1}^{K} w_{k} p\left(y_{i} \mid y_{-i}, M_{k}\right)
$$
这里是数据点的数量和模型的数量。为了强制实施解决方案，我们将其约束为和。该数量是模型的留一预测分布。正如我们已经讨论过的，计算它需要拟合每个模型时间，每次都会遗漏一个数据点。幸运的是，我们可以使用 `WAIC`或LOO来近似精确的留一预测分布，这就是PyMC3所做的。

还有其他方法来平均模型，例如，显式地构建包括所有感兴趣的模型作为子模型的元模型。我们可以这样构建这样一个模型：我们对每个子模型的参数进行推断，同时计算每个模型的相对概率(有关此方面的示例，请参阅贝叶斯因子一节)。

除了平均离散模型之外，我们有时还可以考虑它们的连续版本。一个玩具的例子是，假设我们有一个抛硬币的问题，我们有两个不同的模型：一个偏向正面，另一个偏向反面。它的一个连续版本将是一个分层模型，其中先验分布直接从数据中估计出来。该层次模型包括离散模型作为特例。

哪种方法更好？这取决于我们的具体问题。我们是否真的有很好的理由考虑离散模型，或者我们的问题更好地表示为连续模型？对于我们的问题来说，挑出一个模型很重要，因为我们是从相互竞争的解释角度思考的，或者平均是更好的想法，因为我们对预测更感兴趣，或者我们真的可以将流程生成过程视为子流程的平均吗？所有这些问题都不是由统计数据来回答的，而是由领域知识背景下的统计数据来提供信息的。

以下只是如何从PyMC3获得加权后验预测样本的一个虚拟示例。在这里，我们使用的是pm.samplepredictive_w函数(请注意函数名称末尾的w)。Pm.Sample_Retrositive_Predictive和pm.Sample_Beacter_Predictive_w之间的区别在于，后者接受多个轨迹和模型，以及权重列表(默认情况下，所有模型的权重都是相同的)。您可以从az.Compare或任何其他您想要的来源获取这些权重：

```python
w = 0.5
y_lp = pm.sample_posterior_predictive_w([trace_l, trace_p],
                                        samples=1000,
                                        models=[model_l, model_p],
                                        weights=[w, 1-w])
_, ax = plt.subplots(figsize=(10, 6))
az.plot_kde(y_l, plot_kwargs={'color': 'C1'},
            label='linear model', ax=ax)
az.plot_kde(y_p, plot_kwargs={'color': 'C2'},
            label='order 2 model', ax=ax)
az.plot_kde(y_lp['y_pred'], plot_kwargs={'color': 'C3'},
           label='weighted model', ax=ax)
plt.plot(y_1s, np.zeros_like(y_1s), '|', label='observed data')
plt.yticks([])
plt.legend()
```

<center>

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210524232628_ac.webp)

图 5.9

</center>



我说这是一个虚拟示例，因为与线性模型相比，二次模型的 `WAIC`值非常低，第一个模型的权重基本上是1，而后者的权重基本上是0，为了生成图5.9，我假设这两个模型具有相同的权重。



## 5.6 贝叶斯因子

### 5.6.1 一些评论



### 5.6.2 贝叶斯因子的计算



### 5.6.3 计算贝叶斯因子时的常见问题



### 5.6.4 用序贯蒙特卡罗方法计算贝叶斯因子



### 5.6.5 贝叶斯因子与信息准则

Bayes factors and Information Criteria

在贝叶斯的世界中，评估和比较模型的另一种方式是使用贝叶斯因子。

使用贝叶斯因子时，在单个模型中可能存在某些先验对后验分布没有实际影响，但却对贝叶斯因子有较大影响。前面的例子中你可能注意到了，通常一个标准差为 100 的正态先验与标准差为 1000 的正态

先验对后验的效果差不多，而贝叶斯因子则会受到模型中这类变化的影响。贝叶斯因子的另外一个问题是计算起来可能要比推断过程更复杂。最后一点是，贝叶斯因子可以用来做假设检验，这本来不是什么问题，不过许多作者指出，类似本书提到的基于模型和推断的思维，要比基于假设检验的这类思维在大多数问题上更好。为更好地理解什么是贝叶斯因子，这里我们再重新写一遍贝叶斯理论：

![](C:/ProgramFiles/Typora/media/image1941.png){width="1.9157130358705161in"height="0.4477241907261592in"}

其中，*y *表示数据，*θ*表示参数，你也可以写成如下形式：

![](C:/ProgramFiles/Typora/media/image1942.png){width="2.842334864391951in"height="0.45813648293963255in"}

两个式子的唯一区别是：重写后的式子中显式地描述了推断过程中依赖的模型* M*。其中分母称为证据或者边缘似然。目前为止，得益于推断引擎（如 Metropolis 和 NUTS），我们将本项省略了。这里

可以将证据表示成如下：

![](C:/ProgramFiles/Typora/media/image1943.png){width="3.0713877952755904in"height="0.19783136482939634in"}

![](C:/ProgramFiles/Typora/media/image1944.png){width="0.666334208223972in"height="0.187419072615923in"}也就是说，为计算出证据，我们需要边缘化（通过求

243

![](C:/ProgramFiles/Typora/media/image1945.png){width="0.666334208223972in"height="0.187419072615923in"}和或者积分）所有可能的，即根据给定模型边缘化所有*θ*的先验。

![](C:/ProgramFiles/Typora/media/image1944.png){width="0.666334208223972in"height="0.187419072615923in"}本身没有多少信息量，就像信息准则一样，重要的是其、
相对值，因此，当我们希望比较两个不同模型的时候，我们会计算其

证据的比例，从而得到贝叶斯因子：

![](C:/ProgramFiles/Typora/media/image1948.png){width="1.3639041994750656in"height="0.45813648293963255in"}

当* BF*\1 时，模型 0 比模型 1 对数据解释得更好。有人总结出了下面的列表来表示模型 0 与模型 1 的对比。

1-3：微弱、
3-10：中等、
10-30：强、
30-100：很强、100：非常强

注意，这些准则都是一些经验性的指导，最终结果一定要放在具体场景中去解释，同时还应该给出足够多的信息方便别人检查，从而确定是否同意我们的结论。得出结论所需的证据在不同场合下是不一样的。比如说你是在做粒子物理学，或是在法庭上，又或者是决定是否要撤离一个城镇以防止数百人死亡。

5.4.1 类比信息量准则

如果对贝叶斯因子求 log，我们可以将两个边缘似然的比值转换、
成做差，这样比较边缘似然就与前面比较信息准则类似了。不过，衡

量模型对数据的拟合程度的项以及惩罚项去哪儿了呢？前者包含在了

244

似然的部分，而后者是对先验取平均的部分。参数越多，先验空间相比似然就越大，因而平均之后似然就会较低，而且参数越多，先验就会越分散，因而在计算证据的时候惩罚越大。这也是为什么人们说贝叶斯理论会很自然地惩罚更复杂的模型，或者称贝叶斯理论自带奥卡姆剃刀。

5.4.2 计算贝叶斯因子

![](C:/ProgramFiles/Typora/media/image1955.png){width="0.7392147856517935in"height="0.187419072615923in"}贝叶斯因子的计算可以视作分层模型的应用，其中高层的参数可以看作是从一个类别分布中采样后将序号赋给每个模型。换句话说，我们同时对两个（或多个）模型进行推断，同时用一个离散的变量在模型之间做选择。对每个模型的采样次数正比于，为计算

贝叶斯因子，我们有下式：

![](C:/ProgramFiles/Typora/media/image1955.png){width="2.4258759842519684in"height="0.45813648293963255in"}

等式右边的第一项称作后验相对可能性，第 2 项称作先验相对可能性。回忆一下前面我们对相对可能性的定义。如果你好奇等式是怎

![](C:/ProgramFiles/Typora/media/image1957.png){width="0.7288035870516185in"height="0.187419072615923in"}么来的，根据贝叶斯理论将和展开后相除即可。为展示贝叶斯因子的计算过程，这里再次以抛硬币问题为例。

coins=30

heads=9

y=np.repeat([0,1],[coins-heads,heads])

用 Kruschke 图将我们的模型表示出来，如图所示，该例子中，我们选用了两个 beta 先验：一个趋近于 0；另一个趋近于 1。

245

![](C:/ProgramFiles/Typora/media/image1964.png){width="3.768956692913386in"height="5.268577209098862in"}

注意这里我们计算贝叶斯因子时比较的是模型之间先验的不同，当然，模型之间的似然或者两者同时都有可能不同，本质上思想是一致的。接下来用 PyMC3 构建模型。为切换先验，我们使用了

pm.switch() 函数，如果该函数的第一个参数为 0，则返回第 2 个参、
数，否则返回第 3 个参数。这里我们同样使用 pm.math.eq() 函数去检查 model_index 变量是否为 0。

![](C:/ProgramFiles/Typora/media/image1965.png){width="3.818897637795276e-2in"height="1.8915616797900263in"}withpm.Model()asmodel_BF:\
p=np.array([0.5,0.5])\
model_index=pm.Categorical('model_index',p=p)

m_0=(4,8)\
m_1=(8,4)\
m=pm.switch(pm.math.eq(model_index,0),m_0,m_1)

theta=pm.Beta('theta',m[0],m[1])

y=pm.Bernoulli('y',theta,observed=y)

246![](C:/ProgramFiles/Typora/media/image1968.png){width="2.7777777777777776e-2in"height="2.7777777777777776e-2in"}

![](C:/ProgramFiles/Typora/media/image1970.png){width="2.7777777777777776e-2in"height="2.7777777777777776e-2in"}trace_BF=pm.sample(5000)chain_BF=trace_BF[500:]\
pm.traceplot(chain_BF)

![](C:/ProgramFiles/Typora/media/image1974.png){width="5.643024934383202in"height="1.7596620734908137in"}

现在我们可以通过变量 model_index 计算贝叶斯因子，注意我们已经有了每个模型的先验：

pM1=chain_BF['model_index'].mean()pM0=1-pM1

BF=(pM0/pM1)\*(p[1]/p[0])

最终得到的贝叶斯因子的值约为 11，也就是说，我们更倾向于使用模型 0。该结论完全合理，因为对于*θ*=5 的情况，正面朝上出现得更少，两个模型之间的唯一区别是模型 0 更适用于*θ*\<0.5（反面朝上多于正面朝上），而模型 1 更适用于*θ*\0.5（正面朝上多于反面朝上）。

下面讲一下计算贝叶斯因子的一些常见问题。

用我们定义的方式计算贝叶斯因子会有一些问题，比如当其中一个模型比另一个模型更好时，根据定义，我们会对更好的该模型采样次数更多，这可能会导致我们对另外一个模型欠采样。另外，第 1

个问题是：即使某些参数没有用于拟合数据，也会更新。也就是说，当模型 0 被选择时，模型 1 中的参数也会更新，不过由于这部分参数并没有用于解释数据，值受限于先验。如果先验太模糊，有可能当我们选到模型 1 时，参数值距离上一次被接受的值太远了，因而该步被拒

绝，从而导致采样会出现问题。

247

程。

为避免遇到这些问题，我们对模型做了两处修改来改进采样过

理想情况下，如果两个模型都访问相同次数，我们会得到一个更好的采样，因此我们对模型的先验做出调整（前一个模型中的* p *值），从而向原来访问频次较低的模型倾斜。该过程对贝叶斯因子的计算不会有多大影响，因为我们在计算过程中包含了先、
验。\
根据 Kruschke 以及其他人的建议，可以使用伪先验，其思想很简单：当没被选择的模型的参数出现自由漂移时，可以尝试手动限制它们，不过是在该模型没被使用的时候。你可以在 Kruschke 的书中找到使用伪先验的例子，我将对应的例子转成了、
Python/PyMC3，可以查看这里：\
[https://github.com/aloctavodia/Doing*bayesian_data*](https://github.com/aloctavodia/Doingbayesian_data)analysis。

248

5.5 贝叶斯因子与信息量准则

前面我们已经说过，贝叶斯因子对先验过于敏感，做推断时某些参数对推断结果几乎没有影响，但得到的贝叶斯因子却有很大区别。这也是许多贝叶斯学派的人不喜欢贝叶斯因子的原因之一。现在我

们来看一个例子，帮助我们理解什么是贝叶斯因子以及信息准则。回到抛硬币例子中定义数据的部分，现在我们将硬币数量设为 300 个，90 个正面朝上。该比例与之前的类似，不过现在的数据量是之前的 10 倍，然后单独运行每个模型。

withpm.Model()asmodel_BF_0:

theta=pm.Beta('theta',4,8)

y=pm.Bernoulli('y',theta,observed=y)

trace_BF_0=pm.sample(5000)chain_BF_0=trace_BF_0[500:]\
pm.traceplot(trace_BF_0);

![](C:/ProgramFiles/Typora/media/image1989.png){width="5.643024934383202in"height="0.791327646544182in"}

withpm.Model()asmodel_BF_1:

theta=pm.Beta('theta',8,4)

y=pm.Bernoulli('y',theta,observed=y)trace_BF_1=pm.sample(5000)\
pm.traceplot(trace_BF_1);

![](C:/ProgramFiles/Typora/media/image1994.png){width="5.643024934383202in"height="0.8017399387576553in"}

对后验进行检查，尽管二者的先验不同，可以看到两个模型的预测结果很相似。原因是我们有足够的数据，从而将先验的效果削弱了（尽管效果还在）。现在对两个模型计算贝叶斯因子，可以得到结果约为 25，该结果意味着我们更倾向于模型 0。可以看出，当我们增加数据个数的时候，不同模型之间的对比更明显了，本点完全合理，因为数据更多的时候，我们更加确定模型 1 的先验假设与实际的数据不符。不过需要注意，当我们增加数据的时候，两个模型都倾向于得出相同的*θ*值，实际上两个模型得到的值都接近 0.3。因此，如果打算用*θ*做预测，那么两个模型其实没有特别大的区别。在该例子中，贝叶斯因子告诉我们一个模型要比另外一个模型更好，某种程度上是在帮助我们找到真正的模型，不过如果只是根据两个模型估计出来的参数做预测，二者的结果差不多。

再比较下 WAIC 和 LOO（如图）；模型 0 和模型 1 的 WAIC 分别约为 368.4 和 368.6，LOO 分别约为 365.4 和 365.7。直观上看区别似乎很小，不过重要的是在 30 个硬币中应该有 9 个正面朝上的情况下，模型 0 和模型 1 的 WAIC 分别为 38.1 和 39.4，LOO 分别为 35.6 和 38.0。也就是说，相对差别随着数据量的增加而减少了，*θ*的估计值越相似，根据信息准则得到的预测准确率的结果也越相似。该例子应该能澄清贝叶斯因子与信息准则之间的区别了。

下图显示了 WAIC 和 LOO 以及它们的标准差，第 1 行对应抛硬币问题中的 30 个样本中 9 次正面朝上的情况；第 2 行对应 300 个样本中 90个正面朝上的情况。





## 5.7 其他

### 5.7.1 正则先验

使用信息丰富和信息弱的先验是在模型中引入偏差的一种方式，如果操作得当，这可能是一个非常好的方法，因为偏差可以防止过度拟合，从而有助于模型能够做出概括性很好的预测。在不影响模型对用于拟合的数据进行适当建模的能力的情况下，添加偏差以减少泛化误差的想法称为正则化。这种正则化通常采取惩罚模型中参数的较大值的形式。这是一种减少模型能够表示的信息的方法，从而降低了模型捕获噪声而不是信号的机会。

正则化思想是如此强大和有用，以至于它已经被发现了好几次，包括在贝叶斯框架之外。在某些领域，这种思想被称为Tikhonov正则化。在非贝叶斯统计中，这种正则化思想表现为对最小二乘法的两种修正，称为岭回归和套索回归。从贝叶斯的观点来看，岭回归可以解释为对(线性模型的)贝塔系数使用正态分布，具有将系数推向零的小的标准偏差。从这个意义上说，我们一直在为本书中的每一个线性模型做一些类似岭回归的事情(除了本章中使用本章的例子以外)。另一方面，Lasso回归可以从贝叶斯的观点解释为从贝塔系数具有Laplace先验的模型计算出的后验分布图。拉普拉斯分布看起来类似于高斯分布，但它的一阶导数在零处没有定义，因为它在零处有一个非常尖锐的峰值(参见图5.14)。与正态分布相比，拉普拉斯分布使其概率质量更接近于零。使用这种先验的想法是既提供正则化又提供变量选择。我们的想法是，由于我们的峰值为零，我们预计先验会导致稀疏性，也就是说，我们创建了一个具有许多参数的模型，先验将自动使大多数参数为零，只保留对模型输出有贡献的相关变量。不幸的是，贝叶斯套索不是这样工作的，基本上是因为为了有很多参数，拉普拉斯先验迫使非零参数变小。幸运的是，并不是所有的东西都丢失了--有一些贝叶斯模型可以用来诱导稀疏性和执行变量选择，比如马蹄铁和芬兰的马蹄铁。

值得注意的是，经典版本的岭和套索回归对应于单点估计，而贝叶斯版本则给出了完整的后验分布结果：

<center>

![](https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210524233016_7c.webp)

图 5.14
</center>



### 5.7.2 深入 WAIC



### 5.7.3 熵



### 5.7.4 KL 散度



## 5.8 总结

后验预测检查是一个通用的概念和实践，它可以帮助我们了解模型捕获数据的能力，以及模型捕获我们感兴趣问题的各个方面的能力。我们可以只用一个模型进行后验预测检验，也可以用多个模型进行后验预测检验，因此我们可以用它作为模型比较的一种方法。后验预测检查通常是通过可视化完成的，但像贝叶斯值这样的数字总结也可能是有帮助的。

好的模型在复杂性和预测准确性之间有很好的平衡。我们用多项式回归的经典例子来举例说明这一特征。我们讨论了两种在不留数据的情况下估计样本外准确度的方法：交叉验证法和信息准则法。我们集中讨论了后者。从实践的角度来看，信息准则是一系列平衡两种贡献的方法：一种是衡量模型与数据的拟合程度，另一种是惩罚复杂的模型。在众多可用的信息准则中， `WAIC`是贝叶斯模型中最有用的。另一个有用的方法是PSIS-LOO-CV(或LOO)，它在实践中提供了与 `WAIC`非常相似的结果。这是一种用于近似遗漏一次交叉验证的方法，而不需要多次实际改装模型的高计算成本。 `WAIC`和LOO可用于模型选择，也可用于模型平均。模型平均不是选择单个最佳模型，而是通过对所有可用模型进行加权平均来组合所有可用模型。

模型选择、比较和模型平均的另一种方法是贝叶斯因子，它是两个模型的边际可能性之比。贝叶斯因子的计算可能真的很有挑战性。在本章中，我们介绍了使用PyMC3计算它们的两种方法：一种是直接尝试使用离散指数估计每个模型的相对概率的分层模型，另一种是称为序贯蒙特卡罗(Sequential Monte Carlo)的抽样方法。我们建议使用后者。

考虑到贝叶斯因子对先前的规范非常敏感，除了在计算上具有挑战性之外，使用贝叶斯因子也是有问题的，我们还比较了贝叶斯因子和信息准则，并通过一个示例向您介绍了它们解决两个相关但不同的问题-一个侧重于确定正确的模型，另一个侧重于最佳预测或更低的泛化损失。这些方法没有一个是没有问题的，但 `WAIC`和Loo在实践中要健壮得多。



## 5.9 习题

