
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>第 3 章 线性回归模型的贝叶斯视角 &#8212; Python贝叶斯分析(中文)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="第 4 章 广义线性回归模型与分类任务" href="chapter04-GeneralizedLinearRegression.html" />
    <link rel="prev" title="第 2 章 概率编程" href="chapter02-ProgrammingProbabilistically.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Python贝叶斯分析(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   封面
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter01-ThinkingProbabilistically.html">
   第 1 章 概率思维
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter02-ProgrammingProbabilistically.html">
   第 2 章 概率编程
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   第 3 章 线性回归模型的贝叶斯视角
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter04-GeneralizedLinearRegression.html">
   第 4 章 广义线性回归模型与分类任务
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter05-ModelComparison.html">
   第 5 章 模型比较
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter06-MixtureModels.html">
   第 6 章 混合模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter07-GaussianProcesses.html">
   第 7 章 高斯过程
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/chapter03-ModellingwithLinearRegression.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   3.1 一元线性回归
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     3.1.1 与机器学习的联系
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     3.1.2 线性回归模型的核心
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     3.1.3 线性模型与高自相关性问题
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id6">
       （1）解决方法 1：运行之前做中心化或标准化处理
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id7">
       （2）解决办法 2：更换采样方法
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     3.1.4 对后验进行解释和可视化
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     3.1.5 皮尔逊相关系数
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     3.1.6 多元高斯分布的皮尔逊系数
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   3.2 稳健的线性回归
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id12">
   3.3 分层线性回归
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     3.3.1 相关性与因果性
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id14">
   3.4 多项式回归
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     3.3.1 解释多项式回归的系数
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     3.3.2 多项式回归— 终极模型？
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id17">
   3.5 多元线性回归
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     3.5.1 混淆变量和冗余变量
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id19">
     3.5.2 多重共线性或相关性太高
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id20">
     3.5.3  掩藏有效变量
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id21">
     3.5.4 增加变量间的交互作用
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id22">
   3.6 变量方差的线性回归模型
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id23">
   3.7 总结
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id24">
   3.8 练习
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1>第 3 章 线性回归模型的贝叶斯视角<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>本章将介绍最流行、最有用的一种模型–线性模型。其本身就是非常有用的模型，也是许多其他模型的基础。如果你学过统计学课程，你可能听说过简单线性回归、多元线性回归、逻辑回归、方差分析、方差分析等。所有这些方法都是同一基本主题–线性回归模型的变体。在本章中，我们将介绍以下主题：</p>
<ul class="simple">
<li><p>一元线性回归</p></li>
<li><p>稳健的线性回归</p></li>
<li><p>层次线性回归</p></li>
<li><p>多项式线性回归</p></li>
<li><p>多元线性回归</p></li>
<li><p>交互作用</p></li>
<li><p>变量方差</p></li>
</ul>
<hr class="docutils" />
<div class="section" id="id2">
<h2>3.1 一元线性回归<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>实际应用中经常会遇到下面这类问题：我们有一个连续变量 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> ，希望对另外一个变量 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 建模并进行预测。这些变量通常以 <span class="math notranslate nohighlight">\(\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}\)</span> 成对方式出现。此类问题一般可通过线性回归建模，当只有一个自变量时，被称为一元线性回归模型。</p>
<p>在上述描述中，通常 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 被称为自变量、预测变量或输入变量，而 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 被称为因变量、被预测变量或输出变量。对于一元线性回归模型，在 <code class="docutils literal notranslate"><span class="pre">Numpy</span></code> 中， <span class="math notranslate nohighlight">\(\mathbf{x,y}\)</span> 均可以表达成一维数组的形式。如果有多个自变量，即 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 为矩阵时，则该模型称为多元线性回归模型。</p>
<p>使用线性模型的一些典型场景包括：</p>
<ul class="simple">
<li><p>对多个因素之间的关系建模，如雨量、土壤盐度与农作物是否施肥等，然后回答：它们之间的关系是否线性？关系有多强？哪个因素影响最强？</p></li>
<li><p>找出全国平均巧克力摄入量与诺贝尔奖得主数量之间的关系。理解为什么这二者之间的关系可能是假的。</p></li>
<li><p>根据当地天气预报中的太阳辐射，预测家里的燃气账单。该预测的准确性如何？</p></li>
</ul>
<div class="section" id="id3">
<h3>3.1.1 与机器学习的联系<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>按照 <code class="docutils literal notranslate"><span class="pre">Kevin</span> <span class="pre">P.</span> <span class="pre">Murphy</span></code> 的说法，机器学习是一个总称，指一系列从数据中自动学习隐藏规律，并用来预测未知数据，或在不确定状态中做决策的方法。机器学习与统计学相互交织，不过正如 <code class="docutils literal notranslate"><span class="pre">Kevin</span> <span class="pre">P.</span> <span class="pre">Murphy</span></code> 在《 <code class="docutils literal notranslate"><span class="pre">Machine</span> <span class="pre">learning:</span> <span class="pre">A</span> <span class="pre">probabilistic</span> <span class="pre">perspective</span></code> 》中所说：“如果从概率视角来看，二者间的关系就比较清晰了”。尽管两个领域在概念和数学上紧密联系，但二者间不同的术语让这种联系显得不那么清晰了。因此，本文会介绍一些机器学习中的术语。</p>
<p>用机器学习的行话来说，回归问题属于典型的<code class="docutils literal notranslate"><span class="pre">监督学习</span></code>。在机器学习框架中，如果学习从 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 到 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 的一个映射，这就是<code class="docutils literal notranslate"><span class="pre">回归</span></code>问题。其中 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 是连续变量。在机器学习中通常用 <code class="docutils literal notranslate"><span class="pre">特征</span></code> 这个名词取代统计中的 <code class="docutils literal notranslate"><span class="pre">变量</span></code>。而 <code class="docutils literal notranslate"><span class="pre">监督</span></code> 的意思指，已经知道 <span class="math notranslate nohighlight">\(\{X,Y\}\)</span> 对的观测值，如何从这些观测值（或者数据集）中抽象出一种映射关系来处理未来的观测（即只知道 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 而不知道 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 的情形）。</p>
</div>
<div class="section" id="id4">
<h3>3.1.2 线性回归模型的核心<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>让我们开始学习如何构建线性模型。看下面该公式：</p>
<div class="math notranslate nohighlight">
\[
\mathbb{y} = \alpha + \mathbb{x} \beta \tag{式3.1}
\]</div>
<p>该等式描述了变量 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 与变量 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 之间的线性关系。其中，参数 <span class="math notranslate nohighlight">\(β\)</span> 控制直线的斜率，可以理解为变量 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 的单位变化量所对应 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 的变化量。另外一个参数 <span class="math notranslate nohighlight">\(α\)</span> 为截距，可以解释为当 <span class="math notranslate nohighlight">\(x_i=0\)</span> 时, <span class="math notranslate nohighlight">\(y_i\)</span> 的值，在图形上表示， <span class="math notranslate nohighlight">\(α\)</span> 就是直线与 <span class="math notranslate nohighlight">\(y\)</span> 轴交点的坐标。</p>
<p>计算线性模型参数的方法很多，最小二乘法是其中之一。每次使用软件去拟合直线时，底层可能用的就是该方法。最小二乘法返回的 <span class="math notranslate nohighlight">\(α\)</span> 和 <span class="math notranslate nohighlight">\(β\)</span> 能够让观测到的 <span class="math notranslate nohighlight">\(y\)</span> 与预测的 <span class="math notranslate nohighlight">\(\hat y\)</span> 之间误差平方的均值最小。这样估计 <span class="math notranslate nohighlight">\(α\)</span> 和 <span class="math notranslate nohighlight">\(β\)</span> 就变成了一个最优化问题，最优化问题的目标一般是寻找目标函数的最小值（或最大值）。</p>
<p>最优化并非求解线性模型的唯一方法，同样的问题可以从概率（贝叶斯）角度描述。用概率方式思考带来的优势是：在得到最优的参数 <span class="math notranslate nohighlight">\(α\)</span> 和 <span class="math notranslate nohighlight">\(β\)</span> 同时，还知道这些参数的不确定程度，而最优化方法则需要一些其他工作来提供不确定性信息。此外，贝叶斯方法还具备很大灵活性，尤其在使用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 等工具时，可将模型应用到本章后面所介绍的各种特定问题中。</p>
<p>从概率角度，线性回归模型可以表示成如下形式：</p>
<div class="math notranslate nohighlight">
\[
\mathbb{y} \sim \mathcal{N} ( \mu = \alpha + \mathbb{x} \beta , \epsilon ) \tag{式3.2}
\]</div>
<p>也就是说，这里假设向量 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 是服从均值为 <span class="math notranslate nohighlight">\(α+\mathbb{x}β\)</span> 、标准差为 <span class="math notranslate nohighlight">\(\epsilon\)</span> 的正态分布。其中 <span class="math notranslate nohighlight">\(α\)</span> 、 <span class="math notranslate nohighlight">\(β\)</span> 、 <span class="math notranslate nohighlight">\(\epsilon\)</span> 为未知的模型参数（贝叶斯角度视为随机变量），需设置先验，下面是一组假设参数服从正态分布的先验设置：</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} \tag{式3.3}
\alpha &amp;\sim \mathcal{N}\left(\mu_{\alpha}, \sigma_{\alpha}\right) \\ 
\beta &amp;\sim \mathcal{N}\left(\mu_{\beta}, \sigma_{\beta}\right) \\ 
\epsilon &amp;\sim\left|N\left(0, \sigma_{\epsilon}\right)\right|  
\end{align*}\]</div>
<p>对于 <span class="math notranslate nohighlight">\(α\)</span> 的先验，由于截距值根据问题不同会有很大变化，因此可使用一个分布平坦的高斯分布（即 <span class="math notranslate nohighlight">\(\sigma_\alpha\)</span> 相对于数据的值域大很多）。根据以往经验，通常截距都是以 0 为中心，并且不大于 10，但这仅限于作者处理的问题域范畴，不能转移到其他问题上。</p>
<p>关于斜率 <span class="math notranslate nohighlight">\(\beta\)</span> ，可能比截距更容易获得预期。许多问题中至少可以先验地知道斜率的正负符号；例如，期望可变权重 <span class="math notranslate nohighlight">\(\beta\)</span> 平均随着高度变化而增加。</p>
<p>对于 <span class="math notranslate nohighlight">\(\epsilon\)</span> ，由于其值大于 0，采用半高斯分布，可以将 <span class="math notranslate nohighlight">\(\sigma_\epsilon\)</span> 设置为相对 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 较大的值，如设置为 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 标准差的 10 倍，以保留足够的空间通过数据似然来驱动不确定性的收缩。</p>
<p>上述模糊的先验可保证先验对后验影响较小，并很容易被数据所克服。</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>使用最小二乘法得到的点估计与采用了平坦先验的贝叶斯一元线性回归的最大后验估计（MAP）一致。</p>
</div>
<p>半高斯分布可以采用均匀分布或半柯西分布来代替。半柯西分布是一个很好的正则化先验，而均匀分布通常不是一个非常好的选择，除非知道参数确实受到了硬边界的限制。</p>
<p>如果想在标准差的某个特定值周围使用强先验，可以使用伽马分布。许多软件包中伽马分布的默认参数化可能有点混乱，但幸运的是，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 允许使用形状和速率、或者平均值和标准差来定义它。要查看伽马和其他分布的形状，可以查看 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 文档：<a class="reference external" href="https://docs.pymc.io/api/distributions/continuous.html%E3%80%82">https://docs.pymc.io/api/distributions/continuous.html。</a></p>
<p>回过头来再看线性回归模型，借助  <code class="docutils literal notranslate"><span class="pre">Kruschke</span> <span class="pre">图</span></code> ，我们有下面这张图。在上一章的  <code class="docutils literal notranslate"><span class="pre">Kruschke</span> <span class="pre">图</span></code> 中，我们使用符号 = 来定义确定性变量（如 <span class="math notranslate nohighlight">\(\mu\)</span> )，使用 ∼ 来定义随机变量，如 <span class="math notranslate nohighlight">\(\alpha\)</span> 、 <span class="math notranslate nohighlight">\(\beta\)</span> 和 <span class="math notranslate nohighlight">\(\epsilon\)</span> ：</p>
<center>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505092353_aa.webp" style="zoom:50%;" />
<p>图3.1</p>
</center>
<p>定义好模型后，需要为其提供数据。这里继续采用合成数据集（合成数据集的优点是，我们知道参数的正确值，进而很方便检查是否能够使用模型恢复它们）：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">N</span><span class="o">=</span><span class="mi">100</span>
<span class="n">alpha_real</span><span class="o">=</span><span class="mf">2.5</span>
<span class="n">beta_real</span><span class="o">=</span><span class="mf">0.9</span>
<span class="n">eps_real</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mf">0.5</span><span class="p">,</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">x</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">N</span><span class="p">)</span>
<span class="n">y_real</span><span class="o">=</span><span class="n">alpha_real</span><span class="o">+</span><span class="n">beta_real</span><span class="o">*</span><span class="n">x</span>
<span class="n">y</span><span class="o">=</span><span class="n">y_real</span><span class="o">+</span><span class="n">eps_real</span>

<span class="n">_</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span><span class="mi">4</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="s1">&#39;C0.&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span><span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y_real</span><span class="p">,</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">y</span><span class="p">,</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<center>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505092831_15.webp" style="zoom:67%;"/>
<p>图3.2</p>
</center>
<p>现在使用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 来构建和拟合模型。注意这里 <span class="math notranslate nohighlight">\(\mu\)</span> 在模型中通过 <code class="docutils literal notranslate"><span class="pre">pm.deterministic</span></code> 来定义，表示它是<code class="docutils literal notranslate"><span class="pre">确定性变量</span></code>，反映了数学表达式和   <code class="docutils literal notranslate"><span class="pre">Kruschke</span> <span class="pre">图</span></code> 的内容。如果显式定义了一个确定性变量，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 会计算该变量并保存它的迹：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">withpm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span><span class="n">asmodel_g</span><span class="p">:</span>
<span class="n">α</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span><span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">β</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span><span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ϵ</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">μ</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;μ&#39;</span><span class="p">,</span><span class="n">α</span><span class="o">+</span><span class="n">β</span><span class="o">*</span><span class="n">x</span><span class="p">)</span>
<span class="n">y_pred</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span><span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span><span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span><span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
<span class="n">trace_g</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span><span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>你也可以不在模型中显式地定义它。此时 PCMC3 仍会计算该变量，但不会将其保存在迹中。例如，可编写以下代码：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y_pred</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span><span class="n">mu</span><span class="o">=</span><span class="n">α</span><span class="o">+</span><span class="n">β</span><span class="o">*</span><span class="n">x</span><span class="p">,</span><span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span><span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>为探索推断的结果，可以生成未知变量的迹图，此处省略了确定性变量 <span class="math notranslate nohighlight">\(\mu\)</span> 。可以通过将希望包含在绘图中的变量名称以列表形式传递给 <code class="docutils literal notranslate"><span class="pre">var_names</span></code> 参数来实现多变量绘制。许多 <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 函数都有一个 <code class="docutils literal notranslate"><span class="pre">var_names</span></code> 参数，你可以随意尝试其他 <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 的绘图来探索后验。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_g</span><span class="p">,</span><span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">,</span><span class="s1">&#39;β&#39;</span><span class="p">,</span><span class="s1">&#39;ϵ&#39;</span><span class="p">])</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505093554_e3.webp" /></p>
<p>图3.3</p>
</center>
<p>下一节将讨论线性模型性质，以及它如何影响采样过程和模型解释，并将介绍几种解释和可视化后验的方法。</p>
</div>
<div class="section" id="id5">
<h3>3.1.3 线性模型与高自相关性问题<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>前面模型中， <span class="math notranslate nohighlight">\(α\)</span> 和 <span class="math notranslate nohighlight">\(β\)</span> 有很严重的自相关性，这意味着采样结果会很差，而且相比实际采样数，有效的采样很少。为什么呢？其实我们是被自己的假设误导了。</p>
<p>事实上，不论用哪条直线去拟合数据，该直线都会穿过 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 均值和 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 均值的点。因此，拟合直线的过程相当于将直线固定在数据中心上旋转，造成斜率越大截距越小的相关性。如果将后验画出来的话可以很清楚地看到这点（这里暂时忽略 <span class="math notranslate nohighlight">\(ε\)</span> ）。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">trace_g</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="s1">&#39;β&#39;</span><span class="p">],</span> <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;alpha&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">})</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510120946ab.webp" /></p>
<p>图3.4</p>
</center>
<p>可以看到，后验（忽略 <span class="math notranslate nohighlight">\(ε\)</span> 后）呈斜对角形状，这对于类似 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code>-Hastings 的采样器会有问题（详细解释见第 8 章），而且参数维度越高，这种情况越严重。</p>
<p>在继续深入前，需澄清一点：前面提到的拟合直线会穿过均值点的现象只在最小二乘算法假设下成立。使用贝叶斯方法后，该限制会被放松。后面的例子中可以看到，通常直线会在 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 均值附近而不是正好穿过均值。不过没关系，自相关性与直线固定在某一点附近的假设仍然成立。接下来从两个方面理解和解决高自相关性问题。</p>
<div class="section" id="id6">
<h4>（1）解决方法 1：运行之前做中心化或标准化处理<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<p>解决问题的一个简单办法是先将 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 中心化，也就是说，对于每个点 <span class="math notranslate nohighlight">\(x_i\)</span> ，减去 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 的均值。这样做的结果是 <span class="math notranslate nohighlight">\(x'\)</span> 的中心在 0 附近，从而在修改斜率时，旋转点与截距点重合，参数空间也会变得不那么自相关。</p>
<div class="math notranslate nohighlight">
\[
x'=x-\bar x \tag{式3.4}
\]</div>
<p>中心化不仅是一种计算技巧，同时有利于解释数据。截距是指当 <span class="math notranslate nohighlight">\(x_i=0\)</span> 时 <span class="math notranslate nohighlight">\(y_i\)</span> 的值，不过对许多问题而言，截距并没有什么实际意义。例如，对于身高或者体重的关系模型，当值为 0 时没有实际意义，因而截距对理解数据也就没有帮助；对于另外一些问题，估计出截距可能很有用，因为在实验中可能无法测量出 <span class="math notranslate nohighlight">\(x_i=0\)</span> 的情况，此时截距的估计值能够提供有价值的信息。不管怎么说，外推都有其局限性，应当谨慎使用！</p>
<p>根据问题和受众不同，我们可能需要汇报中心化之前或者之后估计到的参数值。如果我们需要汇报的是中心化之前的参数，那么可以像下面这样将参数转换成原来的尺度：</p>
<div class="math notranslate nohighlight">
\[
\alpha=\alpha^{\prime}-\beta^{\prime} \bar{x} \tag{式3.5}
\]</div>
<p>上面的公式可以通过以下公式推导出来：</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} \tag{式3.6}
y&amp;=\alpha^{\prime}+\beta^{\prime} x^{\prime}+\epsilon  \\
y&amp;=\alpha^{\prime}+\beta^{\prime}(x-\bar{x})+\epsilon  \\
y&amp;=\alpha^{\prime}-\beta^{\prime} \bar{x}+\beta^{\prime} x+\epsilon 
\end{align*}\]</div>
<p>然后可以得出：</p>
<div class="math notranslate nohighlight">
\[
\beta = \beta' \tag{式3.7}
\]</div>
<p>进一步，在运行模型之前可以对数据进行<code class="docutils literal notranslate"><span class="pre">标准化处理</span></code>。标准化在统计学和机器学习中是常见的数据处理手段，因为许多算法对标准化后的数据效果更好。标准化过程是在中心化后再除以标准差，其数学形式如下：</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} \tag{式3.8}
x^{\prime} &amp;=\frac{x-\bar{x}}{x_{s d}} \\
y^{\prime} &amp;=\frac{y-\bar{y}}{y_{s d}} 
\end{align*}\]</div>
<p>标准化的好处之一是可以对数据使用相同的弱先验，而不必关心数据具体值域有多大，因为我们已经对数据做了尺度变换。对于标准化后的数据，截距通常在 0 附近，斜率在-1～1 附近。标准化后的数据可以使用 <code class="docutils literal notranslate"><span class="pre">标准分数（z-score）</span></code> 来描述参数。如果某人声称一个参数的 <code class="docutils literal notranslate"><span class="pre">z-score</span></code> 为 1.3，那么我们就知道该值在标准化前位于均值附近 1.3 倍标准差处。<code class="docutils literal notranslate"><span class="pre">z-score</span></code> 每变化一个单位，对应原始数据中变化 1 倍标准差。这点在分析多变量时很有用，因为所有参数都在同一个尺度上，进而简化了对数据的解释。</p>
</div>
<div class="section" id="id7">
<h4>（2）解决办法 2：更换采样方法<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h4>
<p>另外一种解决高自相关性的办法是使用不同采样方法。<code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 算法与 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 算法相比，在类似受限的对角空间中遇到的困难小一些。原因是 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 是根据后验曲率来移动的，因而更容易沿着对角空间移动。<code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 算法每走一步都要比 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 算法更慢，但得到一个合理后验近似值所需步数更少（相关解释见第 8 章）。</p>
</div>
</div>
<div class="section" id="id8">
<h3>3.1.4 对后验进行解释和可视化<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>正如已经看到的，我们可以使用 <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 函数（如 <code class="docutils literal notranslate"><span class="pre">plot_trace</span></code> 和 <code class="docutils literal notranslate"><span class="pre">summary</span></code> ) 探索后验，也可以使用自己的函数。对于线性回归，绘制出符合数据均值的直线，并标示参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 的均值可能很有用。为反映后验的不确定性，可以从后验中采样并以半透明线条形式绘制。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;C0.&#39;</span><span class="p">)</span>
<span class="n">alpha_m</span> <span class="o">=</span> <span class="n">trace_g</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">beta_m</span> <span class="o">=</span> <span class="n">trace_g</span><span class="p">[</span><span class="s1">&#39;β&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">draws</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">trace_g</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">]),</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">trace_g</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">][</span><span class="n">draws</span><span class="p">]</span> <span class="o">+</span> <span class="n">trace_g</span><span class="p">[</span><span class="s1">&#39;β&#39;</span><span class="p">][</span><span class="n">draws</span><span class="p">]</span>
         <span class="o">*</span> <span class="n">x</span><span class="p">[:,</span> <span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha_m</span> <span class="o">+</span> <span class="n">beta_m</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;y = </span><span class="si">{</span><span class="n">alpha_m</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">beta_m</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> * x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<center>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051012334888.webp" style="zoom: 67%;" />
<p>图3.5</p>
</center>
<p>可以看到，上图中间部分的不确定性较低，不过直线并没有都相交于一个点（后验并不强制所有直线都穿过均值点）。</p>
<p>半透明的直线看起来不错，不过我们可能想给该图增加点更酷的东西：用半透明的区间来描述 <span class="math notranslate nohighlight">\(μ\)</span> 的最大后验密度（ <code class="docutils literal notranslate"><span class="pre">HPD</span></code> ）区间。注意这也是在模型中将变量 <span class="math notranslate nohighlight">\(μ\)</span> 定义为确定性变量的原因，简化以下代码：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha_m</span> <span class="o">+</span> <span class="n">beta_m</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;y = </span><span class="si">{</span><span class="n">alpha_m</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">beta_m</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> * x&#39;</span><span class="p">)</span>
<span class="n">sig</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">plot_hpd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">trace_g</span><span class="p">[</span><span class="s1">&#39;μ&#39;</span><span class="p">],</span> <span class="n">credible_interval</span><span class="o">=</span><span class="mf">0.98</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<center>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051012370861.webp" style="zoom:67%;" />
<p>图3.6</p>
</center>
<p>另外一种方式是画预测值 <span class="math notranslate nohighlight">\(\hat y\)</span> 的 <code class="docutils literal notranslate"><span class="pre">HPD</span></code>（例如 94%和 50%）区间。也就是说，我们想要根据模型看到未来 94%和 50%的数据的分布范围。我们在图中将 <code class="docutils literal notranslate"><span class="pre">50%</span> <span class="pre">HPD</span> <span class="pre">区间</span></code> 用深灰色区域表示，将 <code class="docutils literal notranslate"><span class="pre">94%</span> <span class="pre">HPD</span></code> 区间用浅灰色表示。</p>
<p>利用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 中的 <code class="docutils literal notranslate"><span class="pre">sample_posterior_predictive</span></code> 函数可以很容易得到预测值的采样。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_g</span><span class="p">,</span>
                                     <span class="n">samples</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>
                                     <span class="n">model</span><span class="o">=</span><span class="n">model_g</span><span class="p">)</span>
</pre></div>
</div>
<p>然后我们可以画出结果：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;b.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">alpha_m</span> <span class="o">+</span> <span class="n">beta_m</span> <span class="o">*</span> <span class="n">x</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
         <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;y = </span><span class="si">{</span><span class="n">alpha_m</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">beta_m</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> * x&#39;</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_hpd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ppc</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">],</span> <span class="n">credible_interval</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_hpd</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">ppc</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<center>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051012410596.webp" style="zoom:67%;" />
<p>图3.7</p>
</center>
<p>函数 <code class="docutils literal notranslate"><span class="pre">az.plot_hpd</span></code> 是一个辅助函数，可以使用它来绘制线性回归的 HPD 间隔。默认情况下，此功能会平滑间隔，可尝试传递参数 <code class="docutils literal notranslate"><span class="pre">smooth=false</span></code> 取消默认值。</p>
</div>
<div class="section" id="id9">
<h3>3.1.5 皮尔逊相关系数<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>有时我们希望衡量两个变量之间的（线性）依赖关系。度量两个变量间线性相关性最常见的指标是<code class="docutils literal notranslate"><span class="pre">皮尔逊相关系数（Pearson</span> <span class="pre">correlation</span> <span class="pre">coefficient）</span></code> ，通常用小写的 <span class="math notranslate nohighlight">\(r\)</span> 表示。如果 <span class="math notranslate nohighlight">\(r\)</span> 值为 <span class="math notranslate nohighlight">\(+1\)</span> ，我们称两个变量完全正相关，即一个变量随另一个变量的增加而增加；如果 <span class="math notranslate nohighlight">\(r\)</span> 值为 <span class="math notranslate nohighlight">\(-1\)</span> ，则称完全负相关，即一个变量随另一变量的增加而减少；当 <span class="math notranslate nohighlight">\(r\)</span> 为 0 时，称两个变量间没有线性相关性。皮尔逊相关系数并不涉及非线性相关性。人们很容易将皮尔逊相关系数与线性回归中的斜率弄混淆，但查看 <a class="reference external" href="https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg">此链接</a> 就可以明白，二者本质上是两个完全不同的量。</p>
<p>下面的公式可以在某种程度上减轻你的疑惑：</p>
<div class="math notranslate nohighlight">
\[
r=\beta \frac{\sigma_{x}}{\sigma_{y}} \tag{式3.9}
\]</div>
<p>只有在 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 的标准差相等时，皮尔逊相关系数才与斜率相等。也就是说，当我们对数据标准化后，两者之间确实可以等价。需要注意：</p>
<ul class="simple">
<li><p>皮尔逊相关系数衡量的是两个变量之间的相关性程度，其值位于 [-1,1] 区间内，与数据尺度无关；</p></li>
<li><p>斜率 <span class="math notranslate nohighlight">\(\beta\)</span> 表示 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 变化一个单位时 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 的变化量，可以取任意实数。</p></li>
</ul>
<p>皮尔逊系数与一个被称为 <code class="docutils literal notranslate"><span class="pre">确定系数</span></code> 的量有关。对于线性回归模型，确定系数就是皮尔逊系数的平方，即 <span class="math notranslate nohighlight">\(r^2\)</span> （有时写为 <span class="math notranslate nohighlight">\(R^2\)</span> ），发音为 <span class="math notranslate nohighlight">\(r\)</span> 平方。确定系数可以定义为预测值的方差除以测量数据的方差。因此，可以解释为从自变量预测的因变量中方差（变异部分）所占的比例。对于贝叶斯线性回归，预测值的方差可以大于数据的方差，这将导致 <span class="math notranslate nohighlight">\(R^2\)</span> 大于 1。一个解决方案是定义 <span class="math notranslate nohighlight">\(R^2\)</span> 如下：</p>
<div class="math notranslate nohighlight">
\[
R^{2}=\frac{\mathbf{V}_{n=1}^{N} \mathbf{E}\left[\hat{y}^{s}\right]}{\mathbf{V}_{n=1}^{N} \mathbf{E}\left[\hat{y}^{s}\right]+\mathbf{V}_{n=1}^{S}\left(\hat{y}^{s}-y\right)} \tag{式3.10}
\]</div>
<p>上式中，<span class="math notranslate nohighlight">\(E[\hat y^2]\)</span> 是后验样本 <span class="math notranslate nohighlight">\(S\)</span> 上，预测值 <span class="math notranslate nohighlight">\(\hat y\)</span> 的期望（或平均值）。</p>
<p>上式为“预测值的方差”除以“预测值的方差加误差（或残差）”。该定义的优点是确保 <span class="math notranslate nohighlight">\(R^2\)</span> 被限制在区间 [0，1] 内。</p>
<p>计算 <span class="math notranslate nohighlight">\(R^2\)</span> 最简单的方法是使用 <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 的 <code class="docutils literal notranslate"><span class="pre">r2_core()</span></code> 函数。我们需要观测值 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 和预测值 <span class="math notranslate nohighlight">\(\hat y\)</span> 。其中， <span class="math notranslate nohighlight">\(\hat y\)</span> 可利用 <code class="docutils literal notranslate"><span class="pre">sample_posterior_predictive</span></code> 函数轻松获得：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ppc</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>默认情况下，此函数将返回 <span class="math notranslate nohighlight">\(R^2\)</span> （本例为 0.8) 和标准差 (0.03)。</p>
</div>
<div class="section" id="id10">
<h3>3.1.6 多元高斯分布的皮尔逊系数<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>计算皮尔逊系数的另一种方法是估计多变量高斯分布的协方差矩阵。多元高斯分布是高斯分布在一维以上的推广。以二维为例，要完全描述一个二元高斯分布，需要两个均值（或一个具有两个元素的向量），每个高斯一个。我们还需要一个 2x2 的协方差矩阵，如下所示：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\Sigma=\left[\begin{array}{cc}\sigma_{\mathbb{x}_{1}}^{2} &amp; \rho \sigma_{\mathbb{x}_{1}} \sigma_{\mathbb{x}_{2}} \\ \rho \sigma_{\mathbb{x}_{1}} \sigma_{\mathbb{x}_{2}} &amp; \sigma_{\mathbb{x}_{2}}^{2}\end{array}\right]  
\end{split}\]</div>
<p>这里 <span class="math notranslate nohighlight">\(\Sigma\)</span> 为希腊大写的 sigma 字母，通常使用它表示协方差矩阵。在主对角线上，有每个变量的方差，用其标准差 <span class="math notranslate nohighlight">\(\sigma_{\mathbb{x}_1}、\sigma_{\mathbb{x}_2}\)</span>  的平方来表示。矩阵中其余元素是协方差（变量之间的方差），用单个标准差和变量间的皮尔逊相关系数 <span class="math notranslate nohighlight">\(\rho\)</span> 表示。请注意，我们只有一个 <span class="math notranslate nohighlight">\(\rho\)</span> ，因为只有两个维度。对于三个变量，我们会有三个皮尔逊系数。</p>
<p>下面的代码为双变量高斯分布生成等高线图，两个均值都固定在 (0，0) 。其中一个标准差是固定的，而另一个标准差采用值 1 或 2 以及皮尔逊相关系数 <span class="math notranslate nohighlight">\(\rho\)</span> 的不同值：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sigma_x1</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">sigmas_x2</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="n">rhos</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mf">0.90</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.90</span><span class="p">]</span>
<span class="n">k</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mgrid</span><span class="p">[</span><span class="o">-</span><span class="mi">5</span><span class="p">:</span><span class="mi">5</span><span class="p">:</span><span class="mf">.1</span><span class="p">,</span> <span class="o">-</span><span class="mi">5</span><span class="p">:</span><span class="mi">5</span><span class="p">:</span><span class="mf">.1</span><span class="p">]</span>
<span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">shape</span> <span class="o">+</span> <span class="p">(</span><span class="mi">2</span><span class="p">,))</span>
<span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span>
<span class="n">pos</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">l</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sigmas_x2</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">rhos</span><span class="p">),</span>
                     <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span>
                     <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
        <span class="n">sigma_x2</span> <span class="o">=</span> <span class="n">sigmas_x2</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">rho</span> <span class="o">=</span> <span class="n">rhos</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">cov</span> <span class="o">=</span> <span class="p">[[</span><span class="n">sigma_x1</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">sigma_x1</span><span class="o">*</span><span class="n">sigma_x2</span><span class="o">*</span><span class="n">rho</span><span class="p">],</span>
               <span class="p">[</span><span class="n">sigma_x1</span><span class="o">*</span><span class="n">sigma_x2</span><span class="o">*</span><span class="n">rho</span><span class="p">,</span> <span class="n">sigma_x2</span><span class="o">**</span><span class="mi">2</span><span class="p">]]</span>
        <span class="n">rv</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">cov</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">rv</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">pos</span><span class="p">))</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="o">-</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>
                      <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;$</span><span class="se">\\</span><span class="s1">sigma_</span><span class="se">{{</span><span class="s1">x2</span><span class="se">}}</span><span class="s1">$ = </span><span class="si">{</span><span class="n">sigma_x2</span><span class="si">:</span><span class="s1">3.2f</span><span class="si">}</span><span class="se">\n</span><span class="s1">$</span><span class="se">\\</span><span class="s1">rho$ =</span>
<span class="p">{</span><span class="n">rho</span><span class="p">:</span><span class="mf">3.2</span><span class="n">f</span><span class="p">}</span><span class="s1">&#39;, alpha=0)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">f</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="s1">&#39;x_1&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;x_2&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510131210a9.webp" /></p>
<p>图3.8</p>
</center>
<p>现在知道了多元高斯分布，就可以用其估计皮尔逊相关系数。由于不知道协方差矩阵的值，所以考虑其先验分布。一种解决方案是使用 <code class="docutils literal notranslate"><span class="pre">Wishart</span> <span class="pre">分布</span></code>，它是多变量正态分布的逆协方差矩阵的共轭先验。<code class="docutils literal notranslate"><span class="pre">Wishart</span> <span class="pre">分布</span></code> 可以被认为是伽马分布的高维推广，或者也可以被认为是 <span class="math notranslate nohighlight">\(\chi^2\)</span> 分布的推广。第二种选择是使用 <a class="reference external" href="https://docs.pymc.io/notebooks/LKJ.html"><code class="docutils literal notranslate"><span class="pre">LKJ</span> <span class="pre">先验</span></code></a>，它是相关矩阵（而不是协方差矩阵）的先验。</p>
<p>此处将探索第三种选择，我们直接为 <span class="math notranslate nohighlight">\(\sigma_{\mathbb{x}_1}\)</span> 、<span class="math notranslate nohighlight">\(\sigma_{\mathbb{x}_2}\)</span> 和 <span class="math notranslate nohighlight">\(\rho\)</span> 设置先验，然后使用这些值手动构建协方差矩阵：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">pearson_model</span><span class="p">:</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;μ&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">σ_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;σ_1&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">σ_2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;σ_2&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">ρ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Uniform</span><span class="p">(</span><span class="s1">&#39;ρ&#39;</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;r2&#39;</span><span class="p">,</span> <span class="n">ρ</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">cov</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">stack</span><span class="p">(([</span><span class="n">σ_1</span><span class="o">**</span><span class="mi">2</span><span class="p">,</span> <span class="n">σ_1</span><span class="o">*</span><span class="n">σ_2</span><span class="o">*</span><span class="n">ρ</span><span class="p">],</span>
				    <span class="p">[</span><span class="n">σ_1</span><span class="o">*</span><span class="n">σ_2</span><span class="o">*</span><span class="n">ρ</span><span class="p">,</span> <span class="n">σ_2</span><span class="o">**</span><span class="mi">2</span><span class="p">]))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">MvNormal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">cov</span><span class="o">=</span><span class="n">cov</span><span class="p">,</span> 
    				<span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
    <span class="n">trace_p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>我们将省去 <span class="math notranslate nohighlight">\(R^2\)</span> 之外的所有变量：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_p</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;r2&#39;</span><span class="p">])</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051013133311.webp" /></p>
<p>图3.9</p>
</center>
<p>可以看到，<span class="math notranslate nohighlight">\(r^2\)</span> 值的分布与上一个示例中使用 <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 的 <code class="docutils literal notranslate"><span class="pre">r2_core</span></code> 函数获得的值基本相同。通过汇总可以更简单地进行比较。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_p</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;r2&#39;</span><span class="p">])</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510131427f7.webp" /></p>
</center>
</div>
</div>
<div class="section" id="id11">
<h2>3.2 稳健的线性回归<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>在许多情况下，假设数据服从高斯分布是合理的。假设数据符合高斯特性，并不是说数据真的就符合高斯分布，而是说我们认为高斯分布对于问题而言是一个合理的近似。有时候高斯假设并不成立，例如出现异常值的时候，如上一章所述，利用 <span class="math notranslate nohighlight">\(t\)</span> 分布可以有效地解决该问题，从而得到更稳健的推断。类似思想同样可以用于线性回归问题。</p>
<p>为了验证 <span class="math notranslate nohighlight">\(t\)</span> 分布确实能增加线性回归的稳健性，这里我们使用一个简单的数据集：<code class="docutils literal notranslate"><span class="pre">Anscombe</span> <span class="pre">quartet</span></code> 中的第 3 组数据。如果你不知道该数据集，可以在 <a class="reference external" href="https://en.wikipedia.org/wiki/Anscombe%27s_quartet">维基百科</a> 上查看。我们可以用 <code class="docutils literal notranslate"><span class="pre">Pandas</span></code> 读取数据，并对数据做中心化处理，以便采样器更容易收敛。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ans</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/anscombe.csv&#39;</span><span class="p">)</span>
<span class="n">x_3</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="n">ans</span><span class="o">.</span><span class="n">group</span> <span class="o">==</span> <span class="s1">&#39;III&#39;</span><span class="p">][</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_3</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="n">ans</span><span class="o">.</span><span class="n">group</span> <span class="o">==</span> <span class="s1">&#39;III&#39;</span><span class="p">][</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">x_3</span> <span class="o">=</span> <span class="n">x_3</span> <span class="o">-</span> <span class="n">x_3</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>然后来看看该数据集长什么样：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">beta_c</span><span class="p">,</span> <span class="n">alpha_c</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">linregress</span><span class="p">(</span><span class="n">x_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">)[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_3</span><span class="p">,</span> <span class="p">(</span><span class="n">alpha_c</span> <span class="o">+</span> <span class="n">beta_c</span> <span class="o">*</span> <span class="n">x_3</span><span class="p">),</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span>
           <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;y =</span><span class="si">{</span><span class="n">alpha_c</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">beta_c</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> * x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">,</span> <span class="s1">&#39;C0o&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">y_3</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">rug</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510133712af.webp" /></p>
<p>图3.10</p>
</center>
<p>现在用 <span class="math notranslate nohighlight">\(t\)</span> 分布替换模型中的高斯分布，该改变需要引入正态参数 <span class="math notranslate nohighlight">\(\nu\)</span> ，有关该参数的含义，可参照前一章的内容。</p>
<p>在下面的模型中，我们使用平移的指数分布来避免接近零的 <span class="math notranslate nohighlight">\(\nu\)</span> 值。非平移的指数分布将接近于零的值放在了过高权重上。这对于没有到中等异常值的数据来说很好，但是对于有极端异常值的数据（比如 <code class="docutils literal notranslate"><span class="pre">Anscombe</span></code> 的第三个数据集）最好避免这么低的值。当然，默认设置是很好的起点，但没必要拘泥于它。其他常见的先验还包括 <span class="math notranslate nohighlight">\(\Gamma(2，0.1)\)</span> 或 <span class="math notranslate nohighlight">\(\Gamma(\mu=20，SD=15)\)</span> 。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_t</span><span class="p">:</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">y_3</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">ν_</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s1">&#39;ν_&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">29</span><span class="p">)</span>
    <span class="n">ν</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;ν&#39;</span><span class="p">,</span> <span class="n">ν_</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">α</span> <span class="o">+</span> <span class="n">β</span> <span class="o">*</span> <span class="n">x_3</span><span class="p">,</span>
                         <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">ν</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_3</span><span class="p">)</span>
    <span class="n">trace_t</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
<p>在下图中，我们可以看到根据 <code class="docutils literal notranslate"><span class="pre">model_t</span></code> 的稳健拟合和根据 <code class="docutils literal notranslate"><span class="pre">SciPy</span></code> 线性回归的非稳健拟合（采用最小二乘回归）。作为额外练习，你可以尝试添加使用 <code class="docutils literal notranslate"><span class="pre">model_g</span></code> 获得的最佳直线：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">beta_c</span><span class="p">,</span> <span class="n">alpha_c</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">linregress</span><span class="p">(</span><span class="n">x_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">)[:</span><span class="mi">2</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_3</span><span class="p">,</span> <span class="p">(</span><span class="n">alpha_c</span> <span class="o">+</span> <span class="n">beta_c</span> <span class="o">*</span> <span class="n">x_3</span><span class="p">),</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;non-robust&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_3</span><span class="p">,</span> <span class="n">y_3</span><span class="p">,</span> <span class="s1">&#39;C0o&#39;</span><span class="p">)</span>
<span class="n">alpha_m</span> <span class="o">=</span> <span class="n">trace_t</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">beta_m</span> <span class="o">=</span> <span class="n">trace_t</span><span class="p">[</span><span class="s1">&#39;β&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_3</span><span class="p">,</span> <span class="n">alpha_m</span> <span class="o">+</span> <span class="n">beta_m</span> <span class="o">*</span> <span class="n">x_3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;robust&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510134431b0.webp" /></p>
<p>图3.11</p>
</center>
<p>当非稳健拟合试图折衷并包含所有点时，稳健贝叶斯模型 <code class="docutils literal notranslate"><span class="pre">model_t</span></code> 会自动丢弃一个点，并拟合一条恰好通过所有剩余点的直线。我知道这是一个非常奇特的数据集，但该信息仍然适用于更真实、更复杂的数据集。由于学生 <span class="math notranslate nohighlight">\(t\)</span> 分布的尾部较重，所以能给远离大量数据的点以较小的重要性。</p>
<p>在继续前，花一点时间来考虑参数的值（我省略了中间参数 <span class="math notranslate nohighlight">\(\nu\)</span> ，因为它不是直接感兴趣的）：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_t</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="n">varnames</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051013485414.webp" /></p>
</center>
<p>如您所见，<span class="math notranslate nohighlight">\(\alpha\)</span> 、 <span class="math notranslate nohighlight">\(\beta\)</span> 和 <span class="math notranslate nohighlight">\(\epsilon\)</span> 的值定义非常狭窄，其中对于基本上为 0 的 <span class="math notranslate nohighlight">\(\epsilon\)</span> 值更是如此。这是完全合理的，因为我们正在将一条线拟合到一组完全对齐的点上（如果我们忽略离群点）。</p>
<p>让我们运行一个后验预测检查，以探索模型捕获数据的能力。可以让 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 为完成从后验采样的艰苦工作：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_t</span><span class="p">,</span> <span class="n">samples</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_t</span><span class="p">,</span><span class="n">random_seed</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">data_ppc</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">from_PyMC3</span><span class="p">(</span><span class="n">trace</span><span class="o">=</span><span class="n">trace_t</span><span class="p">,</span> <span class="n">posterior_predictive</span><span class="o">=</span><span class="n">ppc</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">plot_ppc</span><span class="p">(</span><span class="n">data_ppc</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">),</span> <span class="n">mean</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051013515477.webp" /></p>
<p>图3.12</p>
</center>
<p>对于大部分数据，我们得到了非常好的匹配。需要注意的是：此模型的预测值不仅大于整体值，而是位于其两边。就目前目的而言，此模型是运行良好，不需要进一步更改。不过对于某些问题，我们可能希望避免这种情况，而是希望预测值大于整体值，此时应返回并更改模型。</p>
</div>
<div class="section" id="id12">
<h2>3.3 分层线性回归<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h2>
<p>上一章学习了分层模型的基础知识，现在可以将其应用到线性回归，在组这一层和高于组的层次建模并估计。与之前相同，这里引入 <code class="docutils literal notranslate"><span class="pre">超先验（hyperpriors）</span></code>。</p>
<p>首先创建 8 个相关的数据组，其中有一组仅包含一个数据点。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">8</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">N</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="mi">7</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">314</span><span class="p">)</span>
<span class="n">alpha_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
<span class="n">beta_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
<span class="n">eps_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
<span class="n">y_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
<span class="n">x_m</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">))</span>
<span class="n">y_m</span> <span class="o">=</span> <span class="n">alpha_real</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">beta_real</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_m</span> <span class="o">+</span> <span class="n">eps_real</span>
<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
<span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">N</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_m</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">k</span><span class="p">],</span> <span class="n">y_m</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">k</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;x_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">15</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">17</span><span class="p">)</span>
    <span class="n">j</span> <span class="o">+=</span> <span class="n">N</span>
    <span class="n">k</span> <span class="o">+=</span> <span class="n">N</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051014002833.webp" /></p>
<p>图3.13</p>
</center>
<p>在将数据提供给模型前先对其做中心化处理：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_centered</span> <span class="o">=</span> <span class="n">x_m</span> <span class="o">-</span> <span class="n">x_m</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
<p>首先，和前面做法一样，先用非多层的模型拟合，唯一区别是需要增加部分代码将 <span class="math notranslate nohighlight">\(α\)</span> 转换到原始尺度。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">unpooled_model</span><span class="p">:</span>
    <span class="n">α_tmp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α_tmp&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">ν</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s1">&#39;ν&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">α_tmp</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_centered</span><span class="p">,</span>
                         <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">ν</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_m</span><span class="p">)</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">α_tmp</span> <span class="o">-</span> <span class="n">β</span> <span class="o">*</span> <span class="n">x_m</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">trace_up</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
<p>从结果中可以看到，除了其中一组 <span class="math notranslate nohighlight">\(α\)</span> 和 <span class="math notranslate nohighlight">\(β\)</span> 参数，大多数情况下结果都很正常。根据它们的迹来看，似乎这一组参数一直在自由移动而没有收敛。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_up</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="s1">&#39;β&#39;</span><span class="p">],</span> <span class="n">combined</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="image-20210510140311255" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_202105101641358f.webp" /></p>
<p>图3.14</p>
</center>
<p>显然，用一条唯一的直线去拟合一个点是不合适的，我们至少需要两个点，或者参数 <span class="math notranslate nohighlight">\(α\)</span> 和 <span class="math notranslate nohighlight">\(β\)</span> 是有界的，除非我们能提供一些额外的信息（比如加入先验），给 <span class="math notranslate nohighlight">\(α\)</span> 加入一个很强的先验能够得到一组明确定义的先验，即使我们的数据中只有一个点。另一种方式是通过构建多层模型往模型中加入信息，这主要是因为多层模型中组与组之间的信息能够共享，这一点对于已经有不同分组的稀疏数据非常有用。这里我们用到的例子中将数据稀疏性推向了极致（其中一组只有一个数据），目的是将问题描述得更清楚一些。</p>
<p>现在我们实现一个与前面线性回归模型相同的多层模型，不过这次用的是超先验，你可以从下面的  <code class="docutils literal notranslate"><span class="pre">Kruschke</span> <span class="pre">图</span></code> 中看到。</p>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510141026c7.webp" /></p>
<p>图3.15</p>
</center>
<p>用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 代码实现的模型与之前模型的主要区别如下：</p>
<ul class="simple">
<li><p>增加了超先验。</p></li>
<li><p>增加了几行代码将参数转换到中心化前的尺度。记住这并非强制的，我们完全可以将参数保留在转换后的尺度上，只是对结果进行解释的时候需要小心。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">hierarchical_model</span><span class="p">:</span>
    <span class="c1"># hyper-priors</span>
    <span class="n">α_μ_tmp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α_μ_tmp&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">α_σ_tmp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;α_σ_tmp&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">β_μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β_μ&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β_σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;β_σ&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="c1"># priors</span>
    <span class="n">α_tmp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α_tmp&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">α_μ_tmp</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">α_σ_tmp</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">β_μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">β_σ</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">M</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">ν</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Exponential</span><span class="p">(</span><span class="s1">&#39;ν&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="mi">30</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">StudentT</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span>
                         <span class="n">mu</span><span class="o">=</span><span class="n">α_tmp</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">+</span> <span class="n">β</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">*</span> <span class="n">x_centered</span><span class="p">,</span>
                         <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">nu</span><span class="o">=</span><span class="n">ν</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_m</span><span class="p">)</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">α_tmp</span> <span class="o">-</span> <span class="n">β</span> <span class="o">*</span> <span class="n">x_m</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">α_μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;α_μ&#39;</span><span class="p">,</span> <span class="n">α_μ_tmp</span> <span class="o">-</span> <span class="n">β_μ</span> <span class="o">*</span> <span class="n">x_m</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">α_σ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;α_sd&#39;</span><span class="p">,</span> <span class="n">α_σ_tmp</span> <span class="o">-</span> <span class="n">β_μ</span> <span class="o">*</span> <span class="n">x_m</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
    <span class="n">trace_hm</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>为了比较 <code class="docutils literal notranslate"><span class="pre">unpooled_model</span></code> 和 <code class="docutils literal notranslate"><span class="pre">hierarhical_model</span></code> 的结果，我们将再做一个森林图：</p>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510141313e8.webp" /></p>
<p>图3.16</p>
</center>
<p>使用 <code class="docutils literal notranslate"><span class="pre">az.plot_forest()</span></code> 比较模型的一个好方法是在同一绘图中同时显示两个模型 ( <code class="docutils literal notranslate"><span class="pre">unpooled_model</span></code>、<code class="docutils literal notranslate"><span class="pre">hierarhical_model</span></code>) 的参数。要做到这一点，您只需传递一个迹的列表。为了更好地理解模型捕获的有关数据的内容，为八组中的每一组绘制拟合线：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ravel</span><span class="p">(</span><span class="n">ax</span><span class="p">)</span>
<span class="n">j</span><span class="p">,</span> <span class="n">k</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">N</span>
<span class="n">x_range</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_m</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x_m</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_m</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">k</span><span class="p">],</span> <span class="n">y_m</span><span class="p">[</span><span class="n">j</span><span class="p">:</span><span class="n">k</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;x_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y_</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">17</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">alpha_m</span> <span class="o">=</span> <span class="n">trace_hm</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">][:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">beta_m</span> <span class="o">=</span> <span class="n">trace_hm</span><span class="p">[</span><span class="s1">&#39;β&#39;</span><span class="p">][:,</span> <span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_range</span><span class="p">,</span> <span class="n">alpha_m</span> <span class="o">+</span> <span class="n">beta_m</span> <span class="o">*</span> <span class="n">x_range</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span>
               <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;y = </span><span class="si">{</span><span class="n">alpha_m</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> + </span><span class="si">{</span><span class="n">beta_m</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1"> * x&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="n">x_m</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">x_m</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="n">y_m</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">y_m</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">j</span> <span class="o">+=</span> <span class="n">N</span>
    <span class="n">k</span> <span class="o">+=</span> <span class="n">N</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051014144830.webp" /></p>
<p>图3.17</p>
</center>
<p>使用分层模型，我们能够将一条线拟合于单个数据点，如上图所示。乍一看，这可能听起来很奇怪，甚至有点可疑，但这只是分层模型结构的结果。每一条线都由其他组的线提供通报，因此并不是真正地将一条线拟合为一个点。取而代之的是，将一条线调整为由其他组中的点通报的单个点。</p>
<div class="section" id="id13">
<h3>3.3.1 相关性与因果性<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>现在假设已经知道了当地的太阳辐射量，我们想要预测冬天家里的燃气费。在该问题中，太阳的辐射量是自变量 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> ，燃气费是因变量 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 。当然，我们完全可以将问题反过来，根据燃气费推算太阳辐射量，一旦我们建立了一种线性关系（或者其他什么关系），我们就可以根据 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 得出 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> ，或者反过来这么做。我们称一个变量为自变量是因为它的值不是从模型中预测出来的，而是作为模型的输入，相应的因变量作为模型的输出。当我们说一个变量依赖于另一个变量的时候，这其中的依赖关系是由模型决定的。</p>
<p>我们建立的并不是变量之间的因果关系，即并不是说 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 导致了 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 。永远要记住这句话：相关性并不意味着因果关系。就该话题多说一点，我们可能根据家中的燃气费预测出太阳辐射量或者反过来根据太阳辐射量预测出家中的燃气费。但是我们显然并不能通过调节燃气阀门来控制太阳的辐射量。不过，太阳辐射量的高低是与燃气费的高低相关的。</p>
<p>因此，需要强调一点，我们构建的统计模型是一回事，变量之间的物理机制又是另外一回事。想要将相关性解释为因果关系，我们还需要给问题的描述增加一些可信的物理机制，仅仅相关性还不够。有一个网页，描述了一些有相关性但并没有因果关系的变量：<a class="reference external" href="http://www.tylervigen.com/spurious-correlations">http://www.tylervigen.com/spurious-correlations</a></p>
<p>那么，相关性是否在确定因果关系时一点用都没有呢？不是。事实上如果能够进行一些精心设计的实验，那么相关性是能够用于支撑因果关系的。举例来说：</p>
<p>我们知道全球变暖与大气中二氧化碳的含量是高度相关的。仅仅根据该观测，我们无法得出结论是温度升高导致的二氧化碳含量上升，还是二氧化碳含量的上升导致了温度升高。更进一步，可能存在某种我们没考虑到的第 3 个变量，导致二氧化碳含量和温度同时上升了。不过，我们可以设计一个实验，将玻璃箱子中充满不同比例的二氧化碳含量，其中一个是正常空气中的含量（约 0.04%），其余箱子中二氧化碳含量逐渐增加，然后让这些箱子接受一定时间的阳光照射（比如 3 个小时）。如果这么做之后能证实二氧化碳含量较高的箱子温度也更高，那么就能得出二氧化碳的含量导致温室效应的结论。同样的实验，我们可以反过来让相同二氧化碳含量的箱子接受不同温度的照射，然后可以看到二氧化碳含量并不会上升（至少空气中的二氧化碳含量不会上升）。事实上，更高的温度会导致二氧化碳含量的上升，因为海洋中蕴含着二氧化碳，随着温度上升，水中蕴含的二氧化碳含量会降低。简言之，全球正在变暖而我们没有采取足够措施解决该问题。</p>
<p>该例子中还有一点需要说明下，尽管太阳辐射量与燃气费相关，根据太阳辐射量可能预测出燃气费，不过如果考虑到一些其他变量，这中间的关系就变得复杂了。我们一起来看一下，更高的太阳辐射量意味着更多的能量传递到家里，部分能量被反射掉了，还有部分转化成了热能，其中部分热量被房子吸收，还有部分散失到环境中了。热能消失多少取决于许多因素，比如室外的温度、风力等。此外，我们还知道，燃气费也受到很多因素影响，比如国际上石油和燃气的价格，燃气公司的成本/利润（及其贪婪程度），国家对燃气公司的管控等。而我们在尝试用两个变量和一条直线对所有这一切建模。因此，充分考虑问题的上下文是有必要的，而且有利于得出更合理的解释，降低得出荒谬结论的风险，从而得到更好的预测，此外还有可能为我们提供线索改进模型。</p>
<p>总而言之，生活是杂乱无章的，问题通常不容易理解，上下文总是很重要的。统计模型可以帮助我们实现更好的解释，降低做出无稽之谈的风险，并获得更好的预测，但这些都不是自动的。</p>
</div>
</div>
<div class="section" id="id14">
<h2>3.4 多项式回归<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h2>
<p>接下来，我们将学习如何用线性回归拟合曲线。使用线性回归模型去拟合曲线的一种做法是构建如下多项式：</p>
<div class="math notranslate nohighlight">
\[
\mu=\beta_{0} x^{0}+\beta_{1} x^{1} \cdots+\beta_{m} x^{m} \tag{式3.12}
\]</div>
<p>可以看到多项式中其实包含了一元线性回归模型，只需将上式中 <span class="math notranslate nohighlight">\( n&gt;1\)</span> 的系数 <span class="math notranslate nohighlight">\(β_n\)</span> 设为 0 即可得到下式：</p>
<div class="math notranslate nohighlight">
\[
\mu=\beta_{0}+\beta_{1} x^{1} \tag{式3.13}
\]</div>
<p>多项式回归仍然是线性回归，这里线性的意思是指模型中的参数是线性组合的，而不是指变量是线性变化的。现在先从一个简单的多项式（抛物线）开始构建多项式回归模型：</p>
<div class="math notranslate nohighlight">
\[
\mu=\beta_{0}+\beta_{1} x^{1}+\beta_{2} x^{2}  \tag{式3.14}
\]</div>
<p>其中第 3 项控制的是曲率。我们选用 <code class="docutils literal notranslate"><span class="pre">Anscombe</span> <span class="pre">quartet</span></code> 的第 2 组数据集</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_2</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="n">ans</span><span class="o">.</span><span class="n">group</span> <span class="o">==</span> <span class="s1">&#39;II&#39;</span><span class="p">][</span><span class="s1">&#39;x&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_2</span> <span class="o">=</span> <span class="n">ans</span><span class="p">[</span><span class="n">ans</span><span class="o">.</span><span class="n">group</span> <span class="o">==</span> <span class="s1">&#39;II&#39;</span><span class="p">][</span><span class="s1">&#39;y&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">x_2</span> <span class="o">-</span> <span class="n">x_2</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_2</span><span class="p">,</span> <span class="n">y_2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051014252547.webp" /></p>
<p>图3.18</p>
</center>
<p>现在建立 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 模型如下：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_poly</span><span class="p">:</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">y_2</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">β1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β1&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">β2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β2&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">mu</span> <span class="o">=</span> <span class="n">α</span> <span class="o">+</span> <span class="n">β1</span> <span class="o">*</span> <span class="n">x_2</span> <span class="o">+</span> <span class="n">β2</span> <span class="o">*</span> <span class="n">x_2</span><span class="o">**</span><span class="mi">2</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_2</span><span class="p">)</span>
    <span class="n">trace_poly</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
<p>同样，我们将省略一些检查和汇总，只绘制结果，这将是一条很好的曲线，几乎没有错误地拟合数据。考虑到数据集的极简主义性质：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">6</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">y_p</span> <span class="o">=</span> <span class="n">trace_poly</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">+</span> <span class="n">trace_poly</span><span class="p">[</span><span class="s1">&#39;β1&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> \
    <span class="n">x_p</span> <span class="o">+</span> <span class="n">trace_poly</span><span class="p">[</span><span class="s1">&#39;β2&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="n">x_p</span><span class="o">**</span><span class="mi">2</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_2</span><span class="p">,</span> <span class="n">y_2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_p</span><span class="p">,</span> <span class="n">y_p</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="id15">
<h3>3.3.1 解释多项式回归的系数<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>多项式回归的问题之一在于参数的可解释性。如果我们想知道 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 相对于 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 的变化量，不能只看 <span class="math notranslate nohighlight">\(β_1\)</span> ，因为 <span class="math notranslate nohighlight">\(β_2\)</span> 和更高项的系数对其也有影响。因此，系数 <span class="math notranslate nohighlight">\(β\)</span> 的值不再表示斜率。前面的例子中 <span class="math notranslate nohighlight">\(β_1\)</span> 是正数，因而曲线是以一个大于 0 的斜率开始的，但由于 <span class="math notranslate nohighlight">\(β_2\)</span> 是负数，因而随后曲线的斜率开始下降。这看起来就好像有两股力量，一个使直线向上，另一个使直线向下，二者相互作用的结果取决于 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> ，当 <span class="math notranslate nohighlight">\(x_i&lt;11\)</span> 时（在原始尺度上，如果是在中心尺度上则为 2）， <span class="math notranslate nohighlight">\(β_1\)</span> 起决定作用，而当 <span class="math notranslate nohighlight">\(x_i&gt;11\)</span> 时， <span class="math notranslate nohighlight">\(β_2\)</span> 起决定作用。</p>
<p>如何解释参数不仅是个数学问题，因为我们可以通过仔细检查和理解模型来解决该问题。不过许多情况下，参数并不能根据我们的领域知识转换成有意义的量，我们无法将其与细胞的新陈代谢速率或者恒星释放的能量或者房间里的卧室数联系起来。它们只是些没有物理意义的参数。这样一个模型或许对于预测很有用，不过对于理解数据在底层是如何生成的并没有多大帮助。而且在实际中，超过 2 阶或者 3 阶的多项式模型并没有多大用途，我们更倾向于使用一些其他模型，这部分将在后面的章节中讨论。</p>
</div>
<div class="section" id="id16">
<h3>3.3.2 多项式回归— 终极模型？<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>我们知道，直线可以看作是当 <span class="math notranslate nohighlight">\(β_2\)</span> 为 0 时抛物线的子模型，还可以看作是 <span class="math notranslate nohighlight">\(β_2\)</span> 和 <span class="math notranslate nohighlight">\(β_3\)</span> 都为 0 时的 3 次方模型的子模型。显然，抛物线模型也可以看作是当 <span class="math notranslate nohighlight">\(β_3\)</span> 为 0 时 3 次方模型的子模型。….</p>
<p>这似乎意味着存在一种算法可以使用线性回归模型去拟合任意复杂的模型。我们先构建一个无限高阶的多项式，然后将其中的大部分参数置零，直到得到对数据的完美拟合。为验证该想法，可以从简单例子开始，用刚刚构建的 2 次模型去拟合 <code class="docutils literal notranslate"><span class="pre">Anscombe</span> <span class="pre">quartet</span></code> 的第 3 个数据集。</p>
<p>完成练习之后，你会发现用 2 次模型去拟合直线是可能的。该例子看起来似乎验证了可以使用无限高阶多项式去拟合数据这一思想，但是通常用多项式去拟合数据并不是最好的办法。为什么呢？</p>
<p>因为该方法并不关心数据是怎么来的，从原理上讲，我们始终能够找到一个多项式去完美拟合数据。如果一个模型完美拟合了当前的数据，那么通常对于没有观测到的数据会表现得很糟糕，原因是现实中的任意数据集都同时包含一些噪声和一些感兴趣的模式。一个过于复杂的模型会同时拟合噪声，从而使得预测结果变差，这称作过拟合，一个在统计学和机器学习中常见的现象。越复杂的模型越容易导致过拟合，因而分析数据时，需要确保模型没有产生过拟合，我们将在第 6 章模型比较中详细讨论。</p>
<p>除了过拟合问题，我们通常倾向于更容易理解的模型。从物理意义上讲，线性模型的参数要比 3 次模型的参数更容易解释，即便 3 次模型对数据拟合得更好。</p>
</div>
</div>
<div class="section" id="id17">
<h2>3.5 多元线性回归<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h2>
<p>前面的所有例子中，我们讨论的都是一个因变量和一个自变量的情况，不过在许多例子中，我们的模型可能包含多个自变量。例如：</p>
<ul class="simple">
<li><p>红酒的口感（因变量）与酒的酸度、比重、酒精含量、甜度以及硫酸盐含量（自变量）的关系；</p></li>
<li><p>学生的平均成绩（因变量）与家庭收入、家到学校的距离、母亲的受教育程度（自变量）的关系。</p></li>
</ul>
<p>这种情况下，因变量可以这样建模：</p>
<div class="math notranslate nohighlight">
\[
\mu=\alpha+\beta_{1} x_{1}+\beta_{2} x_{2} \cdots+\beta_{m} x_{m} \tag{式3.15}
\]</div>
<p>注意该式与多项式回归的式子不一样，现在有了多个变量而不再是一个变量的多次方。用线性代数方法可以表示为更简洁的形式：</p>
<div class="math notranslate nohighlight">
\[
\mu=\alpha+X \beta \tag{式3.16}
\]</div>
<p>其中， <span class="math notranslate nohighlight">\(β\)</span> 是一个长度为 <span class="math notranslate nohighlight">\(m\)</span> 的系数向量，也就是说，自变量的个数为 <span class="math notranslate nohighlight">\(m\)</span> 。变量 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 是一个维度为 <span class="math notranslate nohighlight">\(n×m\)</span> 的矩阵，其中， <span class="math notranslate nohighlight">\(n\)</span> 表示观测的样本数， <span class="math notranslate nohighlight">\(m\)</span> 表示自变量个数。有关线性代数，可参阅相关书籍。本书中，您需要知道的只是使用了一种更短、更方便的方式来编写我们的模型：</p>
<div class="math notranslate nohighlight">
\[
X \beta=\sum_{i=1}^{n} \beta_{i} x_{i}=\beta_{1} x_{1}+\beta_{2} x_{2} \cdots+\beta_{m} x_{m} \tag{式3.17}
\]</div>
<p>在一元线性回归模型中，我们希望找到一条直线来解释数据，而在多元线性回归模型中，我们希望找到的是一个维度为 <span class="math notranslate nohighlight">\(m\)</span> 的超平面。因此，多元线性回归模型本质上与一元线性回归模型是一样的，唯一区别是：现在 <span class="math notranslate nohighlight">\(β\)</span> 是一个向量而 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 是一个矩阵。</p>
<p>现在我们定义如下数据：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">314</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">alpha_real</span> <span class="o">=</span> <span class="mf">2.5</span>
<span class="n">beta_real</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
<span class="n">eps_real</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">,</span> <span class="n">N</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span>
<span class="mf">1.5</span><span class="p">])])</span><span class="o">.</span><span class="n">T</span>
<span class="n">X_mean</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">X_centered</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X_mean</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">alpha_real</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">beta_real</span><span class="p">)</span> <span class="o">+</span> <span class="n">eps_real</span>
</pre></div>
</div>
<p>然后定义一个函数去画 3 个散点图，前两个表示的是自变量与因变量的关系，最后一个表示的是两个自变量之间的关系。这只是个普通的绘图函数，本章后面将会反复用到。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scatter_plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">x_i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">T</span><span class="p">):</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_i</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;x_</span><span class="si">{</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">x</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;x_</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;x_</span><span class="si">{</span><span class="n">idx</span><span class="o">+</span><span class="mi">1</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>用前面刚刚定义的 scatter_plot 可以将我们的合成数据可视化地表示出来。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scatter_plot</span><span class="p">(</span><span class="n">X_centered</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510144455e0.webp" /></p>
<p>图3.19</p>
</center>
<p>现在用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 针对多变量线性回归问题定义出一个合适的模型，代码部分与单变量线性回归的代码基本一致，唯一的区别是：</p>
<ul class="simple">
<li><p>参数 <span class="math notranslate nohighlight">\(\beta\)</span> 是高斯分布，<code class="docutils literal notranslate"><span class="pre">shape</span></code> 大小为 2，每个独立参数都有一个斜率；</p></li>
<li><p>使用 <code class="docutils literal notranslate"><span class="pre">pm.math.dot()</span></code> 来定义变量 <span class="math notranslate nohighlight">\(\mu\)</span> ，也就是前面提到的线性代数中的点乘（或者矩阵相乘）。</p></li>
</ul>
<p>如果你对 <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> 比较熟悉，那么你应该知道 <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> 包含一个点乘函数，而且 <code class="docutils literal notranslate"><span class="pre">Python3.5</span></code>（以及 <code class="docutils literal notranslate"><span class="pre">NumPy1.10</span></code>）之后增加了一个新的操作符&#64;。不过这里我们使用的是 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 中的点乘函数（其实是 <code class="docutils literal notranslate"><span class="pre">Theano</span></code> 中矩阵相乘的一个别名），因为变量 <span class="math notranslate nohighlight">\(\beta\)</span> 在这里是一个 <code class="docutils literal notranslate"><span class="pre">Theano</span></code> 中的张量而不是 <code class="docutils literal notranslate"><span class="pre">NumPy</span></code> 数组。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_mlr</span><span class="p">:</span>
    <span class="n">α_tmp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α_tmp&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">α_tmp</span> <span class="o">+</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_centered</span><span class="p">,</span> <span class="n">β</span><span class="p">)</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">α_tmp</span> <span class="o">-</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_mean</span><span class="p">,</span> <span class="n">β</span><span class="p">))</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">trace_mlr</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
<p>现在看一下推断出来的参数的总结，这样分析结果会更容易一些。我们的模型表现如何呢？</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">varnames</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="s1">&#39;β&#39;</span><span class="p">,</span> <span class="s1">&#39;ϵ&#39;</span><span class="p">]</span>
<span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_mlr</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="n">varnames</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510144845f1.webp" /></p>
</center>
<p>可以看到，模型能够重现正确的值（对比生成数据用的值）。</p>
<p>接下来，我们将重点关注在分析多变量线性回归模型中需要注意的点，特别是对斜率的解释。这里需要特别提醒的是：每个参数只有在整体考虑了其他参数的情况下才有意义。</p>
<div class="section" id="id18">
<h3>3.5.1 混淆变量和冗余变量<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>设想这样一种情况：有一个变量 <span class="math notranslate nohighlight">\(z\)</span> 与预测变量 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 相关，同时还与另一个被预测的变量 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 相关。假设 <span class="math notranslate nohighlight">\(z\)</span> 对 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 和 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 都有影响，例如， <span class="math notranslate nohighlight">\(z\)</span> 可以是工业革命（一个相当复杂的变量）， <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 是海盗的数量， <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 是二氧化碳浓度。如果在分析中将 <span class="math notranslate nohighlight">\(z\)</span> 去掉，我们会得出结论： <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 与 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 之间有着完美的线性相关性，我们甚至可以通过 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 预测 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 。不过，如果我们关注的重点是如何缓解全球变暖问题，那么可能完全没搞清到底发生了什么以及其内在机制是什么。</p>
<p>前面已经讨论了相关性并不意味着因果关系，原因是我们可能在分析过程中忽略了变量 <span class="math notranslate nohighlight">\(z\)</span> 。这种情况下， <span class="math notranslate nohighlight">\(z\)</span> 称作混淆变量，或者是混淆因素。问题是在很多情况下， <span class="math notranslate nohighlight">\(z\)</span> 很容易被忽略。也许是因为我们压根没有测量 <span class="math notranslate nohighlight">\(z\)</span> ，或者是因为没有包含在传给我们的数据集中，又或者是因为我们压根没想到它可能与我们的问题有联系。</p>
<p>没有考虑到混淆变量可能会导致我们的分析得出奇怪的相关性，在解释数据和做预测（有时候我们不关心内在的机制）的时候，这可能是个问题。理解底层的机制有利于将学到的东西迁移到新的场景中，相反，盲目的预测很难迁移。例如，帆布鞋产量可以作为一个国家经济实力的易测指标，不过对于生产链不同或者文化背景不同的国家而言，这可能是个糟糕的指标。</p>
<p>我们将使用合成数据来探索混淆变量的问题。下面的代码中模拟了一个混淆变量 <span class="math notranslate nohighlight">\(x_1\)</span>，注意该变量是如何影响 <span class="math notranslate nohighlight">\(x_2\)</span>和 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 的。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="c1">#x_2 = x_1 + np.random.normal(size=N, scale=0.01)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

</pre></div>
</div>
<p>根据生成数据的方式，可以看出这些变量已经是中心化了。因此，不需要再进一步对数据进行中心化处理来加速推断过程了。事实上该例子中的数据已经是标准化的了。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scatter_plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510145615c1.webp" /></p>
<p>图3.20</p>
</center>
<p>现在建立三个相关模型，第一个是 <code class="docutils literal notranslate"><span class="pre">m_x1x2</span></code>，这是一个有两个自变量的线性回归模型，<span class="math notranslate nohighlight">\(x_1\)</span> 和 <span class="math notranslate nohighlight">\(x_2\)</span> （在变量 X 中堆叠在一起）。第二个模型 <code class="docutils literal notranslate"><span class="pre">m_x1</span></code> 是 <span class="math notranslate nohighlight">\(x_1\)</span> 的一个简单线性回归，第三个模型 <code class="docutils literal notranslate"><span class="pre">m_x2</span></code> 是 <span class="math notranslate nohighlight">\(x_2\)</span> 的一个简单线性回归：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m_x1x2</span><span class="p">:</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β1&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β2&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">α</span> <span class="o">+</span> <span class="n">β1</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">β2</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">trace_x1x2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m_x1</span><span class="p">:</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β1&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">α</span> <span class="o">+</span> <span class="n">β1</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">trace_x1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">m_x2</span><span class="p">:</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β2&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">α</span> <span class="o">+</span> <span class="n">β2</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">trace_x2</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
<p>看看这些模型的参数 <span class="math notranslate nohighlight">\(\beta\)</span> 。使用森林图，可以在一个图中对它们进行比较：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_forest</span><span class="p">([</span><span class="n">trace_x1x2</span><span class="p">,</span> <span class="n">trace_x1</span><span class="p">,</span> <span class="n">trace_x2</span><span class="p">],</span>
               <span class="n">model_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;m_x1x2&#39;</span><span class="p">,</span> <span class="s1">&#39;m_x1&#39;</span><span class="p">,</span> <span class="s1">&#39;m_x2&#39;</span><span class="p">],</span>
               <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;β1&#39;</span><span class="p">,</span> <span class="s1">&#39;β2&#39;</span><span class="p">],</span>
               <span class="n">combined</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;cycle&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_202105101500367e.webp" /></p>
<p>图3.21</p>
</center>
<p>正如我们所看到的，对于模型 <code class="docutils literal notranslate"><span class="pre">m_x1x2</span></code>，其 <span class="math notranslate nohighlight">\(\beta_2\)</span> 值大约为零，这表明对于解释 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 变量， <span class="math notranslate nohighlight">\(x_2\)</span> 的贡献几乎为零。这很有趣，因为我们已经知道（查看合成数据）真正重要的变量是 <span class="math notranslate nohighlight">\(x_1\)</span> 。还要注意：模型 <code class="docutils literal notranslate"><span class="pre">m_x2</span></code> 的 <span class="math notranslate nohighlight">\(\beta_2\)</span> 值约为 0.55。这比模型 <code class="docutils literal notranslate"><span class="pre">m_x1x2</span></code> 的大。当我们考虑到 <span class="math notranslate nohighlight">\(x_1\)</span> 时，<span class="math notranslate nohighlight">\(x_2\)</span> 预测的能力就会降低；也就是说，当给定 <span class="math notranslate nohighlight">\(x_1\)</span> 时， <span class="math notranslate nohighlight">\(x_2\)</span> 给出的信息是冗余的。</p>
</div>
<div class="section" id="id19">
<h3>3.5.2 多重共线性或相关性太高<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<p>前面的例子中，我们看到了多元线性回归模型中的冗余变量问题，同时还了解了混淆变量的重要性。接下来我们沿着前面例子继续深入学习当两个变量高度相关时会发生什么。为了研究该问题及其对推断的影响，我们使用和前面一样的合成数据和模型，不过通过减小根据 <span class="math notranslate nohighlight">\(x_1\)</span> 生成 <span class="math notranslate nohighlight">\(x_2\)</span> 时的随机噪声，增加了  <span class="math notranslate nohighlight">\(x_1\)</span> 和  <span class="math notranslate nohighlight">\(x_2\)</span> 之间的相关性：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x_1</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
<p>数据生成代码中的这种变化实际上等同于将零加到 <span class="math notranslate nohighlight">\(x_1\)</span> ，因此，在所有实际目的中，这两个变量都是相等的。然后，您可以尝试改变尺度值并使用不太极端的值，但现在我们想让事情简单些。生成新数据后，检查散点图的外观：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">scatter_plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051015131081.webp" /></p>
<p>图3.22</p>
</center>
<p>您应该看到上图中，<span class="math notranslate nohighlight">\(x_1\)</span> 和 <span class="math notranslate nohighlight">\(x_2\)</span> 的散点图实际上是一条斜率约为 1 的直线。然后，我们运行多元线性回归：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_red</span><span class="p">:</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfCauchy</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">α</span> <span class="o">+</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">β</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>
    <span class="n">trace_red</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
<p>然后，我们用森林图检查参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的结果：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_red</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;β&#39;</span><span class="p">],</span> <span class="n">combined</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510151553cf.webp" /></p>
<p>图3.23</p>
</center>
<p><span class="math notranslate nohighlight">\(\beta\)</span> 参数的 <code class="docutils literal notranslate"><span class="pre">HPD</span> <span class="pre">区间</span></code> 相当广，与先验几乎一样。我们可以从系数的散点图中得到一些线索：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">trace_red</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;β&#39;</span><span class="p">])</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510151732e5.webp" /></p>
<p>图3.24</p>
</center>
<p>哇！参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的边际后验是一条非常窄的对角线。当一个系数上升时，另一个系数必然下降。两者实际上是相互关联的。这只是模型和数据的结果。根据我们的模型，平均值 <span class="math notranslate nohighlight">\(\mu\)</span> 是：</p>
<div class="math notranslate nohighlight">
\[
\mu=\alpha+\beta_{1} x_{1}+\beta_{2} x_{2} \tag{式3.19}
\]</div>
<p>假设  <span class="math notranslate nohighlight">\(x_1\)</span> 和  <span class="math notranslate nohighlight">\(x_2\)</span> 不只是近似相同，而是完全一样的，那么可以将模型改写成如下形式：</p>
<div class="math notranslate nohighlight">
\[
\mu=\alpha+（\beta_{1} +\beta_{2}） x \tag{式3.20}
\]</div>
<p>可以看到，对 <span class="math notranslate nohighlight">\(μ\)</span> 有影响的是  <span class="math notranslate nohighlight">\(\beta_1\)</span> 与 <span class="math notranslate nohighlight">\(\beta_2\)</span> 的和而不是二者单独的值，因而模型是不确定的（或者说，数据并不能决定  <span class="math notranslate nohighlight">\(\beta_1\)</span> 和 <span class="math notranslate nohighlight">\(\beta_2\)</span> 的值）。在我们的例子中，<span class="math notranslate nohighlight">\(\beta\)</span> 并不能在区间 [-∞,∞] 内自由移动，原因有两个：其一，两个变量几乎是相同的，不过并非完全一样；其二，更重要的是 <span class="math notranslate nohighlight">\(\beta\)</span> 系数的可能取值受到先验的限制。</p>
<p>该例子中有几点需要注意。</p>
<ul class="simple">
<li><p>首先，后验只是根据模型和数据得出的逻辑上的结果，因而得出一个分布很广的 <span class="math notranslate nohighlight">\(\beta\)</span> 分布并没有错，事实就是这样子；</p></li>
<li><p>第 2 点是，我们可以依据该模型做预测，可以尝试做一些后验预测检查，该模型预测得到的值与数据分布是一致的，也就是说模型对数据拟合得很好；</p></li>
<li><p>第 3 点是，对于理解问题而言这可能不是一个很好的模型，更好的做法是从模型中去掉一个参数，这样模型的预测能力与以前一样，但是更容易解释。</p></li>
</ul>
<p>在任何真实的数据集中，相关性在某种程度上是普遍存在的。那么两个或多个变量之间相关性多高时会导致问题呢？事实上并没有确切的数值。我们可以在运行贝叶斯模型之前，构建一个相关性矩阵，对其中相关性较高（比如说高于 0.9）的变量进行检查。不过，这种做法的问题是：根据相关性矩阵观察到的成对变量之间的相关性并不太重要，重要的是在某个具体模型中变量的相关性。前面已经看到了，不同变量在单独情况下的表现与在模型中放一起的表现是不同的。在多元回归模型中，两个或多个变量之间的相关性可能会受到其他变量的影响，从而使得他们之间的相关性降低或者升高。通常，建议在迭代式构建模型的同时加入一些诊断（比如检查自相关性和后验），这有利于发现问题和理解模型与数据。</p>
<p>如果发现了高度相关的变量应该怎么做呢？</p>
<ul class="simple">
<li><p>如果相关性非常高，我们可以从分析中将其中一个变量去掉。如果两个变量的信息都差不多，具体去掉哪个并不重要，可以视方便程度（比如去掉最不常见的或者最难解释或测量的变量）。</p></li>
<li><p>另外一种可行的做法是构建一个新变量对冗余变量求均值。更高级的做法是使用一些降维算法，如<code class="docutils literal notranslate"><span class="pre">主成分分析法（PCA）</span></code>。不过 <code class="docutils literal notranslate"><span class="pre">PCA</span></code> 的问题是得到的结果变量是原始变量的线性组合，通常会对结果的可解释性造成模糊。</p></li>
<li><p>还有一种办法是给变量可能的取值设置一个较强的先验。在第 6 章 模型比较中会简要讨论如何选择这类先验（通常称作正则先验）。</p></li>
</ul>
</div>
<div class="section" id="id20">
<h3>3.5.3  掩藏有效变量<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<p>有一种情况与前面见过的类似，其中某个变量与预测变量正相关而另外一个与预测变量负相关。这里先构建一些玩具数据来说明。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">N</span> <span class="o">=</span> <span class="mi">126</span>
<span class="n">r</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">N</span><span class="p">)</span>
<span class="n">x_2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">x_1</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">r</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">x_1</span> <span class="o">-</span> <span class="n">x_2</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">x_1</span><span class="p">,</span> <span class="n">x_2</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="n">scatter_plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<center>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510155007ff.webp" style="zoom: 50%;" />
<p>图3.25</p>
</center>
<p>正如我们之前所做的那样，我们将构建三个相关的模型。第一个是 <code class="docutils literal notranslate"><span class="pre">m_x1x2</span></code>，它是一个有两个自变量的线性回归模型，并且（在变量 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 中堆叠在一起）。第二个模型 <code class="docutils literal notranslate"><span class="pre">m_x1</span></code> 是对的简单线性回归，第三个模型 <code class="docutils literal notranslate"><span class="pre">m_x2</span></code> 是对的简单线性回归。从这些模型采样后，使用森林图查看参数，以便在单个图中进行比较：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_forest</span><span class="p">([</span><span class="n">trace_x1x2</span><span class="p">,</span> <span class="n">trace_x1</span><span class="p">,</span> <span class="n">trace_x2</span><span class="p">],</span>
               <span class="n">model_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;m_x1x2&#39;</span><span class="p">,</span> <span class="s1">&#39;m_x1&#39;</span><span class="p">,</span> <span class="s1">&#39;m_x2&#39;</span><span class="p">],</span>
               <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;β1&#39;</span><span class="p">,</span> <span class="s1">&#39;β2&#39;</span><span class="p">],</span>
               <span class="n">combined</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;cycle&#39;</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510155120c2.webp" /></p>
<p>图3.26</p>
</center>
<p>从后验可以看出，<span class="math notranslate nohighlight">\(\beta\)</span> 的值接近 1 和-1。也就是说，  <span class="math notranslate nohighlight">\(x_1\)</span> 与 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 正相关，而  <span class="math notranslate nohighlight">\(x_2\)</span> 与 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 负相关。现在重新分析，不过（也许你已经猜到了）这一次我们对每个单独的变量进行分析。</p>
<p>对于单个的变量，可以看到 <span class="math notranslate nohighlight">\(β\)</span> 接近 0，也就是说每个单独变量 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 都不足以预测 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 。相反，如果我们将 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 组合在一起后就可以预测 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 。</p>
<p>当  <span class="math notranslate nohighlight">\(x_1\)</span> 增加时  <span class="math notranslate nohighlight">\(x_2\)</span> 也增加，而当  <span class="math notranslate nohighlight">\(x_2\)</span> 增加时 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 也降低，因此如果单独看变量  <span class="math notranslate nohighlight">\(x_1\)</span> 而忽略  <span class="math notranslate nohighlight">\(x_2\)</span> 的话，我们会看到当  <span class="math notranslate nohighlight">\(x_1\)</span> 增加的时候， <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 几乎不增加，而当  <span class="math notranslate nohighlight">\(x_2\)</span> 增加时， <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 几乎不降低。因变量之间具有相关性，每个因变量都有反作用，因而忽略其中任何一个都会造成对变量影响力的低估。</p>
</div>
<div class="section" id="id21">
<h3>3.5.4 增加变量间的交互作用<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<p>目前为止，所有多元回归模型的定义中，在其他预测变量固定的条件下，  <span class="math notranslate nohighlight">\(x_1\)</span> 的变化都会（隐式地）带来 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 的稳定变化。不过这显然并非一定的，有可能改变  <span class="math notranslate nohighlight">\(x_2\)</span> 之后，原来 <span class="math notranslate nohighlight">\(\mathbb{y}\)</span> 与  <span class="math notranslate nohighlight">\(x_1\)</span> 之间的关系发生了改变。一个经典的例子是药物之间的相互作用，例如，在没有使用药物 B（或者药物 B 的剂量较低）时，增加药物 A 的剂量有正向影响，而当增加药物 B 的剂量时，药物 A 反而有负向（甚至致命的）影响。</p>
<p>目前见过的所有例子中，因变量对于预测变量的作用都是叠加的。我们做的只是增加变量（每个变量乘以一个系数）。如果我们希望捕捉到变量的效果，就像前面的药物例子一样，我们需要给模型增加一项非叠加的量，比如，变量间的乘积：</p>
<div class="math notranslate nohighlight">
\[
\mu=\alpha+\beta_{1} x_{1}+\beta_{2} x_{2}+\beta_{3} x_{1} x_{2} \tag{式3.21}
\]</div>
<p>注意这里系数 <span class="math notranslate nohighlight">\( β_3\)</span>  乘的是  <span class="math notranslate nohighlight">\(x_1\)</span> 和  <span class="math notranslate nohighlight">\(x_2\)</span> 的乘积，该非叠加项只是一个用来说明统计学中的变量间相互作用的例子，因为它衡量了变量（在前面的例子中是药物）之间的相关性。对相关性建模的表达式有很多种，相乘只是其中一个比较常用的。</p>
<p>解释有交互作用的线性模型并不像解释没有交互作用的线性模型那么容易。让我们重写表达式 3.21：</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*} \tag{式3.22} 
\mu &amp;=\alpha+\underbrace{\left(\beta_{1}+\beta_{3} x_{2}\right)}_{\text {slope of } x_{1}} x_{1}+\beta_{2} x_{2} \\
\mu &amp;=\alpha+\beta_{1} x_{1}+\underbrace{\left(\beta_{2}+\beta_{3} x_{1}\right)}_{\text {slope of } x_{2}} x_{2} 
\end{align*}\]</div>
<p>这向我们展示了以下内容：</p>
<ul class="simple">
<li><p>相互作用项可以理解为线性模型。因此，平均值 <span class="math notranslate nohighlight">\(\mu\)</span> 的表达式是一个线性模型，其中包含另一个线性模型！</p></li>
<li><p>相互作用是对称的，我们可以把它看作 <span class="math notranslate nohighlight">\(x_1\)</span> 的斜率是 <span class="math notranslate nohighlight">\(x_2\)</span> 的函数，同时也可以看作 <span class="math notranslate nohighlight">\(x_2\)</span>的斜率是 <span class="math notranslate nohighlight">\(x_1\)</span> 的函数。</p></li>
<li><p>在多元线性回归模型中，如果没有变量之间的乘积，我们得到的是一个超平面，也就是说，一个平坦的超曲面，加入乘积之后，该超曲面会变得弯曲。</p></li>
<li><p>系数 <span class="math notranslate nohighlight">\(\beta_1\)</span> 仅描述了 <span class="math notranslate nohighlight">\(x_2=0\)</span> 时 <span class="math notranslate nohighlight">\(x_1\)</span> 的影响。这是正确的，因为对于值 <span class="math notranslate nohighlight">\(\beta_3x_2=0\)</span> ， <span class="math notranslate nohighlight">\(x_1\)</span> 斜率减小到 <span class="math notranslate nohighlight">\(\beta_1x_1\)</span> 。类似的，同样的推理也可以应用于 <span class="math notranslate nohighlight">\(\beta_2\)</span> 。</p></li>
</ul>
</div>
</div>
<div class="section" id="id22">
<h2>3.6 变量方差的线性回归模型<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h2>
<p>我们一直使用线性模型来建模概率分布的平均值 <span class="math notranslate nohighlight">\(\mu\)</span> ，在上一节中，甚至使用它来建模了交互作用。但是，当同方差假设不能成立（或没意义）时，也可以用线性模型来建模方差（或标准差）。此情况下可能希望将方差视为自变量的（线性）函数。</p>
<p>世界卫生组织和世界各地其他卫生机构收集新生儿和学步儿童的数据，并设计了标准的生长图表。这些图表是儿童工具包的重要组成部分，也是衡量人口总体幸福感的指标（ <a class="reference external" href="http://www.Who.int/ChildGrowth/en/">链接</a>）。这些数据的一个例子是新生女孩的身高随年龄（以月为单位）的变化：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;../data/babies.csv&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">.</span><span class="n">plot</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="s1">&#39;Month&#39;</span><span class="p">,</span> <span class="s1">&#39;Lenght&#39;</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510153936bd.webp" />
图 3.27</p>
</center>
<p>为对此数据建模，我们引入三个新元素，与之前模型的区别在于：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> 现在是 <span class="math notranslate nohighlight">\(\mathbb{x}\)</span> 的线性函数。为此，我们添加了两个新参数，<span class="math notranslate nohighlight">\(\gamma\)</span> 和 <span class="math notranslate nohighlight">\(\delta\)</span> 。这是和 <span class="math notranslate nohighlight">\(\alpha、\beta\)</span>的直接类比。</p></li>
<li><p>均值 <span class="math notranslate nohighlight">\(\mu\)</span> 的线性模型是 <span class="math notranslate nohighlight">\(\sqrt{x}\)</span> 的函数，将线性模型拟合到曲线上，仅用于说明案例，无物理解释。</p></li>
<li><p>定义了一个共享变量 <code class="docutils literal notranslate"><span class="pre">x_shared</span></code> 。在模型拟合之后，用它来更改变量（在本例中为 Month) 的值，而无需重新调整模型。</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_vv</span><span class="p">:</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">γ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;γ&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">δ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;δ&#39;</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">x_shared</span> <span class="o">=</span> <span class="n">shared</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Month</span><span class="o">.</span><span class="n">values</span> <span class="o">*</span> <span class="mf">1.</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;μ&#39;</span><span class="p">,</span> <span class="n">α</span> <span class="o">+</span> <span class="n">β</span> <span class="o">*</span> <span class="n">x_shared</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="n">γ</span> <span class="o">+</span> <span class="n">δ</span> <span class="o">*</span> <span class="n">x_shared</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="o">.</span><span class="n">Lenght</span><span class="p">)</span>
    <span class="n">trace_vv</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tune</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
<p>下图显示了我们模型的结果。均值用一条黑色曲线表示，两个半透明的橙色带分别表示 1 个和 2 个标准差：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Month</span><span class="p">,</span> <span class="n">data</span><span class="o">.</span><span class="n">Lenght</span><span class="p">,</span> <span class="s1">&#39;C0.&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">μ_m</span> <span class="o">=</span> <span class="n">trace_vv</span><span class="p">[</span><span class="s1">&#39;μ&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ϵ_m</span> <span class="o">=</span> <span class="n">trace_vv</span><span class="p">[</span><span class="s1">&#39;ϵ&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Month</span><span class="p">,</span> <span class="n">μ_m</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Month</span><span class="p">,</span> <span class="n">μ_m</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">ϵ_m</span><span class="p">,</span> <span class="n">μ_m</span> <span class="o">-</span>
                 <span class="mi">1</span> <span class="o">*</span> <span class="n">ϵ_m</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">data</span><span class="o">.</span><span class="n">Month</span><span class="p">,</span> <span class="n">μ_m</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">ϵ_m</span><span class="p">,</span> <span class="n">μ_m</span> <span class="o">-</span>
                 <span class="mi">2</span> <span class="o">*</span> <span class="n">ϵ_m</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;C1&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051015443536.webp" /></p>
<p>图3.28</p>
</center>
<p>在写此书时，我女儿只有两周大，所以我想知道她的身高与刚绘制的生长图表相比如何。回答此问题的方法是询问半个月大婴儿身高的分布模型。使用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> ，可以通过 <code class="docutils literal notranslate"><span class="pre">sample_posterior_predictive</span></code> 函数回答这个问题。</p>
<p>该函数的输出基于观测数据和所估计参数分布（包括不确定性）的样本。唯一问题是：根据定义，此函数返回对观测值的预测，但数据集中所有度量都是以整月报告的，没有 0.5 个月的情况（我关心的值）。要获得非观测值的预测，更简单的方法是定义一个共享变量（作为模型的一部分），然后在对后验预测分布采样之前更新共享变量的值：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">x_shared</span><span class="o">.</span><span class="n">set_value</span><span class="p">([</span><span class="mf">0.5</span><span class="p">])</span>
<span class="n">ppc</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_vv</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_vv</span><span class="p">)</span>
<span class="n">y_ppc</span> <span class="o">=</span> <span class="n">ppc</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>现在，可以画出两周大婴儿的预期身高分布，并计算额外数据。例如，给定孩子的身高，她所处的百分位数。在下面的代码块和图中查看该示例。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ref</span> <span class="o">=</span> <span class="mf">47.5</span>
<span class="n">density</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">u</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">_fast_kde</span><span class="p">(</span><span class="n">y_ppc</span><span class="p">)</span>
<span class="n">x_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">u</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_</span><span class="p">,</span> <span class="n">density</span><span class="p">)</span>
<span class="n">percentile</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_ppc</span> <span class="o">&lt;=</span> <span class="n">ref</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_ppc</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x_</span><span class="p">[</span><span class="n">x_</span> <span class="o">&lt;</span> <span class="n">ref</span><span class="p">],</span> <span class="n">density</span><span class="p">[</span><span class="n">x_</span> <span class="o">&lt;</span> <span class="n">ref</span><span class="p">],</span>
                 <span class="n">label</span><span class="o">=</span><span class="s1">&#39;percentile = </span><span class="si">{:2d}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">percentile</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;length&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_202105101546025a.webp" /></p>
<p>图3.29</p>
</center>
</div>
<div class="section" id="id23">
<h2>3.7 总结<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h2>
<p>一元线性回归是一种可以用来预测和/或解释一个变量与另一个变量的模型。从机器学习语言表述，这是一个有监督学习的案例。从概率角度来看，线性回归模型是高斯模型的扩展，其中均值不是直接估计的，而是作为自变量和一些附加参数的线性函数来计算的。虽然高斯分布是因变量最常见的选择，但我们也可以选择其他分布。一种在处理潜在异常值时特别有用的替代方法就是学生 <span class="math notranslate nohighlight">\(t\)</span> 分布。在下一章中，我们将探索其他替代方案。</p>
<p>本章还讨论了皮尔逊相关系数，这是两个变量间线性相关性的最常见度量，我们学习了如何使用多元高斯分布从数据和后验预测样本中计算出它的贝叶斯版本。</p>
<p>扩展线性回归模型的一种有用方法是建立其的分层版本，进而具备收缩的优势。使用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 可以非常简单地实现这一点。</p>
<p>本章还简要讨论了不将相关性解释为因果关系的重要性，至少在缺乏物理模型的情况下是这样。</p>
<p>听起来可能令人惊讶，但我们可以使用线性模型来拟合曲线。本章用两个例子来说明这一点：多项式回归和取自变量的平方根。</p>
<p>简单线性回归的另一个扩展是用多元线性回归来处理多个自变量。为避免解释这些类型的模型时出现错误和问题，有必要采取一些预防措施，我们使用了几个示例来说明这一点。</p>
<p>使用线性模型的其他方法是对交互作用进行建模，还有一种方法是处理因变量的变方差。</p>
</div>
<div class="section" id="id24">
<h2>3.8 练习<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h2>
<center>
<p><img alt="image-20210510161310448" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510161347fd.webp" /></p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051016133031.webp" /></p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510161419e6.webp" /></p>
</center></div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="chapter02-ProgrammingProbabilistically.html" title="previous page">第 2 章 概率编程</a>
    <a class='right-next' id="next-link" href="chapter04-GeneralizedLinearRegression.html" title="next page">第 4 章 广义线性回归模型与分类任务</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Osvaldo Martin<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>