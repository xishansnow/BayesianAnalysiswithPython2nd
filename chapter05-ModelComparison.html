
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>第 5 章 模型比较 &#8212; Python贝叶斯分析(中文)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.e8f53015daec13862f6db5e763c41738.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="第 6 章 混合模型" href="chapter06-MixtureModels.html" />
    <link rel="prev" title="第 4 章 广义线性回归模型" href="chapter04-GeneralizedLinearRegression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Python贝叶斯分析(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   封面
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  书籍正文
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter01-ThinkingProbabilistically.html">
   第 1 章 概率思维
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter02-ProgrammingProbabilistically.html">
   第 2 章 概率编程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter03-ModellingwithLinearRegression.html">
   第 3 章 线性回归模型的贝叶斯视角
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter04-GeneralizedLinearRegression.html">
   第 4 章 广义线性回归模型
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   第 5 章 模型比较
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter06-MixtureModels.html">
   第 6 章 混合模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter07-GaussianProcesses.html">
   第 7 章 高斯过程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter08-InterefenceEngine.html">
   第 8 章 推断引擎
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter09-WheretoGoNext.html">
   第 9 章 下一步去哪儿？
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  其他阅读材料
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Ref-chapter08-01-MCMCforDummies.html">
   MCMC 采样的傻瓜书
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ref-chapter08-02-VIforDummies.html">
   变分推断傻瓜书
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ref-chapter08-03-BayesianDeepLearning.html">
   贝叶斯深度学习
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        <a class="dropdown-buttons"
            href="_sources/chapter05-ModelComparison.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download notebook file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/chapter05-ModelComparison.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/xishansnow/BayesianAnalysiswithPython2nd"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/xishansnow/BayesianAnalysiswithPython2nd/issues/new?title=Issue%20on%20page%20%2Fchapter05-ModelComparison.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/xishansnow/BayesianAnalysiswithPython2nd/master?urlpath=tree/chapter05-ModelComparison.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        <a class="jupyterhub-button" href="https://datahub.berkeley.edu/hub/user-redirect/git-pull?repo=https://github.com/xishansnow/BayesianAnalysiswithPython2nd&urlpath=tree/BayesianAnalysiswithPython2nd/chapter05-ModelComparison.ipynb&branch=master"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch JupyterHub" data-toggle="tooltip"
                data-placement="left"><img class="jupyterhub-button-logo"
                    src="_static/images/logo_jupyterhub.svg"
                    alt="Interact on JupyterHub">JupyterHub</button></a>
        
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   5.1 后验预测性检查
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   5.2 奥卡姆剃刀 — 简约性与准确性
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     5.2.1 参数太多导致过拟合
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     5.2.2 参数太少导致欠拟合
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     5.2.3 简约性与准确性之间的平衡
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   5.3 如何对预测准确度进行度量？
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     5.3.1 交叉验证（Cross-validation）
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     5.3.2 信息准则
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#log">
       <strong>
        （1）Log 似然与离差
       </strong>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#aic">
       <strong>
        （2）AIC 信息准则
       </strong>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#waic">
       <strong>
        （3） WAIC 通用信息准则
       </strong>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#pareto">
       <strong>
        （4）Pareto 平滑重要性采样留一交叉验证
       </strong>
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#dic-bic">
       <strong>
        （5）DIC 与 BIC 准则
       </strong>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#pymc3">
   5.4 使用 PyMC3 做模型比较
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id9">
   5.5 模型平均
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     5.5.1 基于信息准则值的加权平均
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     5.5.2 基于预测分布堆叠的加权平均
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     5.5.3 其他模型平均方法
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   5.6 贝叶斯因子
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     5.6.1 一些讨论
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     5.6.2 贝叶斯因子的计算
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     5.6.3 计算贝叶斯因子时的常见问题
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id17">
     5.6.4 用序贯蒙特卡罗方法计算贝叶斯因子
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     5.6.5 贝叶斯因子与信息准则
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id19">
   5.7 其他
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id20">
     5.7.1 正则先验
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id21">
     5.7.2 深入
     <code class="docutils literal notranslate">
      <span class="pre">
       WAIC
      </span>
     </code>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id22">
     5.7.3 熵
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id23">
       （1）熵的定义
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id24">
       （2） 熵与方差
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id25">
       （3） 最大熵原理
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kl">
     5.7.4 KL 散度
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id26">
   5.8 总结
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id27">
   5.9 习题
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1>第 5 章 模型比较<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>模型应该被设计成帮助我们理解特定问题或某类相关问题的近似值，而不是真实世界的翻版，从这个意义上讲所有模型都是错误的。即使在有先验的情况下，模型也都是错误的，但每个模型的错误可能有所不同，而其中一些模型比其他模型更好地描述了给定问题。此前的章节将注意力集中在推断问题上，即如何从数据中学习参数的分布。本章将重点讨论一个互补问题：如何比较用于解释相同数据的多个模型。这是数据分析需解决的关键问题之一。</p>
<p>本章将讨论以下内容：</p>
<ul class="simple">
<li><p>后验预测性检查</p></li>
<li><p>奥卡姆剃刀—简单性和准确性</p></li>
<li><p>过拟合和欠拟合</p></li>
<li><p>信息准则</p></li>
<li><p>贝叶斯因子</p></li>
<li><p>正则化先验</p></li>
</ul>
<hr class="docutils" />
<div class="section" id="id2">
<h2>5.1 后验预测性检查<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p><code class="docutils literal notranslate"><span class="pre">第一章</span> <span class="pre">概率思维</span></code>介绍了后验预测性检查的概念，本章将用它来评估拟合出的模型对相同数据的解释程度。如前所述，所有的模型都是错误的，因此后验预测性检查的目的并非判定某个模型是否错误，而是希望通过后验预测性检查更好地把握模型的局限性，以做出适当改进。模型不会再现所有问题，但这并不是问题，因为构建模型都有特定目的，后验预测性检查则是在该目的背景下评估模型的一种方式；因此当考虑了多个模型时，可使用后验预测性检查来对它们进行比较。</p>
<p>让我们读取并绘制一个简单的数据集：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">stats</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="kn">import</span> <span class="nn">pymc3</span> <span class="k">as</span> <span class="nn">pm</span>
<span class="kn">import</span> <span class="nn">arviz</span> <span class="k">as</span> <span class="nn">az</span>

<span class="n">az</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;arviz-darkgrid&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dummy_data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;../data/dummy.csv&#39;</span><span class="p">)</span>
<span class="n">x_1</span> <span class="o">=</span> <span class="n">dummy_data</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
<span class="n">y_1</span> <span class="o">=</span> <span class="n">dummy_data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">order</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">x_1p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x_1</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">order</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">x_1s</span> <span class="o">=</span> <span class="p">(</span><span class="n">x_1p</span> <span class="o">-</span> <span class="n">x_1p</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span> <span class="o">/</span> <span class="n">x_1p</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">y_1s</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_1</span> <span class="o">-</span> <span class="n">y_1</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span> <span class="o">/</span> <span class="n">y_1</span><span class="o">.</span><span class="n">std</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_1s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_1s</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;y&#39;)
</pre></div>
</div>
<img alt="_images/chapter05-ModelComparison_2_1.png" src="_images/chapter05-ModelComparison_2_1.png" />
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510111807b3.webp" /></p>
<p>图 5.1</p>
</center>
<p>现在，用两个略有不同的模型来拟合数据，第一个是线性模型，第二个是二阶多项式模型：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_l</span><span class="p">:</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">α</span> <span class="o">+</span> <span class="n">β</span> <span class="o">*</span> <span class="n">x_1s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_1s</span><span class="p">)</span>
    
    <span class="n">trace_l</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_p</span><span class="p">:</span>
    <span class="n">α</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;α&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">β</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;β&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">order</span><span class="p">)</span>
    <span class="n">ϵ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;ϵ&#39;</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
    <span class="n">μ</span> <span class="o">=</span> <span class="n">α</span> <span class="o">+</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">β</span><span class="p">,</span> <span class="n">x_1s</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_1s</span><span class="p">)</span>
    
    <span class="n">trace_p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_2747/3228340448.py:8: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.
  trace_l = pm.sample(2000)
Auto-assigning NUTS sampler...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sequential sampling (2 chains in 1 job)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>NUTS: [ϵ, β, α]
</pre></div>
</div>
<div class="output text_html">
<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
        .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
            background: #F44336;
        }
    </style>
  <progress value='3000' class='' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [3000/3000 00:02<00:00 Sampling chain 0, 0 divergences]
</div>
</div><div class="output text_html">
<div>
    <style>
        /* Turns off some styling */
        progress {
            /* gets rid of default border in Firefox and Opera. */
            border: none;
            /* Needs to be in here for Safari polyfill so background images work as expected. */
            background-size: auto;
        }
        .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
            background: #F44336;
        }
    </style>
  <progress value='3000' class='' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>
  100.00% [3000/3000 00:01<00:00 Sampling chain 1, 0 divergences]
</div>
</div><div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Sampling 2 chains for 1_000 tune and 2_000 draw iterations (2_000 + 4_000 draws total) took 4 seconds.
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_2747/3228340448.py:17: FutureWarning: In v4.0, pm.sample will return an `arviz.InferenceData` object instead of a `MultiTrace` by default. You can pass return_inferencedata=True or return_inferencedata=False to be safe and silence this warning.
  trace_p = pm.sample(2000)
Auto-assigning NUTS sampler...
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Initializing NUTS using jitter+adapt_diag...
</pre></div>
</div>
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">KeyboardInterrupt</span><span class="g g-Whitespace">                         </span>Traceback (most recent call last)
<span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">ipykernel_2747</span><span class="o">/</span><span class="mf">3228340448.</span><span class="n">py</span> <span class="ow">in</span> <span class="o">&lt;</span><span class="n">module</span><span class="o">&gt;</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span>     <span class="n">y_pred</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;y_pred&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="n">μ</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="n">ϵ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_1s</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span> 
<span class="ne">---&gt; </span><span class="mi">17</span>     <span class="n">trace_p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/pymc3/sampling.py</span> in <span class="ni">sample</span><span class="nt">(draws, step, init, n_init, start, trace, chain_idx, chains, cores, tune, progressbar, model, random_seed, discard_tuned_samples, compute_convergence_checks, callback, jitter_max_retries, return_inferencedata, idata_kwargs, mp_ctx, pickle_backend, **kwargs)</span>
<span class="g g-Whitespace">    </span><span class="mi">494</span>             <span class="c1"># By default, try to use NUTS</span>
<span class="g g-Whitespace">    </span><span class="mi">495</span>             <span class="n">_log</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Auto-assigning NUTS sampler...&quot;</span><span class="p">)</span>
<span class="ne">--&gt; </span><span class="mi">496</span>             <span class="n">start_</span><span class="p">,</span> <span class="n">step</span> <span class="o">=</span> <span class="n">init_nuts</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">497</span>                 <span class="n">init</span><span class="o">=</span><span class="n">init</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">498</span>                 <span class="n">chains</span><span class="o">=</span><span class="n">chains</span><span class="p">,</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/pymc3/sampling.py</span> in <span class="ni">init_nuts</span><span class="nt">(init, chains, n_init, model, random_seed, progressbar, jitter_max_retries, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">2096</span>         <span class="n">potential</span> <span class="o">=</span> <span class="n">quadpotential</span><span class="o">.</span><span class="n">QuadPotentialDiagAdapt</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">ndim</span><span class="p">,</span> <span class="n">mean</span><span class="p">,</span> <span class="n">var</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2097</span>     <span class="k">elif</span> <span class="n">init</span> <span class="o">==</span> <span class="s2">&quot;jitter+adapt_diag&quot;</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">2098</span>         <span class="n">start</span> <span class="o">=</span> <span class="n">_init_jitter</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">chains</span><span class="p">,</span> <span class="n">jitter_max_retries</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2099</span>         <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">model</span><span class="o">.</span><span class="n">dict_to_array</span><span class="p">(</span><span class="n">vals</span><span class="p">)</span> <span class="k">for</span> <span class="n">vals</span> <span class="ow">in</span> <span class="n">start</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">2100</span>         <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/pymc3/sampling.py</span> in <span class="ni">_init_jitter</span><span class="nt">(model, chains, jitter_max_retries)</span>
<span class="g g-Whitespace">   </span><span class="mi">1989</span>             <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">jitter_max_retries</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1990</span>                 <span class="k">try</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1991</span>                     <span class="n">check_start_vals</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1992</span>                 <span class="k">except</span> <span class="n">SamplingError</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1993</span>                     <span class="k">pass</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/pymc3/util.py</span> in <span class="ni">check_start_vals</span><span class="nt">(start, model)</span>
<span class="g g-Whitespace">    </span><span class="mi">232</span>             <span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">233</span> 
<span class="ne">--&gt; </span><span class="mi">234</span>         <span class="n">initial_eval</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">check_test_point</span><span class="p">(</span><span class="n">test_point</span><span class="o">=</span><span class="n">elem</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">235</span> 
<span class="g g-Whitespace">    </span><span class="mi">236</span>         <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isfinite</span><span class="p">(</span><span class="n">initial_eval</span><span class="p">)):</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/pymc3/model.py</span> in <span class="ni">check_test_point</span><span class="nt">(self, test_point, round_vals)</span>
<span class="g g-Whitespace">   </span><span class="mi">1382</span> 
<span class="g g-Whitespace">   </span><span class="mi">1383</span>         <span class="k">return</span> <span class="n">Series</span><span class="p">(</span>
<span class="ne">-&gt; </span><span class="mi">1384</span>             <span class="p">{</span><span class="n">RV</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">RV</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">test_point</span><span class="p">),</span> <span class="n">round_vals</span><span class="p">)</span> <span class="k">for</span> <span class="n">RV</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">basic_RVs</span><span class="p">},</span>
<span class="g g-Whitespace">   </span><span class="mi">1385</span>             <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Log-probability of test_point&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1386</span>         <span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/pymc3/model.py</span> in <span class="ni">&lt;dictcomp&gt;</span><span class="nt">(.0)</span>
<span class="g g-Whitespace">   </span><span class="mi">1382</span> 
<span class="g g-Whitespace">   </span><span class="mi">1383</span>         <span class="k">return</span> <span class="n">Series</span><span class="p">(</span>
<span class="ne">-&gt; </span><span class="mi">1384</span>             <span class="p">{</span><span class="n">RV</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="n">RV</span><span class="o">.</span><span class="n">logp</span><span class="p">(</span><span class="n">test_point</span><span class="p">),</span> <span class="n">round_vals</span><span class="p">)</span> <span class="k">for</span> <span class="n">RV</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">basic_RVs</span><span class="p">},</span>
<span class="g g-Whitespace">   </span><span class="mi">1385</span>             <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Log-probability of test_point&quot;</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1386</span>         <span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/pymc3/model.py</span> in <span class="ni">logp</span><span class="nt">(self)</span>
<span class="g g-Whitespace">    </span><span class="mi">415</span>     <span class="k">def</span> <span class="nf">logp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="g g-Whitespace">    </span><span class="mi">416</span>         <span class="sd">&quot;&quot;&quot;Compiled log probability density function&quot;&quot;&quot;</span>
<span class="ne">--&gt; </span><span class="mi">417</span>         <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">fn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">logpt</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">418</span> 
<span class="g g-Whitespace">    </span><span class="mi">419</span>     <span class="nd">@property</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/pymc3/model.py</span> in <span class="ni">fn</span><span class="nt">(self, outs, mode, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1276</span>         <span class="n">Compiled</span> <span class="n">Theano</span> <span class="n">function</span>
<span class="g g-Whitespace">   </span><span class="mi">1277</span>         <span class="sd">&quot;&quot;&quot;</span>
<span class="ne">-&gt; </span><span class="mi">1278</span><span class="sd">         return LoosePointFunc(self.makefn(outs, mode, *args, **kwargs), self)</span>
<span class="g g-Whitespace">   </span><span class="mi">1279</span><span class="sd"> </span>
<span class="g g-Whitespace">   </span><span class="mi">1280</span><span class="sd">     def fastfn(self, outs, mode=None, *args, **kwargs):</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/pymc3/model.py</span> in <span class="ni">makefn</span><span class="nt">(self, outs, mode, *args, **kwargs)</span>
<span class="g g-Whitespace">   </span><span class="mi">1252</span><span class="sd">         &quot;&quot;&quot;</span>
<span class="g g-Whitespace">   </span><span class="mi">1253</span>         <span class="k">with</span> <span class="bp">self</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1254</span>             <span class="k">return</span> <span class="n">theano</span><span class="o">.</span><span class="n">function</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1255</span>                 <span class="bp">self</span><span class="o">.</span><span class="n">vars</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1256</span>                 <span class="n">outs</span><span class="p">,</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/compile/function/__init__.py</span> in <span class="ni">function</span><span class="nt">(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)</span>
<span class="g g-Whitespace">    </span><span class="mi">335</span>         <span class="c1"># note: pfunc will also call orig_function -- orig_function is</span>
<span class="g g-Whitespace">    </span><span class="mi">336</span>         <span class="c1">#      a choke point that all compilation must pass through</span>
<span class="ne">--&gt; </span><span class="mi">337</span>         <span class="n">fn</span> <span class="o">=</span> <span class="n">pfunc</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">338</span>             <span class="n">params</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">339</span>             <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/compile/function/pfunc.py</span> in <span class="ni">pfunc</span><span class="nt">(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)</span>
<span class="g g-Whitespace">    </span><span class="mi">522</span>         <span class="n">inputs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">si</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">523</span> 
<span class="ne">--&gt; </span><span class="mi">524</span>     <span class="k">return</span> <span class="n">orig_function</span><span class="p">(</span>
<span class="g g-Whitespace">    </span><span class="mi">525</span>         <span class="n">inputs</span><span class="p">,</span>
<span class="g g-Whitespace">    </span><span class="mi">526</span>         <span class="n">cloned_outputs</span><span class="p">,</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/compile/function/types.py</span> in <span class="ni">orig_function</span><span class="nt">(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)</span>
<span class="g g-Whitespace">   </span><span class="mi">1968</span>     <span class="k">try</span><span class="p">:</span>
<span class="g g-Whitespace">   </span><span class="mi">1969</span>         <span class="n">Maker</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">mode</span><span class="p">,</span> <span class="s2">&quot;function_maker&quot;</span><span class="p">,</span> <span class="n">FunctionMaker</span><span class="p">)</span>
<span class="ne">-&gt; </span><span class="mi">1970</span>         <span class="n">m</span> <span class="o">=</span> <span class="n">Maker</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">1971</span>             <span class="n">inputs</span><span class="p">,</span>
<span class="g g-Whitespace">   </span><span class="mi">1972</span>             <span class="n">outputs</span><span class="p">,</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/compile/function/types.py</span> in <span class="ni">__init__</span><span class="nt">(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys, name)</span>
<span class="g g-Whitespace">   </span><span class="mi">1613</span>                         <span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1614</span>                     <span class="k">else</span><span class="p">:</span>
<span class="ne">-&gt; </span><span class="mi">1615</span>                         <span class="n">optimizer_profile</span> <span class="o">=</span> <span class="n">optimizer</span><span class="p">(</span><span class="n">fgraph</span><span class="p">)</span>
<span class="g g-Whitespace">   </span><span class="mi">1616</span> 
<span class="g g-Whitespace">   </span><span class="mi">1617</span>                     <span class="n">end_optimizer</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/graph/opt.py</span> in <span class="ni">__call__</span><span class="nt">(self, fgraph)</span>
<span class="g g-Whitespace">     </span><span class="mi">90</span> 
<span class="g g-Whitespace">     </span><span class="mi">91</span>         <span class="sd">&quot;&quot;&quot;</span>
<span class="ne">---&gt; </span><span class="mi">92</span><span class="sd">         return self.optimize(fgraph)</span>
<span class="g g-Whitespace">     </span><span class="mi">93</span><span class="sd"> </span>
<span class="g g-Whitespace">     </span><span class="mi">94</span><span class="sd">     def add_requirements(self, fgraph):</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/graph/opt.py</span> in <span class="ni">optimize</span><span class="nt">(self, fgraph, *args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">81</span><span class="sd">         &quot;&quot;&quot;</span>
<span class="g g-Whitespace">     </span><span class="mi">82</span>         <span class="bp">self</span><span class="o">.</span><span class="n">add_requirements</span><span class="p">(</span><span class="n">fgraph</span><span class="p">)</span>
<span class="ne">---&gt; </span><span class="mi">83</span>         <span class="n">ret</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">fgraph</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">84</span>         <span class="k">return</span> <span class="n">ret</span>
<span class="g g-Whitespace">     </span><span class="mi">85</span> 

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/graph/opt.py</span> in <span class="ni">apply</span><span class="nt">(self, fgraph)</span>
<span class="g g-Whitespace">    </span><span class="mi">243</span>                     <span class="n">nb_nodes_before</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">fgraph</span><span class="o">.</span><span class="n">apply_nodes</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">244</span>                     <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="ne">--&gt; </span><span class="mi">245</span>                     <span class="n">sub_prof</span> <span class="o">=</span> <span class="n">optimizer</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">fgraph</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">246</span>                     <span class="n">l</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">))</span>
<span class="g g-Whitespace">    </span><span class="mi">247</span>                     <span class="n">sub_profs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sub_prof</span><span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/graph/opt.py</span> in <span class="ni">optimize</span><span class="nt">(self, fgraph, *args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">81</span>         <span class="sd">&quot;&quot;&quot;</span>
<span class="g g-Whitespace">     </span><span class="mi">82</span><span class="sd">         self.add_requirements(fgraph)</span>
<span class="ne">---&gt; </span><span class="mi">83</span><span class="sd">         ret = self.apply(fgraph, *args, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">84</span><span class="sd">         return ret</span>
<span class="g g-Whitespace">     </span><span class="mi">85</span><span class="sd"> </span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/graph/opt.py</span> in <span class="ni">apply</span><span class="nt">(self, fgraph, start_from)</span>
<span class="g g-Whitespace">   </span><span class="mi">2535</span><span class="sd">                         nb = change_tracker.nb_imported</span>
<span class="g g-Whitespace">   </span><span class="mi">2536</span><span class="sd">                         t_opt = time.time()</span>
<span class="ne">-&gt; </span><span class="mi">2537</span><span class="sd">                         lopt_change = self.process_node(fgraph, node, lopt)</span>
<span class="g g-Whitespace">   </span><span class="mi">2538</span><span class="sd">                         time_opts[lopt] += time.time() - t_opt</span>
<span class="g g-Whitespace">   </span><span class="mi">2539</span><span class="sd">                         if not lopt_change:</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/graph/opt.py</span> in <span class="ni">process_node</span><span class="nt">(self, fgraph, node, lopt)</span>
<span class="g g-Whitespace">   </span><span class="mi">2015</span><span class="sd">         lopt = lopt or self.local_opt</span>
<span class="g g-Whitespace">   </span><span class="mi">2016</span><span class="sd">         try:</span>
<span class="ne">-&gt; </span><span class="mi">2017</span><span class="sd">             replacements = lopt.transform(fgraph, node)</span>
<span class="g g-Whitespace">   </span><span class="mi">2018</span><span class="sd">         except Exception as e:</span>
<span class="g g-Whitespace">   </span><span class="mi">2019</span><span class="sd">             if self.failure_callback is not None:</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/tensor/opt.py</span> in <span class="ni">transform</span><span class="nt">(self, fgraph, node)</span>
<span class="g g-Whitespace">   </span><span class="mi">5346</span><span class="sd">         # See the documentation of get_num_denum and simplify</span>
<span class="g g-Whitespace">   </span><span class="mi">5347</span><span class="sd">         orig_num, orig_denum = self.get_num_denum(node.outputs[0])</span>
<span class="ne">-&gt; </span><span class="mi">5348</span><span class="sd">         num, denum = self.simplify(list(orig_num), list(orig_denum), out.type)</span>
<span class="g g-Whitespace">   </span><span class="mi">5349</span><span class="sd"> </span>
<span class="g g-Whitespace">   </span><span class="mi">5350</span><span class="sd">         def same(x, y):</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/tensor/opt.py</span> in <span class="ni">simplify</span><span class="nt">(self, num, denum, out_type)</span>
<span class="g g-Whitespace">   </span><span class="mi">5190</span><span class="sd"> </span>
<span class="g g-Whitespace">   </span><span class="mi">5191</span><span class="sd">         &quot;&quot;&quot;</span>
<span class="ne">-&gt; </span><span class="mi">5192</span>         <span class="n">rval</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">simplify_constants</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">5193</span>             <span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">simplify_factors</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">denum</span><span class="p">),</span> <span class="n">out_type</span><span class="o">=</span><span class="n">out_type</span>
<span class="g g-Whitespace">   </span><span class="mi">5194</span>         <span class="p">)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/tensor/opt.py</span> in <span class="ni">simplify_constants</span><span class="nt">(self, orig_num, orig_denum, out_type)</span>
<span class="g g-Whitespace">   </span><span class="mi">5294</span>             <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ct</span><span class="p">)</span> <span class="o">==</span> <span class="mi">1</span>
<span class="g g-Whitespace">   </span><span class="mi">5295</span>             <span class="n">first_num_ct</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_constant</span><span class="p">(</span><span class="n">orig_num</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="ne">-&gt; </span><span class="mi">5296</span>             <span class="k">if</span> <span class="n">first_num_ct</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">ct</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">type</span><span class="o">.</span><span class="n">values_eq</span><span class="p">(</span>
<span class="g g-Whitespace">   </span><span class="mi">5297</span>                 <span class="n">ct</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">first_num_ct</span>
<span class="g g-Whitespace">   </span><span class="mi">5298</span>             <span class="p">):</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/theano/tensor/type.py</span> in <span class="ni">values_eq</span><span class="nt">(a, b, force_same_dtype)</span>
<span class="g g-Whitespace">    </span><span class="mi">327</span>             <span class="k">return</span> <span class="kc">False</span>
<span class="g g-Whitespace">    </span><span class="mi">328</span>         <span class="n">a_eq_b</span> <span class="o">=</span> <span class="n">a</span> <span class="o">==</span> <span class="n">b</span>
<span class="ne">--&gt; </span><span class="mi">329</span>         <span class="n">r</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">a_eq_b</span><span class="p">)</span>
<span class="g g-Whitespace">    </span><span class="mi">330</span>         <span class="k">if</span> <span class="n">r</span><span class="p">:</span>
<span class="g g-Whitespace">    </span><span class="mi">331</span>             <span class="k">return</span> <span class="kc">True</span>

<span class="nn">&lt;__array_function__ internals&gt;</span> in <span class="ni">all</span><span class="nt">(*args, **kwargs)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/fromnumeric.py</span> in <span class="ni">all</span><span class="nt">(a, axis, out, keepdims, where)</span>
<span class="g g-Whitespace">   </span><span class="mi">2448</span> 
<span class="g g-Whitespace">   </span><span class="mi">2449</span>     <span class="s2">&quot;&quot;&quot;</span>
<span class="ne">-&gt; </span><span class="mi">2450</span><span class="s2">     return _wrapreduction(a, np.logical_and, &#39;all&#39;, axis, None, out,</span>
<span class="g g-Whitespace">   </span><span class="mi">2451</span><span class="s2">                           keepdims=keepdims, where=where)</span>
<span class="g g-Whitespace">   </span><span class="mi">2452</span><span class="s2"> </span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/fromnumeric.py</span> in <span class="ni">_wrapreduction</span><span class="nt">(obj, ufunc, method, axis, dtype, out, **kwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">82</span><span class="s2">                 return reduction(axis=axis, dtype=dtype, out=out, **passkwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">83</span><span class="s2">             else:</span>
<span class="ne">---&gt; </span><span class="mi">84</span><span class="s2">                 return reduction(axis=axis, out=out, **passkwargs)</span>
<span class="g g-Whitespace">     </span><span class="mi">85</span><span class="s2"> </span>
<span class="g g-Whitespace">     </span><span class="mi">86</span><span class="s2">     return ufunc.reduce(obj, axis, dtype, out, **passkwargs)</span>

<span class="nn">/opt/hostedtoolcache/Python/3.8.11/x64/lib/python3.8/site-packages/numpy/core/_methods.py</span> in <span class="ni">_all</span><span class="nt">(a, axis, dtype, out, keepdims, where)</span>
<span class="g g-Whitespace">     </span><span class="mi">61</span><span class="s2">     # Parsing keyword arguments is currently fairly slow, so avoid it for now</span>
<span class="g g-Whitespace">     </span><span class="mi">62</span><span class="s2">     if where is True:</span>
<span class="ne">---&gt; </span><span class="mi">63</span><span class="s2">         return umr_all(a, axis, dtype, out, keepdims)</span>
<span class="g g-Whitespace">     </span><span class="mi">64</span><span class="s2">     return umr_all(a, axis, dtype, out, keepdims, where=where)</span>
<span class="g g-Whitespace">     </span><span class="mi">65</span><span class="s2"> </span>

<span class="ne">KeyboardInterrupt</span>: 
</pre></div>
</div>
</div>
</div>
<p>现在绘制这两个模型的平均拟合曲线：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_1s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x_1s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">α_l_post</span> <span class="o">=</span> <span class="n">trace_l</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">β_l_post</span> <span class="o">=</span> <span class="n">trace_l</span><span class="p">[</span><span class="s1">&#39;β&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y_l_post</span> <span class="o">=</span> <span class="n">α_l_post</span> <span class="o">+</span> <span class="n">β_l_post</span> <span class="o">*</span><span class="n">x_new</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_new</span><span class="p">,</span> <span class="n">y_l_post</span><span class="p">,</span> <span class="s1">&#39;C1&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;linear model&#39;</span><span class="p">)</span>

<span class="n">α_p_post</span> <span class="o">=</span> <span class="n">trace_p</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">β_p_post</span> <span class="o">=</span> <span class="n">trace_p</span><span class="p">[</span><span class="s1">&#39;β&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">x_1s</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">y_p_post</span> <span class="o">=</span> <span class="n">α_p_post</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">β_p_post</span><span class="p">,</span> <span class="n">x_1s</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_1s</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="n">idx</span><span class="p">],</span> <span class="n">y_p_post</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="s1">&#39;C2&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;model order </span><span class="si">{</span><span class="n">order</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">α_p_post</span> <span class="o">=</span> <span class="n">trace_p</span><span class="p">[</span><span class="s1">&#39;α&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">β_p_post</span> <span class="o">=</span> <span class="n">trace_p</span><span class="p">[</span><span class="s1">&#39;β&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">x_new_p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">x_new</span><span class="o">**</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">order</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">y_p_post</span> <span class="o">=</span> <span class="n">α_p_post</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">β_p_post</span><span class="p">,</span> <span class="n">x_new_p</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x_1s</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">y_1s</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510112129a4.webp" /></p>
<p>图 5.2</p>
</center>
<p>图中二阶模型似乎做得更好，但线性模型也并没有那么糟糕。此时可以使用 PyMC3 来获得两个模型的后验预测样本，并执行检查：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_l</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_l</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_l</span><span class="p">)[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">]</span>
<span class="n">y_p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive</span><span class="p">(</span><span class="n">trace_p</span><span class="p">,</span> <span class="mi">2000</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model_p</span><span class="p">)[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>正如已经看到的，后验预测性检查通常使用可视化方式来执行，如下例所示：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="n">y_1s</span><span class="p">,</span> <span class="n">y_l</span><span class="p">,</span> <span class="n">y_p</span><span class="p">]</span>
<span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="s1">&#39;linear model&#39;</span><span class="p">,</span> <span class="s1">&#39;order 2&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">err</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">d</span><span class="p">,</span> <span class="p">[</span><span class="mi">25</span><span class="p">,</span> <span class="mi">75</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="o">-</span><span class="n">i</span><span class="p">,</span> <span class="n">xerr</span><span class="o">=</span><span class="p">[[</span><span class="o">-</span><span class="n">err</span><span class="p">[</span><span class="mi">0</span><span class="p">]],</span> <span class="p">[</span><span class="n">err</span><span class="p">[</span><span class="mi">1</span><span class="p">]]],</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="o">-</span><span class="n">i</span><span class="o">+</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
</pre></div>
</div>
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210510112319ed.webp" /></p>
<p>图 5.3</p>
</center>
<p>图 5.3 显示了数据、线性模型和二次多项式模型的均值和四分位数范围。该图对各模型的后验预测样本做了平均，而且两个模型的均值都复现得很好，分位数范围也不是很差。不过在实际问题中，一些小差异可能是值得注意的。可以尝试做更多不同曲线图来探索后验预测性分布。例如，绘制均值和四分位数间相对于数据真实值的离散度。下图就是一个例子：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">iqr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
<span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">subtract</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">[</span><span class="mi">75</span><span class="p">,</span> <span class="mi">25</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="n">a</span><span class="p">))</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">func</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="n">iqr</span><span class="p">]):</span>
    <span class="n">T_obs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">y_1s</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">T_obs</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">ls</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">d_sim</span><span class="p">,</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">y_l</span><span class="p">,</span> <span class="n">y_p</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;C1&#39;</span><span class="p">,</span> <span class="s1">&#39;C2&#39;</span><span class="p">]):</span>
    <span class="n">T_sim</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">d_sim</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">p_value</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">T_sim</span> <span class="o">&gt;=</span> <span class="n">T_obs</span><span class="p">)</span>
    <span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">T_sim</span><span class="p">,</span> <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="n">c</span><span class="p">},</span>
    <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;p-value </span><span class="si">{</span><span class="n">p_value</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">func</span><span class="o">.</span><span class="vm">__name__</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021051011245094.webp" /></p>
<p>图 5.4</p>
</center>
<p>图 5.4 中黑色虚线表示根据真实数据计算的平均值和四分位数（因来自真实数据，为确切值而非分布）。图中曲线（与图 5.3 相同颜色代码）表示根据后验预测样本计算得出的均值分布（左图）或四分位数范围分布（右图）。图 5.4 还包括 <code class="docutils literal notranslate"><span class="pre">p-value</span></code> 值，该值来自于预测数据与实际数据的比较和计算。对于两个预测数据集合，我们计算了其平均值和四分位数范围，然后计算了两个统计量等于或大于根据实际数据统计量的比例。<strong>一般而言，如果真实数据和预测结果一致，预期 <code class="docutils literal notranslate"><span class="pre">p-value</span></code> 值在 0.5 左右，否则将处于有偏的后验预测性分布</strong>。</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>贝叶斯 p 值只是一种衡量后验预测性检查拟合度的数字方法。</p>
</div>
<p>贝叶斯 <code class="docutils literal notranslate"><span class="pre">p-value</span></code> 与频率派的 <code class="docutils literal notranslate"><span class="pre">p-value</span></code> 名字相似，定义基本上也相同：</p>
<div class="math notranslate nohighlight">
\[\text{Bayesian p-value}\triangleq p\left(T_{s i m} \geq T_{o b s} \mid y \right) \tag{式5.1}  \label{式5.1}\]</div>
<p>可以解释为：从模拟数据中获得与观测数据相同或更高统计量值的概率。<span class="math notranslate nohighlight">\(T\)</span> 几乎可以是数据的任意统计量。在图 5.4 中，统计量是左侧的平均值和右侧的四分位数范围。通常 <span class="math notranslate nohighlight">\(T\)</span> 应该在最初定义推断任务时就选择好。</p>
<p>这些 <code class="docutils literal notranslate"><span class="pre">p-value</span></code> 是贝叶斯的，因为其采样自后验预测性分布。需要注意的是：贝叶斯的 <code class="docutils literal notranslate"><span class="pre">p-value</span></code> 不需要频率主义的任何零假设作为条件；事实上，我们拥有基于观测数据的整个后验分布。此外，贝叶斯也没有使用类似置信度的任何预定义阈值来声明统计显著性，当然也没有执行假设检验。这里只是试图计算一个数字来评估后验预测性分布与数据集的拟合度。</p>
<p>无论使用曲线图还是数据摘要（如贝叶斯 <code class="docutils literal notranslate"><span class="pre">p-value</span></code> ），或是两者组合，后验预测性检查都是非常灵活的。该概念可让分析师思考不同方法来探索后验预测性分布，并使用合适的方法来讲述一个数据驱动的故事，包括但不限于模型比较。</p>
<p>在接下来几节中，我们探索一些其他模型比较的方法。</p>
</div>
<div class="section" id="id3">
<h2>5.2 奥卡姆剃刀 — 简约性与准确性<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>假如对同一个问题（或数据）有两个模型，二者对数据解释得同样好，应该选哪个模型呢？有一个基本准则叫做<strong>奥卡姆剃刀</strong>，如果对同一现象有两种不同假说，应选用比较简单的那一种。关于奥卡姆剃刀的论证很多，其中一种说法与波普尔的可证伪性有关，还有一种说法是从实用角度提出的，因为简单模型相比复杂模型更容易理解，此外还有一种论证是基于贝叶斯统计的。这里不深入讨论该准则的论证细节，只将该准则当做一个有用而合理的常识。</p>
<p>在比较模型时，既要考虑简约型，也需要同时考虑<strong>模型准确性</strong>，即模型对数据拟合得怎么样。之前章节已出现过一些度量准确性的指标，如： <span class="math notranslate nohighlight">\(R^2\)</span> 系数可视为线性回归中可解释方差的比例。但如果有两个模型，其中一个对数据的解释比另一个更准确，是否应该选更准确率的模型呢？</p>
<p>直觉上，似乎最好选择准确度高且简单的模型。但如果简单模型准确度最差，该怎么办？如何才能平衡这两种要素呢？为简化问题，此处引入一个例子来帮助理解如何平衡准确性与简约性。为了更形象些，该例使用一系列逐渐复杂的多项式来拟合同一个简单数据集，并且未采用贝叶斯方法，而是采用最小二乘估计来建模。当然，最小二乘估计其实可转化成带均匀先验的贝叶斯模型，因此，将其理解成贝叶斯方法也没问题。</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mf">14.</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">4.2</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">10.</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">order</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;o&#39;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">order</span><span class="p">:</span>
    <span class="n">x_n</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">x</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">coeffs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyfit</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">deg</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
    <span class="n">ffit</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">polyval</span><span class="p">(</span><span class="n">coeffs</span><span class="p">,</span> <span class="n">x_n</span><span class="p">)</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">poly1d</span><span class="p">(</span><span class="n">coeffs</span><span class="p">)</span>
    <span class="n">yhat</span> <span class="o">=</span> <span class="n">p</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ybar</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">ssreg</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">yhat</span><span class="o">-</span><span class="n">ybar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">sstot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">y</span> <span class="o">-</span> <span class="n">ybar</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">r2</span> <span class="o">=</span> <span class="n">ssreg</span> <span class="o">/</span> <span class="n">sstot</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x_n</span><span class="p">,</span> <span class="n">ffit</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;order </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">, $R^2$= </span><span class="si">{</span><span class="n">r2</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210512104113_44.webp" /></p>
<p>图 5.5</p>
</center>
<div class="section" id="id4">
<h3>5.2.1 参数太多导致过拟合<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>从图 5.5 可看出，模型复杂度增加时，对应的 <span class="math notranslate nohighlight">\(R^2\)</span> 系数在上升。当多项式为 5 阶时，模型完美拟合了数据（ <span class="math notranslate nohighlight">\(R\)</span>^2$ 趋近于 1 表示更好地拟合了数据）。前面章节中讨论过，用多项式去解决实际问题并非特别好的办法。为什么 5 阶多项式能完美拟合所有数据呢？原因是模型中参数数量与样本数量相同，都是 6。也就是说，模型只是用另一种方式对数据进行了编码，并没有从数据中学到任何内容，只是记住了全部数据而已。此外，如果使用这几种模型做预测，5 阶多项式模型对数据的预测看起来也会非常奇怪。</p>
<p>假设收集了更多数据点。例如，收集到点 [(10，9)，(7，7)] （参见图 5.6）。与 1 阶或 2 阶模型相比，5 阶模型对这些点的解释效果如何？不是很好，对吧？5 阶模型没有在数据中学习任何有趣的模式，反而只是记住了一些东西，因此它在泛化到未来数据方面做得非常糟糕：</p>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210512104753_be.webp" /></p>
<p>图 5.6</p>
</center>
<p>当一个模型与最初用于学习其参数的数据集非常吻合，但在拟合其他数据集却非常差时，被称为 <code class="docutils literal notranslate"><span class="pre">过拟合</span></code> 。过拟合是统计学和机器学习中一个普遍问题。描述过拟合问题的一个有效方法是将数据集视为由 <code class="docutils literal notranslate"><span class="pre">信号</span></code> 和 <code class="docutils literal notranslate"><span class="pre">噪声</span></code> 两部分组成。信号是想要从数据中了解到的东西，如果使用某个数据集，那是必然是因为我们认为该数据集中有一个信号，否则训练毫无意义；而噪声是数据中无用的部分，往往是测量误差、数据生成方式、数据损坏等因素带来的产物。当某个模型过于灵活，甚至能够学到噪声而隐藏信号时，该模型就会变得过拟合。避免过拟合是奥卡姆剃刀的确切理由之一。上例表明，如果仅关注模型对数据的解释能力，很容易被过拟合误导，因为理论上通过增加模型参数数量总是能够提高数据拟合的准确率。</p>
</div>
<div class="section" id="id5">
<h3>5.2.2 参数太少导致欠拟合<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>继续关注该例，不过重点放在 0 阶模型上。在 0 阶模型中，所有 <span class="math notranslate nohighlight">\(\beta\)</span> 参数都为 0，因而变量 <span class="math notranslate nohighlight">\(x\)</span> 和 <span class="math notranslate nohighlight">\(y\)</span> 间的线性关系变成了只描述结果变量的一个高斯模型。对于 0 阶模型来说，预测变量对模型不再有任何影响，模型只捕捉到结果变量的均值。换句话说，模型认为数据能够通过结果变量的均值以及一些高斯噪声来解释。我们称这种模型是欠拟合的，因为它实在太简单了，以至于不能从数据中获取有意义的模式。通常，一个参数很少的模型容易出现欠拟合。</p>
</div>
<div class="section" id="id6">
<h3>5.2.3 简约性与准确性之间的平衡<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>经常与奥卡姆剃刀准则一起提到的是爱因斯坦的一句名言“事情应该尽可能简单，但不必过于简单”。我们在建模时需要保持某种平衡。理想状态下，模型既不过拟合也不欠拟合，因此，通常需要优化或者调整模型来权衡二者。</p>
<p>机器学习领域中，通常从<code class="docutils literal notranslate"><span class="pre">方差（variance）</span></code>和 <code class="docutils literal notranslate"><span class="pre">偏差（bias）</span></code> 两个角度来讨论和权衡二者：</p>
<ul class="simple">
<li><p>高偏差（ <code class="docutils literal notranslate"><span class="pre">bias</span></code> ）是模型适应数据的能力不足导致的。高偏差可能使模型无法捕捉数据中一些关键模式，导致欠拟合。</p></li>
<li><p>高方差（ <code class="docutils literal notranslate"><span class="pre">variance</span></code>）是模型对数据中细节过于敏感导致的。高方差会使模型捕捉到数据中的噪声，导致过拟合。</p></li>
</ul>
<p>图 5.5 中，0 阶模型具有较高偏差（和较低的方差），因为它偏向于在变量 <span class="math notranslate nohighlight">\(y\)</span> 的平均值处返回一条平坦直线，而与 <span class="math notranslate nohighlight">\(x\)</span> 值无关。5 阶模型具有较高的方差（和较低的偏差），你可以采用差别很大的方式设置六个点，会发现曲线将完美拟合其中的大多数点。</p>
<p>具有高偏差的模型具有更多偏见或惯性，而具有高方差的模型是思想更开放的模型。太有偏见的问题是没有能力容纳新证据；太开放的问题是最终会相信荒唐的东西。总体来说，如果提升其中一个方面，就会导致另外一方面的下降，这也是为什么人们称之 <code class="docutils literal notranslate"><span class="pre">偏差-方差平衡</span></code>，而我们最希望得到二者平衡的模型。</p>
<p>如何做到呢？这里有一些经验方法：</p>
<p>处理 variance 较大的问题</p>
<ul class="simple">
<li><p>减少特征数量</p></li>
<li><p>使用更简单的模型</p></li>
<li><p>增大你的训练数据集</p></li>
<li><p>使用正则化</p></li>
<li><p>加入随机因子，例如采用 bagging 和 boosting 方法</p></li>
</ul>
<p>处理 bias 较大的问题</p>
<ul class="simple">
<li><p>增加特征数量</p></li>
<li><p>使用更复杂的模型</p></li>
<li><p>去掉正则化</p></li>
</ul>
</div>
</div>
<div class="section" id="id7">
<h2>5.3 如何对预测准确度进行度量？<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h2>
<p>在上例中，很容易看出 0 阶模型非常简单，而 5 阶模型相对数据过于复杂，但其他两个模型呢？要回答该问题，需要一种原则性的方式，在考虑准确性同时，兼顾考虑简约性。要做到这一点，需要引入几个新概念：</p>
<ul class="simple">
<li><p><strong>样本内精度</strong>：基于拟合模型的样本数据测量得到的模型精度。</p></li>
<li><p><strong>样本外精度</strong>：用拟合模型的样本数据以外的数据测量得到的模型精度（也称为 <code class="docutils literal notranslate"><span class="pre">预测精度</span></code>）。</p></li>
</ul>
<p>对于数据和模型的任意组合，样本内精度平均将小于样本外精度。使用样本内精确度会使我们认为拥有了一个比实际更好的模型。样本外测量比样本内测量更可取，但也存在问题。因此，合理的做法是放弃一部分样本数据，仅仅将其用于对模型的测试。但对大多数分析师来说，仅将花大成本得到的数据用作测试，似乎过于奢侈。为避免该问题，人们花了很多精力用于获得使用样本内数据来估计样本外精度的方法。其中两种方法包括：</p>
<ul class="simple">
<li><p>交叉验证：这是一种经验性策略，将数据分为多个子集，并轮流将其中一个子集作为测试集，剩余子集作为训练集进行评估。</p></li>
<li><p>信息准则：这是几个相对简单的表达式的总称，可认为这些表达式能够近似执行交叉验证后获得的结果。</p></li>
</ul>
<div class="section" id="cross-validation">
<h3>5.3.1 交叉验证（Cross-validation）<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h3>
<p>交叉验证是一种简单有效的解决方案，可在不遗漏数据的情况下评估模型。此过程的示意见下图。通常把数据分成大致相等的 <span class="math notranslate nohighlight">\(K\)</span> 份，使用其中 <span class="math notranslate nohighlight">\(K-1\)</span> 份训练模型 <span class="math notranslate nohighlight">\(A_1\)</span>，剩下的 1 份用来测试模型；然后，从训练集中重新选择不同的 <span class="math notranslate nohighlight">\(K-1\)</span> 份用于训练模型 <span class="math notranslate nohighlight">\(A_2\)</span>，并用剩余的 1 份测试模型；如此直到完成所有 <span class="math notranslate nohighlight">\(K\)</span> 轮，得到模型 <span class="math notranslate nohighlight">\(A_K\)</span>；然后对结果 <span class="math notranslate nohighlight">\(A\)</span> 求平均。</p>
<p>上述交叉验证过程被称为 <code class="docutils literal notranslate"><span class="pre">K-折交叉验证</span></code> 。当 <span class="math notranslate nohighlight">\(K\)</span> 与样本数量相同时（即 <span class="math notranslate nohighlight">\( K = N\)</span> 时），就是常称的 <code class="docutils literal notranslate"><span class="pre">留一法交叉验证</span> <span class="pre">（LOO-CV）</span></code>。在执行留一法交叉验证时，如果数据数量太多，有时会出现轮数少于数据总数的情况。</p>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210512112629_82.webp" /></p>
<p>图 5.7</p>
</center>
<p>交叉验证是机器学习从业者的谋生之本，有关更多细节，可以阅读 <code class="docutils literal notranslate"><span class="pre">Sebastian</span> <span class="pre">Raschka</span></code> 的《<code class="docutils literal notranslate"><span class="pre">Python</span> <span class="pre">Machine</span> <span class="pre">Learning</span></code>》一书，或 <code class="docutils literal notranslate"><span class="pre">Jack</span> <span class="pre">Vanderplas</span></code> 的《<code class="docutils literal notranslate"><span class="pre">Python</span> <span class="pre">Data</span> <span class="pre">Science</span> <span class="pre">Handbook</span></code>》。</p>
<p>交叉验证简单而强大，不过对某些模型或者量很大的数据而言，交叉验证的计算量可能超出可接受范围。因此，许多人尝试提出了一些更容易计算的量，来得到近似交叉验证的效果，或者应用到不能直接使用交叉验证的情况，其中比较出名的是<strong>信息准则</strong>。</p>
</div>
<div class="section" id="id8">
<h3>5.3.2 信息准则<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h3>
<p>信息准则是一系列用来比较模型对数据拟合程度的方法，这类方法引入了一个惩罚项来平衡模型的复杂度。换句话说，信息准则形式化地表达了在本章开始建立的直觉，用一种合适的方式平衡模型的准确性和简约性。这些衡量方式的推导过程与信息论相关，超出了本书范围，我们只从实用的角度去理解这些概念。</p>
<div class="section" id="log">
<h4><strong>（1）Log 似然与离差</strong><a class="headerlink" href="#log" title="Permalink to this headline">¶</a></h4>
<p>一种衡量模型对数据的拟合程度的方法是计算模型预测结果与真实数据之间的均方差：</p>
<div class="math notranslate nohighlight">
\[\frac{1}{N} \sum_{i=1}^{N}\left(y_{i}-\mathrm{E}\left(y_{i} \mid \theta\right)\right)^{2}  \tag{式5.2}  \label{式5.2}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(E(y_i|\theta)\)</span> 是根据估计的参数值计算得到的预测值。</p>
<p>可以看到基本上就是观察值和预测值之间平均差值，求平方是为保证误差为正，不会相互抵消。相比其他的度量指标（比如绝对值误差），平方度量更强调较大的误差。</p>
<p>一种更通用的方法是计算 log 似然：</p>
<div class="math notranslate nohighlight">
\[\sum_{i=1}^{N} \log p\left(y_{i} \mid \theta\right)  \tag{式5.3}  \label{式5.3}\]</div>
<p>当似然服从正态分布时，已经证明 log 似然与二次均方误差成正比。由于历史原因，实践中人们通常不直接使用 log 似然，而是使用一个称作 <code class="docutils literal notranslate"><span class="pre">离差（deviance）</span></code> 的量：</p>
<div class="math notranslate nohighlight">
\[-2 \sum_{i=1}^{N} \log p\left(y_{i} \mid \theta\right)  \tag{式5.4}  \label{式5.4}\]</div>
<p>离差在贝叶斯方法和非贝叶斯方法中类似，区别在于：贝叶斯框架中 <span class="math notranslate nohighlight">\(θ\)</span> 来自后验的采样。而在非贝叶斯方法中，<span class="math notranslate nohighlight">\(θ\)</span> 是一个点估计。在使用离差时，需注意以下两点：</p>
<ul class="simple">
<li><p>离差越小，log 似然值越大，模型的预测结果与数据越吻合。因此我们<strong>希望离差越小越好</strong>。</p></li>
<li><p>离差衡量的是样本内的模型精度，因而复杂模型通常会比简单模型的离差小，此时<strong>需要给复杂模型加入惩罚项</strong>。</p></li>
</ul>
<p>下面我们将学习几个不同的信息准则方法，<strong>它们的共同点是都使用了离差和正则项，区别在于离差和惩罚项的计算方式不同</strong>。</p>
</div>
<div class="section" id="aic">
<h4><strong>（2）AIC 信息准则</strong><a class="headerlink" href="#aic" title="Permalink to this headline">¶</a></h4>
<p>AIC 信息准则（Akaike Information Criterion）是一个广泛应用的信息准则，其定义如下：</p>
<div class="math notranslate nohighlight">
\[\text{AIC} = -2\sum_{i=1}^{n} \log p\left(y_{i} \mid \hat{\theta}_{m l e}\right)+2 \text{pAIC}  \tag{式5.5}  \label{式5.5}\]</div>
<p>其中，<span class="math notranslate nohighlight">\(pAIC\)</span> 表示参数的个数， <span class="math notranslate nohighlight">\(\hat{\theta}_{m l e}\)</span> 为 <span class="math notranslate nohighlight">\(\theta\)</span> 的最大似然估计。最大似然估计在非贝叶斯方法中经常用到，等价于贝叶斯方法中基于均匀先验的最大后验估计。注意这里 <span class="math notranslate nohighlight">\(\hat{\theta}_{mle}\)</span> 是点估计而不是分布。</p>
<p>同样，此处 −2 也是出于历史原因。从实用角度来看，上式中的第 1 项考虑的是模型对数据的拟合效果，第 2 项衡量的是模型复杂度。因此，如果两个模型对数据的解释能力相同，但是其中一个比另一个的参数更多的话，AIC 会告诉我们应该选择参数更少的那个。</p>
<p>AIC 对非贝叶斯方法来说很有用，但对贝叶斯方法可能会有些问题。原因是 AIC 没有使用后验，因而将估计中的不确定信息丢失了，此外将均匀分布作为先验，对使用非均匀先验的模型来说不太合适。因为在使用非均匀先验时，不能简单地计算模型中参数的个数，合理使用非均匀先验实际上相当于对模型已经使用了正则，并且会降低过拟合的可能，也就是说带正则模型的有效参数个数可能比真实参数个数要少。类似情况在多层模型中也会出现，毕竟多层模型可视为从数据中学习先验的有效方式。</p>
</div>
<div class="section" id="waic">
<h4><strong>（3） WAIC 通用信息准则</strong><a class="headerlink" href="#waic" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">通用信息准则（Widely</span> <span class="pre">Available</span> <span class="pre">Information</span> <span class="pre">Criterion，</span> <span class="pre">WAIC）</span></code> 是 <code class="docutils literal notranslate"><span class="pre">AIC</span></code> 的完全贝叶斯版本。与 <code class="docutils literal notranslate"><span class="pre">AIC</span></code> 一样， <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 有两个项：一项衡量模型对数据的拟合效果；另外一项衡量模型的复杂程度。</p>
<div class="math notranslate nohighlight">
\[\text{ `WAIC` }=-2 \times lppd + 2 \times p_{WAIC} \tag{5.6}\]</div>
<p>如果您想更好地理解这两个术语是什么，请阅读后面的 <code class="docutils literal notranslate"><span class="pre">深入</span> <span class="pre">WAIC</span></code> 部分。从应用角度看，只需要知道我们更喜欢较低的值。</p>
</div>
<div class="section" id="pareto">
<h4><strong>（4）Pareto 平滑重要性采样留一交叉验证</strong><a class="headerlink" href="#pareto" title="Permalink to this headline">¶</a></h4>
<p><code class="docutils literal notranslate"><span class="pre">Pareto</span> <span class="pre">平滑重要性采样留一交叉验证</span></code> 是一种用于近似 <code class="docutils literal notranslate"> <span class="pre">LOO-CV</span></code>  结果但不实际执行 K 次迭代的方法。该方法不是一个信息准则，但提供的结果与 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 非常相似，并且在某些条件下， <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 都是渐近收敛的。该方法主要思想是通过对似然适当重新加权来近似 <code class="docutils literal notranslate"><span class="pre">LOO-CV</span></code> ，在统计学中可以通过重要性采样来实现。但普通的重要性采样结果不稳定，为引入了称为 <code class="docutils literal notranslate"><span class="pre">Pareto</span> <span class="pre">平滑重要性采样(PSIS)</span></code> 的新方法，用来计算更可靠的 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 估计值。该方法结果与 <code class="docutils literal notranslate"><span class="pre">AIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 类似，数值越低，模型估计预测的精度就越高。因此，通常更倾向于选择数值较低的模型。</p>
</div>
<div class="section" id="dic-bic">
<h4><strong>（5）DIC 与 BIC 准则</strong><a class="headerlink" href="#dic-bic" title="Permalink to this headline">¶</a></h4>
<p>另一种常见的信息准则是 <code class="docutils literal notranslate"><span class="pre">差分信息准则（DIC）</span></code> 。但无论在理论上还是在实践上， <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 都被证明比 <code class="docutils literal notranslate"><span class="pre">DIC</span></code> 更有效，因此推荐使用 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 而不是 <code class="docutils literal notranslate"><span class="pre">DIC</span></code>。</p>
<p>另一个信息准则是 <code class="docutils literal notranslate"><span class="pre">贝叶斯信息准则（BIC）</span></code>，它类似于 Logistic 回归。 <code class="docutils literal notranslate"><span class="pre">BIC</span></code> 的提出是为了纠正 <code class="docutils literal notranslate"><span class="pre">AIC</span></code> 的一些问题，作者建议采用贝叶斯纠正。但 <code class="docutils literal notranslate"><span class="pre">BIC</span></code> 并不是真正的贝叶斯，实际上它与 <code class="docutils literal notranslate"><span class="pre">AIC</span></code> 非常相似。它假设平坦的先验，并使用最大似然估计。更重要的是， <code class="docutils literal notranslate"><span class="pre">BIC</span></code> 不同于 <code class="docutils literal notranslate"><span class="pre">AIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> ，而更多涉及 <code class="docutils literal notranslate"><span class="pre">贝叶斯因子（Bayesian</span> <span class="pre">Factor）</span></code> 的概念，这点将在本章后面讨论。</p>
</div>
</div>
</div>
<div class="section" id="pymc3">
<h2>5.4 使用 PyMC3 做模型比较<a class="headerlink" href="#pymc3" title="Permalink to this headline">¶</a></h2>
<p>采用 <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 进行模型比较想像起来容易得多！</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">waic_l</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">waic</span><span class="p">(</span><span class="n">trace_l</span><span class="p">)</span>
<span class="n">waic_l</span>
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210525103006_9f.webp" /></p>
<p>如果你想计算 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 而不是 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> ，需要使用 <code class="docutils literal notranslate"><span class="pre">az.loo</span></code> 。对于 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> ，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 报告了四个值（见上表）：</p>
<ul class="simple">
<li><p>一个点估计值</p></li>
<li><p>点估计的标准差（通过假设正态分布计算的，因此在样本量较低时可能不太可靠）</p></li>
<li><p>有效参数的数量</p></li>
<li><p>警告数量</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>在计算 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 或 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 时，可能会收到一些警告消息，指出计算的结果可能不可靠。此警告是根据经验确定的阈值提出的（请参阅相关文献资料）。虽然这不一定是错误，但可能表明这些度量计算存在问题。 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 相对较新，或许需要开发更好的方法来获得其可靠性。</p>
<p>无论如何，如果出现警告的情况，首先应当确保有足够样本，并且是一个混合良好、可靠的样本（参见第8章，推理引擎）。如果仍然接收到警告， <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 方法的提出者建议使用更健壮的模型，如使用学生 t 分布而不是高斯分布。如果上述建议都不起作用，那么可能需要考虑使用另一种方法，例如直接执行 <code class="docutils literal notranslate"><span class="pre">K-折交叉验证</span></code>。 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 只能帮助你在一组给定的模型中进行选择，但不能帮助你决定一个模型是否真的是解决特定问题的好方法。因此， <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 应该得到后验预测性检查以及任何其他信息和测试的补充，这些信息和测试可以帮助我们根据待解决的特定问题和领域知识来设置模型和数据。</p>
</div>
<p>由于 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 总是以相对的方式进行解释，<code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 提供了两个辅助函数来简化比较。第一个是 <code class="docutils literal notranslate"><span class="pre">az.compare</span></code> ：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cmp_df</span> <span class="o">=</span> <span class="n">az</span><span class="o">.</span><span class="n">compare</span><span class="p">({</span><span class="s1">&#39;model_l&#39;</span><span class="p">:</span><span class="n">trace_l</span><span class="p">,</span> <span class="s1">&#39;model_p&#39;</span><span class="p">:</span><span class="n">trace_p</span><span class="p">},</span> <span class="n">method</span><span class="o">=</span><span class="s1">&#39;BB-pseudo-BMA&#39;</span><span class="p">)</span>
<span class="n">cmp_df</span>
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210525103242_82.webp" /></p>
<p>这里有很多列：</p>
<ul class="simple">
<li><p>第 1 列为 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 的值。表格默认以该列升序排列，索引列则反映了该排序。</p></li>
<li><p>第 2 列是估计的有效参数个数。一般来说，参数越多的模型数据拟合越灵活，但也更可能导致过拟合。因此，可以将 <code class="docutils literal notranslate"><span class="pre">pwaic</span></code> 解释为惩罚性术语，也可将其解释为度量每个模型在拟合数据方面的灵活性。</p></li>
<li><p>第 3 列是 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 的相对值，以排名最高的模型 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 值为基准，列出各模型 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 值与基准值间的相对差，第一个模型的值始终为 0 。</p></li>
<li><p>第 4 列为权重。在比较模型时，有时并不想选择量值指示最好的模型，而是希望通过平均若干模型来进行预测，并且通过加权平均，赋予不同模型适当的权重（见 5.5 节）。比较常用的方法是基于 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 值给每个模型赋予一个 <code class="docutils literal notranslate"><span class="pre">Akaike权重</span></code> 。在给定数据时，这些权重解释为每个模型的概率。此方法存在的一个问题是：由于该权重的计算基于 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 的点估计，因此不确定性被忽略了。</p></li>
<li><p>第 5 列记录了 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 值的标准差。标准差可用于评估 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 估计的不确定度。</p></li>
<li><p>第 6 列记录了第 2 列相对值的标准差。由于 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 的不确定性在不同模型之间相关程度不同，不同模型应当拥有不同的值。</p></li>
<li><p>第 7 列名为 <code class="docutils literal notranslate"><span class="pre">WARNING</span></code> 。值 1 表示 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 的计算可能不可靠。</p></li>
</ul>
<p>我们还可以通过使用 <code class="docutils literal notranslate"><span class="pre">az.plot_compare</span></code> 函数可视化上述信息。该函数接受 <code class="docutils literal notranslate"><span class="pre">az.compare</span></code> 的输出，并以 <code class="docutils literal notranslate"><span class="pre">Richard</span> <span class="pre">McElreath</span></code> 的《统计反思》一书中使用的样式生成汇总图：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_compare</span><span class="p">(</span><span class="n">cmp_df</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210525103532_b8.webp" /></p>
<p>图 5.8</p>
<p>让我详细描述一下图5.8：</p>
<ul class="simple">
<li><p>空圆圈代表 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 的值，与之相关的黑色误差条是 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 的标准差。</p></li>
<li><p>最低的 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 值用一条垂直的灰色虚线表示，以便于与其他 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 值进行比较。</p></li>
<li><p>实心黑圆圈是每个模型的<code class="docutils literal notranslate"><span class="pre">样本内离差</span></code>，对于 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 来说，它与相应的 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 值相差 <span class="math notranslate nohighlight">\(2 \times pWAIC\)</span> 。</p></li>
<li><p>除最佳模型外，其他模型会有一个三角形，表示该模型和最佳模型之间的 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 相对值，配套的有一个灰色误差条，表示 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 相对值的标准差。</p></li>
</ul>
<p>依据信息准则来选择模型，可能是一种最简单的决策方式。此时只需要选择信息准则值较低的模型，而忽略其他模型即可。如果遵循这种方式，前例中的二次多项式模型可能是最佳选择。请注意，标准差不重叠给做出此选择提供了信心。相反，如果标准差是重叠的，则应该提供一个更微妙的答案。</p>
</div>
<div class="section" id="id9">
<h2>5.5 模型平均<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h2>
<p>模型选择是一个比较简单的决策，但我们在选择模型时也正在抛弃有关模型中不确定性的信息。这类似于计算好完整的后验后，只保留了后验的平均值。这会造成造成我们对模型过于自信。</p>
<p>一种变通的方案是执行模型选择同时，报告和讨论不同模型的信息准则值、标准差等统计量以及后验预测性检查情况。将所有这些数字和检查放在问题上下文中很重要，只有这样相关人士才能更好地感受到模型可能存在的局限性和缺点。在学术界中，可以使用该方法在论文、演示文稿等的讨论部分添加相关要素。</p>
<p>除了上述对各模型均做出报告和讨论的方法外，还有一种做法是充分利用模型比较中的不确定性，执行模型平均。下面介绍其中几种比较常用的模型平均方法：</p>
<div class="section" id="id10">
<h3>5.5.1 基于信息准则值的加权平均<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>本方法使用每个模型的加权平均值来生成 <code class="docutils literal notranslate"><span class="pre">元模型（meta-model）</span></code> 和 <code class="docutils literal notranslate"><span class="pre">元预测（meta-predictions）</span></code> 。不同模型的权重计算基于某些信息准则值（如 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code>），公式如下：</p>
<div class="math notranslate nohighlight">
\[w_{i}=\frac{e^{\frac{1}{2} d E_{i}}}{\sum_{j}^{M} e^{-\frac{1}{2} d E_{j}}} \tag{式5.7}  \label{式5.7}\]</div>
<p>这里 <span class="math notranslate nohighlight">\(dE_i\)</span> 是第 <span class="math notranslate nohighlight">\(i\)</span> 个模型相对于最佳模型（<code class="docutils literal notranslate"><span class="pre">WAIC</span></code>值最小的模型）的 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 相对差值。除 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 外，此处也可以使用其他信息准则值，如 <code class="docutils literal notranslate"><span class="pre">AIC</span></code> 或 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 等。此公式是根据 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 值计算各模型相对概率的启发式方法。分母为归一化因子，<code class="docutils literal notranslate"><span class="pre">第4章</span> <span class="pre">广义线性模型</span></code>中有过类似的表达式。</p>
<p>采用式 5.7 的权重对模型进行加权平均称为 <code class="docutils literal notranslate"><span class="pre">伪贝叶斯模型平均</span></code> 。真正的贝叶斯模型平均应当使用边缘似然，而非 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 或 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 。不过尽管边缘似然在理论上很有吸引力，但在模型比较和模型平均中使用很少。大多还是选择 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 或 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 而非边缘似然。在后续 <code class="docutils literal notranslate"><span class="pre">贝叶斯因子</span></code> 一节中，会有更多此方面的讨论。</p>
<p>使用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> ，可以通过将 <code class="docutils literal notranslate"><span class="pre">method=‘pseudo-BMA’</span></code> （伪贝叶斯模型平均）参数传递给 <code class="docutils literal notranslate"><span class="pre">az.compare</span></code> 函数来计算式 5.7 的权重。其问题是未考虑计算 <span class="math notranslate nohighlight">\(E_i\)</span> 时的不确定性。通过高斯近似可以计算每一个 <span class="math notranslate nohighlight">\(E_i\)</span> 的标准差。这也是函数 <code class="docutils literal notranslate"><span class="pre">az.waic</span></code>、<code class="docutils literal notranslate"><span class="pre">az.loo</span></code> 和 <code class="docutils literal notranslate"><span class="pre">az.compare</span></code> 在传递 <code class="docutils literal notranslate"><span class="pre">method=‘pseudo-BMA’</span></code> 参数时返回的误差值。此外，还可以使用 <code class="docutils literal notranslate"><span class="pre">贝叶斯自举（Bayesian</span> <span class="pre">bootstrapping）法</span></code> 来估计不确定性。这是一种比高斯近似更可靠的方法。通过将 <code class="docutils literal notranslate"><span class="pre">method=‘BB-pseudo-BMA’</span></code> 传递给 <code class="docutils literal notranslate"><span class="pre">az.compare</span></code> 函数即可实现。</p>
</div>
<div class="section" id="id11">
<h3>5.5.2 基于预测分布堆叠的加权平均<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>另一种计算平均模型权重的方法被称为 <code class="docutils literal notranslate"><span class="pre">预测性分布堆叠（stacking</span> <span class="pre">of</span> <span class="pre">predictive</span> <span class="pre">distributions）</span></code> 。这在 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 中通过将 <code class="docutils literal notranslate"><span class="pre">method=‘stacking’</span></code> 传递给 <code class="docutils literal notranslate"><span class="pre">az.compare</span></code> 实现。其基本思想是通过最小化元模型和真实生成模型之间的差异，将多个模型组合到一个元模型中。当使用对数打分规则时，这等价于：</p>
<div class="math notranslate nohighlight">
\[\max _{n} \frac{1}{n} \sum_{i=1}^{n} \log \sum_{k=1}^{K} w_{k} p\left(y_{i} \mid y_{-i}, M_{k}\right) \tag{式5.8}  \label{式5.8}\]</div>
<p>这里，<span class="math notranslate nohighlight">\(n\)</span> 是数据点的数量，<span class="math notranslate nohighlight">\(k\)</span> 是模型的数量。为了强制实施方案，我们将 <span class="math notranslate nohighlight">\(w\)</span> 约束为 <span class="math notranslate nohighlight">\(w_k \geq 0\)</span> 并且 <span class="math notranslate nohighlight">\(\sum w_k =1\)</span>。量 <span class="math notranslate nohighlight">\(p(y_i|y_{-i},M_k)\)</span> 是模型 <span class="math notranslate nohighlight">\(M_k\)</span> 的留一预测性分布。根据留一法，计算需要拟合每个模型 <span class="math notranslate nohighlight">\(n\)</span> 次，每次遗留一个数据点。幸运的是，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 可以使用 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 或 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 来近似留一预测性分布。</p>
</div>
<div class="section" id="id12">
<h3>5.5.3 其他模型平均方法<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>还有其他方法来平均模型，例如，显式构建包括所有感兴趣模型作为子模型的元模型。可以构建这样一个模型：我们对每个子模型的参数进行推断，同时计算每个模型的相对概率（有关此方面的示例，请参阅 <code class="docutils literal notranslate"><span class="pre">贝叶斯因子</span></code> 一节）。</p>
<p>除了平均离散模型之外，有时还可以考虑它们的连续版本。例如，假设有一个抛硬币问题，我们有两个不同的模型：一个偏向正面，另一个偏向反面。那么它的连续版本将是一个分层模型，其中先验分布直接从数据中估计出来。该层次模型包括离散模型作为特例。</p>
<p>哪种方法更好取决于我们的具体问题：</p>
<ul class="simple">
<li><p>我们是否真的有很好的理由考虑离散模型，或者我们的问题更好地表示为连续模型？</p></li>
<li><p>对于我们的问题来说，挑出一个模型很重要，因为我们是从相互竞争的解释角度思考的，或者平均是更好的想法，因为我们对预测更感兴趣，或者我们真的可以将流程生成过程视为子流程的平均吗？</p></li>
</ul>
<p>所有这些问题都不是由统计数据来回答的，而是由领域知识背景下的统计数据来提供信息的。</p>
<p>以下只是如何从 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 获得加权后验预测样本的一个虚拟示例。在这里，我们使用的是 <code class="docutils literal notranslate"><span class="pre">pm.sample_posterior_predictive_w</span></code> 函数（注意函数名称末尾的 <code class="docutils literal notranslate"><span class="pre">w</span></code> ）。<code class="docutils literal notranslate"><span class="pre">pm.sample_posterior_predictive</span></code> 和<code class="docutils literal notranslate"><span class="pre">pm.sample_posterior_predictive_w</span></code> 之间的区别在于，后者接受多个迹和模型，以及权重列表（默认值为所有模型的权重相同）。您可以通过 <code class="docutils literal notranslate"><span class="pre">az.compare</span></code> 或其他来源获取这些权重：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">w</span> <span class="o">=</span> <span class="mf">0.5</span>
<span class="n">y_lp</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample_posterior_predictive_w</span><span class="p">([</span><span class="n">trace_l</span><span class="p">,</span> <span class="n">trace_p</span><span class="p">],</span>
                                        <span class="n">samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
                                        <span class="n">models</span><span class="o">=</span><span class="p">[</span><span class="n">model_l</span><span class="p">,</span> <span class="n">model_p</span><span class="p">],</span>
                                        <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">w</span><span class="p">])</span>
<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">y_l</span><span class="p">,</span> <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;C1&#39;</span><span class="p">},</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;linear model&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">y_p</span><span class="p">,</span> <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;C2&#39;</span><span class="p">},</span>
            <span class="n">label</span><span class="o">=</span><span class="s1">&#39;order 2 model&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_kde</span><span class="p">(</span><span class="n">y_lp</span><span class="p">[</span><span class="s1">&#39;y_pred&#39;</span><span class="p">],</span> <span class="n">plot_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span> <span class="s1">&#39;C3&#39;</span><span class="p">},</span>
           <span class="n">label</span><span class="o">=</span><span class="s1">&#39;weighted model&#39;</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">y_1s</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">y_1s</span><span class="p">),</span> <span class="s1">&#39;|&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;observed data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210524232628_ac.webp" /></p>
<p>图 5.9</p>
</center>
<p>前面提到这是一个虚拟示例，因为与线性模型相比，二次多项式模型的 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 值非常低，第一个模型的权重基本上是 1 ，而后者权重基本上是 0 ，为生成图 5.9 ，我假设了这两个模型具有相同的权重。</p>
</div>
</div>
<div class="section" id="id13">
<h2>5.6 贝叶斯因子<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h2>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>背景：零假设显著性检验（NHST）是频率主义进行数据分析的主要工具。但在统计学领域，NHST 受到了广泛批评。越来越多的统计学者提倡使用贝叶斯方法检验研究假设，在实证研究中也有越来越多的学者使用贝叶斯因子进行数据分析。</p>
</div>
<p>在贝叶斯世界中，评估和比较模型的一种常见选择是 <code class="docutils literal notranslate"><span class="pre">贝叶斯因子（Bayes</span> <span class="pre">factor,</span> <span class="pre">BF）</span></code> 。 为理解什么是贝叶斯因子，让我们重温一遍贝叶斯定理：</p>
<div class="math notranslate nohighlight">
\[p(\theta \mid y)=\frac{p(y \mid \theta) p(\theta)}{p(y)} \tag{式5.9}  \label{式5.9}\]</div>
<p>这里，<span class="math notranslate nohighlight">\(y\)</span> 表示数据。我们可以显式地基于给定模型 <span class="math notranslate nohighlight">\(M\)</span> 计算依赖关系：</p>
<div class="math notranslate nohighlight">
\[p\left(\theta \mid y, M_{k}\right)=\frac{p\left(y \mid \theta, M_{k}\right) p\left(\theta \mid M_{k}\right)}{p\left(y \mid M_{k}\right)}\tag{式5.10}   \label{式5.10}\]</div>
<p>第一章中曾经介绍过，分母中的术语称为边缘似然（或证据），可视为一个归一化常数。在进行单模型推断时，通常不需要真实地计算它，而是基于一个常数因子来计算后验。但对于模型比较和模型平均，边缘似然却是一个重要的量。如果主要目标是从一组 <span class="math notranslate nohighlight">\(k\)</span> 个模型中选择一种最好的模型，我们可以只选择 <span class="math notranslate nohighlight">\(p(y|M_k)\)</span> 最大的那个。一般来说， <span class="math notranslate nohighlight">\(p(y|M_k)\)</span> 值的大小本身并不能告诉我们太多信息，重要的是相对值。因此，实践中经常计算两个边缘似然的比率，这个比率被称为贝叶斯因子：</p>
<div class="math notranslate nohighlight">
\[B F=\frac{p\left(y \mid M_{0}\right)}{p\left(y \mid M_{1}\right)} \tag{式5.11}  \label{式5.11}\]</div>
<p>当 <span class="math notranslate nohighlight">\(BF（M_0,M_1） &gt; 1\)</span> 时，模型 0 比模型 1 更好地解释了数据。</p>
<p>一些作者提出了带有范围的表格，以便于离散化和简化 <span class="math notranslate nohighlight">\(BF\)</span> 的解释。例如，下列符号列表显示了 ”支持模式 0 而不支持模式 1“ 的证据强度：</p>
<ul class="simple">
<li><p>1-3：初级</p></li>
<li><p>3-10：中等</p></li>
<li><p>10-30：强</p></li>
<li><p>30-100：非常强</p></li>
<li><p>大于100：极端</p></li>
</ul>
<p>不过需要注意的是：这些规则只是一些约定，最终结果始终应放在上下文中，并伴随足够细节，以便其他人可以检查是否同意我们的结论。</p>
<p>如果假设所有模型都具有相同先验概率，则使用 <span class="math notranslate nohighlight">\(p(y|M_k)\)</span> 来比较模型完全没有问题。否则，必须计算后验赔率：</p>
<div class="math notranslate nohighlight">
\[\underbrace{\frac{p\left(M_{0} \mid y\right)}{p\left(M_{1} \mid y\right)}}_{\text {posterior odds }}=\underbrace{\frac{p\left(y \mid M_{0}\right)}{p\left(y \mid M_{1}\right)}}_{\text {Bayes factors}} \underbrace{\frac{p\left(M_{0}\right)}{p\left(M_{1}\right)}}_{\text{prior odds} } \tag{式5.12}  \label{式5.12}\]</div>
<div class="section" id="id14">
<h3>5.6.1 一些讨论<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>现在简要讨论有关边缘似然的一些关键事实。通过仔细检查定义，可以理解边缘似然的性质和应用效果：</p>
<div class="math notranslate nohighlight">
\[p\left(y \mid M_{k}\right)=\int_{\theta_{k}} p\left(y \mid \theta_{k}, M_{k}\right) p\left(\theta_{k}, M_{k}\right) d \theta_{k} \tag{式5.13}  \label{式5.13}\]</div>
<ul class="simple">
<li><p><strong>好处</strong>：参数多的模型比参数少的模型具有更大惩罚。贝叶斯因子内置奥卡姆剃刀，因为参数数量越多，先验分布相对于似然就越广。结合贝叶斯因子公式，越宽广的先验（参数更多）积分越大，而越聚集的先验（参数更少）积分越小，从而间接实现了对参数数量的惩罚。</p></li>
<li><p><strong>缺点</strong>：计算边缘似然是一项艰巨的任务，因为要计算高维参数空间上的多变量函数积分，需要使用复杂方法进行数值求解。</p></li>
<li><p><strong>尴尬之处</strong>：边缘似然对先验取值的依赖过于敏感。</p></li>
</ul>
<p>使用边缘似然来比较模型是一个好主意，因为复杂模型的惩罚已经包括在内。但同时先验信息的变化会影响边缘似然的计算，其中关键词是 ”敏感“ 。也就是说先验的微小变化，可能会对边缘似然的值产生很大影响。在前例中，标准差为 100 的正态先验与标准差为 1000 的正态先验变化很小，但相应的贝叶斯因子受这些变化却产生了较大变化。</p>
<p>另一个相关的批评是，贝叶斯因子可以被用作进行假设检验的贝叶斯统计方法。这种批评本身没错，但许多文章指出，推断方法比假设检验方法（无论是否为贝叶斯方法）更适合于大多数问题。</p>
</div>
<div class="section" id="id15">
<h3>5.6.2 贝叶斯因子的计算<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>贝叶斯因子的计算可以被视为一个分层模型，其中超参数是分配给每个模型并从<code class="docutils literal notranslate"><span class="pre">类别分布</span></code>中采样的 <code class="docutils literal notranslate"><span class="pre">index</span></code> 。换句话说，我们同时对多个相互竞争的模型进行推断，并使用在模型间跳跃的离散变量。我们花在每个模型上的采样时间与 <span class="math notranslate nohighlight">\(p(M_k|y)\)</span> 成正比。然后，应用公式 5.10 求出贝叶斯因子。</p>
<p>为举例说明贝叶斯因子的计算，我们再来一次抛硬币：</p>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210525130312_77.webp" /></p>
<p>图 5.10</p>
</center>
<p>让我们创建一些数据，以便在示例中使用：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coins</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">heads</span> <span class="o">=</span> <span class="mi">9</span>
<span class="n">y_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">coins</span><span class="o">-</span><span class="n">heads</span><span class="p">,</span> <span class="n">heads</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>现在，来看一下 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 模型。为在之前的代码之间切换，我们使用了 <code class="docutils literal notranslate"><span class="pre">pm.math.switch</span></code> 函数。如果此函数的第一个参数的计算结果为 <code class="docutils literal notranslate"><span class="pre">true</span></code>，则返回第二个参数，否则返回第三个参数。请注意，还使用 <code class="docutils literal notranslate"><span class="pre">pm.math.eq</span></code> 函数来检查 <code class="docutils literal notranslate"><span class="pre">model_index</span></code> 变量是否等于 0 ：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_BF</span><span class="p">:</span>
    <span class="n">p</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
    <span class="n">model_index</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Categorical</span><span class="p">(</span><span class="s1">&#39;model_index&#39;</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span>
    <span class="n">m_0</span> <span class="o">=</span> <span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">m_1</span> <span class="o">=</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">switch</span><span class="p">(</span><span class="n">pm</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">model_index</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">m_0</span><span class="p">,</span> <span class="n">m_1</span><span class="p">)</span>
    <span class="c1"># a priori</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;θ&#39;</span><span class="p">,</span> <span class="n">m</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">m</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># likelihood</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">θ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_d</span><span class="p">)</span>

    <span class="n">trace_BF</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">5000</span><span class="p">)</span>

<span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_BF</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210525130426_94.webp" /></p>
<p>图 5.11</p>
</center>
<p>现在，需要通过计算 <code class="docutils literal notranslate"><span class="pre">model_index</span></code> 变量来计算贝叶斯因子。请注意，我们已经包括了每个模型的先验值：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pM1</span> <span class="o">=</span> <span class="n">trace_BF</span><span class="p">[</span><span class="s1">&#39;model_index&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">pM0</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">pM1</span>
<span class="n">BF</span> <span class="o">=</span> <span class="p">(</span><span class="n">pM0</span> <span class="o">/</span> <span class="n">pM1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">/</span> <span class="n">p</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>结果，我们得到的值为≈11，这意味着模型 0 比模型 1 高出一个数量级。这非常有意义，因为数据的正面值比预期的 <span class="math notranslate nohighlight">\(\theta=0.5\)</span> 要少，两个模型之间的唯一区别是模型 0 的先验更兼容 <span class="math notranslate nohighlight">\(\theta&lt;0.5\)</span> (背面比正面多)，模型 1 更兼容 <span class="math notranslate nohighlight">\(\theta&gt;0.5\)</span> (正面比背面多)。</p>
</div>
<div class="section" id="id16">
<h3>5.6.3 计算贝叶斯因子时的常见问题<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>计算贝叶斯因子常见的问题是，如果一个模型比另一个模型更好，我们从它身上采样的时间也会比另一个模型时间更多。另一个问题是，即使参数没有被用来拟合模型，参数值也会更新。也就是说，当选择模型 0 时，模型 1 中的参数也会更新，但由于它们不用解释数据，因此只受先验信息的限制。如果先验条件太模糊，当我们选择模型 1 时，参数值可能离以前接受的值太远，因此该步骤被拒绝。最终导致采样遇到问题。</p>
<p>如果遇到此类问题，可以对模型进行两个调整以改进采样：</p>
<ul class="simple">
<li><p>理想情况下，如果两个模型被同等访问，可以获得更好的采样，这样就可以调整每个模型的先验(前一个模型中的 <span class="math notranslate nohighlight">\(p\)</span> 变量)，以便支持不太有利的模型，而不支持最有利的模型。这不会影响贝叶斯因子的计算，因为计算中包含了先验信息。</p></li>
<li><p>按照 <code class="docutils literal notranslate"><span class="pre">Kruschke</span></code>和其他人的建议，使用 <code class="docutils literal notranslate"><span class="pre">伪先验</span></code> 。想法很简单：如果问题是当所属的模型没有被选择时，参数会不受限制地漂移，那么一种解决方案是尝试人为地在不使用的时候限制它们。您可以找到一个在 <code class="docutils literal notranslate"><span class="pre">Kruschke</span></code> 书中使用的模型示例，我将该模型移植到了 <a class="reference external" href="https:///%E2%80%8B/%E2%80%8Bgithub.%E2%80%8Bcom/%E2%80%8Baloctavodia/%E2%80%8BDoing_%E2%80%8Bbayesian_%E2%80%8Bdata_%E2%80%8Banalysis"><code class="docutils literal notranslate"><span class="pre">PyMC3</span></code></a> 中。</p></li>
</ul>
</div>
<div class="section" id="id17">
<h3>5.6.4 用序贯蒙特卡罗方法计算贝叶斯因子<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>另一种计算贝叶斯因子的方法是使用 <code class="docutils literal notranslate"><span class="pre">序贯蒙特卡罗(SMC)采样方法</span></code>。我们将在 <code class="docutils literal notranslate"><span class="pre">第8章-推理引擎</span></code> 中学习此方法的详细信息。现在只需要知道这个采样器计算的边缘似然估计是一个副产品，可以直接使用它来计算贝叶斯因子。要在 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 中使用 <code class="docutils literal notranslate"><span class="pre">SMC</span></code>，需将 <code class="docutils literal notranslate"><span class="pre">pm.SMC()</span></code> 传递给 <code class="docutils literal notranslate"><span class="pre">sample</span></code> 的 <code class="docutils literal notranslate"><span class="pre">step</span></code> 参数：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_BF_0</span><span class="p">:</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;θ&#39;</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">θ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_d</span><span class="p">)</span>
    <span class="n">trace_BF_0</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2500</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">SMC</span><span class="p">())</span>

<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model_BF_1</span><span class="p">:</span>
    <span class="n">θ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;θ&#39;</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">θ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_d</span><span class="p">)</span>
    <span class="n">trace_BF_1</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2500</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="n">pm</span><span class="o">.</span><span class="n">SMC</span><span class="p">())</span>
    
<span class="n">model_BF_0</span><span class="o">.</span><span class="n">marginal_likelihood</span> <span class="o">/</span> <span class="n">model_BF_1</span><span class="o">.</span><span class="n">marginal_likelihood</span>
</pre></div>
</div>
</div>
</div>
<p>根据 <code class="docutils literal notranslate"><span class="pre">SMC</span> <span class="pre">方法</span></code>，贝叶斯因子也在 11 左右，如果你想用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 计算贝叶斯因子，我强烈推荐使用 SMC 方法。本书中提出的另一种方法在计算上更加繁琐，需要更多手动调整，主要是因为模型间的跳跃需要用户通过反复试验进行更多调整。从这点上来说，SMC 是一种自动化程度更高的方法。</p>
</div>
<div class="section" id="id18">
<h3>5.6.5 贝叶斯因子与信息准则<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>如果对贝叶斯因子求对数，可以将两个边缘似然的比值转换成求差，这样比较边缘似然就与比较信息准则类似了。但是，衡量模型的数据拟合程度项以及惩罚项去哪儿了呢？前者包含在了似然部分，而后者包含在对先验取平均的部分。参数越多，先验空间相比似然就越大，平均之后似然就会较低，而参数越多，先验就会越分散，在计算证据时惩罚越大。这也是为什么人们说贝叶斯理论会很自然地惩罚更复杂的模型，或者称贝叶斯理论自带奥卡姆剃刀。</p>
<p>此前说过，贝叶斯因子对先验过于敏感。这在执行推断时会导致本来不相关的差异，在计算贝叶斯因子时被证明为非常重要。现在我们来看一个例子，它将有助于阐明贝叶斯因子在做什么，信息准则在做什么，以及它们如何在相似的情况下专注于两个不同的方面。回到抛硬币例子的数据定义，现在设置 300 枚硬币和 90 个正面；这与以前的比例相同，但数据多了 10 倍。然后，分别运行每个模型：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">traces</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">waics</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">coins</span><span class="p">,</span> <span class="n">heads</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">9</span><span class="p">),</span> <span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">90</span><span class="p">)]:</span>
    <span class="n">y_d</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="n">coins</span><span class="o">-</span><span class="n">heads</span><span class="p">,</span> <span class="n">heads</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">priors</span> <span class="ow">in</span> <span class="p">[(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">4</span><span class="p">)]:</span>
        <span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">model</span><span class="p">:</span>
            <span class="n">θ</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;θ&#39;</span><span class="p">,</span> <span class="o">*</span><span class="n">priors</span><span class="p">)</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">,</span> <span class="n">θ</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">y_d</span><span class="p">)</span>
            <span class="n">trace</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">)</span>
            <span class="n">traces</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">trace</span><span class="p">)</span>
            <span class="n">waics</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">az</span><span class="o">.</span><span class="n">waic</span><span class="p">(</span><span class="n">trace</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210525130903_77.webp" /></p>
<p>图 5.12</p>
</center>
<p>通过增加数据，我们几乎完全克服了先验，现在两个模型都做出了类似的预测。用 30 枚硬币和 9 个正面作为数据，可以看到的 <span class="math notranslate nohighlight">\(BF \approx 11\)</span> ，如果用 300 个硬币和 90 个正面的数据重复计算，我们会看到 <span class="math notranslate nohighlight">\(BF \approx 25\)</span> 。贝叶斯因子表明模型 0 比模型 1 更受青睐。当增加数据时，模型之间的决定变得更加清晰。这完全有道理，因为现在我们更确定模型 1 有一个与数据不一致的先验。</p>
<p>还要注意，随着数据量增加，两个模型的 <span class="math notranslate nohighlight">\(\theta\)</span> 值趋于一致；实际上，两个模型的值都大约是 0.3。因此，如果决定用 <span class="math notranslate nohighlight">\(\theta\)</span> 来预测新的结果，将与计算 <span class="math notranslate nohighlight">\(\theta\)</span> 的分布的模型几乎没有什么不同。</p>
<p>现在，比较一下 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 告诉我们的内容（参见图5.13）。模型 0 的 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 是 368.4，模型 1 的是 368.6，直觉上差别不大。比实际差异更重要的是，如果重新计算数据的信息准则，也就是 30 枚硬币和 9 个正面，你会得到模型 0 的 38.1 和模型 1 的 39.4 。也就是说，在增加数据时，相对差异变得越小，<span class="math notranslate nohighlight">\(\theta\)</span> 的估计值越相近，与信息准则估计出的预测准确度的值就越相似。如果你用 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 代替 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> ，会发现本质上是一样的：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">model_names</span>
<span class="n">indices</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">ind</span><span class="p">,</span> <span class="n">d</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">indices</span><span class="p">,</span> <span class="n">waics</span><span class="p">)):</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">d</span><span class="o">.</span><span class="n">waic</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="o">.</span><span class="n">errorbar</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="o">-</span><span class="n">i</span><span class="p">,</span> <span class="n">xerr</span><span class="o">=</span><span class="n">d</span><span class="o">.</span><span class="n">waic_se</span><span class="p">,</span> <span class="n">fmt</span><span class="o">=</span><span class="s1">&#39;o&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">ind</span><span class="p">]</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="n">mean</span><span class="p">,</span> <span class="o">-</span><span class="n">i</span><span class="o">+</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">labels</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">30</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="mi">330</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="o">-</span><span class="n">i</span><span class="o">-</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">wspace</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;Deviance&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210525131100_4d.webp" /></p>
<p>图 5.13</p>
</center>
<p><strong>贝叶斯因子关注的是哪个模型更好，而 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> (和 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> ) 关注的是哪个模型能给出更好的预测。</strong> 如果检查公式 5.6 和 5.11，你就会看到这些不同。 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 和其他信息准则一样，以这样或那样的方式使用对数似然，先验并不直接作为计算的一部分。先验只间接参与，辅助我们估计。取而代之的是，贝叶斯因子直接使用先验，因为我们需要对先验值的整个范围内的似然进行平均。</p>
</div>
</div>
<div class="section" id="id19">
<h2>5.7 其他<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id20">
<h3>5.7.1 正则先验<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<p>使用强信息和弱信息先验是在模型中引入偏差的一种方式，如果操作得当，这可能是一个非常好的方法，因为偏差可以防止过拟合，从而有助于模型做出泛化性能更好的预测。在不影响模型建模能力的情况下，添加偏差以减少泛化误差的想法称为正则化。这种正则化通常采用对模型参数数量的较大值实施惩罚的形式。正则先验是一种减少模型所能表示信息的方法，从而降低了模型捕获噪声而不是信号的机会。</p>
<p>正则化思想如此强大和有用，以至于本书中已经出现了很多次。在非贝叶斯统计中，正则化思想表现为对最小二乘法的两种修正： <code class="docutils literal notranslate"><span class="pre">岭回归</span></code> 和 <code class="docutils literal notranslate"><span class="pre">套索回归</span></code> 。从贝叶斯观点来看，岭回归可解释为 <code class="docutils literal notranslate"><span class="pre">对线性模型的贝塔系数采用标准差趋近于</span> <span class="pre">0</span> <span class="pre">的正态分布，使该系数趋向于零</span></code>。从该意义上说，我们一直在为本书中的每一个线性模型做类似岭回归的事情。另一方面，套索回归可以从贝叶斯的观点解释为 <code class="docutils literal notranslate"><span class="pre">从贝塔系数具有Laplace先验的模型计算出的后验分布图</span></code>。拉普拉斯分布看起来类似于高斯分布，但它的一阶导数在零处没有定义，因为它在零处有一个非常尖锐的峰值（参见图5.14）。与正态分布相比，拉普拉斯分布使其概率质量更接近于零。使用这种先验的出发点是提供 <code class="docutils literal notranslate"><span class="pre">正则化</span></code> 的同时实现 <code class="docutils literal notranslate"><span class="pre">变量选择</span></code>。其思路是，由于峰值为零，预计先验会导致稀疏性，也就是说，我们创建了一个具有许多参数的模型，先验将自动使大多数参数为零，只保留对模型输出有贡献的相关变量。不幸的是，贝叶斯套索不是这样工作的，基本上是为了有很多参数，拉普拉斯先验迫使非零参数变小。幸运的是，并不是所有东西都丢失了 – 有一些贝叶斯模型可以用来诱导稀疏性和执行变量选择。</p>
<p>值得注意的是，经典版本的岭回归和套索回归对应于点估计，而贝叶斯版本则给出了完整的后验分布结果：</p>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210524233016_7c.webp" /></p>
<p>图 5.14</p>
</center>
</div>
<div class="section" id="id21">
<h3>5.7.2 深入 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code><a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<p>如果展开公式 5.6，会得到以下结果：</p>
<div class="math notranslate nohighlight">
\[\text{WAIC}=-2 \sum_{i}^{n} \log \left(\frac{1}{S} \sum_{s=1}^{S} p\left(y_{i} \mid \theta^{s}\right)\right)+2 \sum_{i}^{n}\left(\text{V}_{s=1}^{S}\left(\log p\left(y_{i} \mid \theta^{s}\right)\right)\right. \tag{式5.14}  \label{式5.14}\]</div>
<p>该表达式中的两项看起来非常相似。第一项是式 5.6 中的<code class="docutils literal notranslate"><span class="pre">对数点预测密度（lppd）</span></code>，计算的是后验样本集 <span class="math notranslate nohighlight">\(S\)</span> 的平均似然。我们对每个数据点都先求平均似然，然后取对数，最后对所有数据点求和。请将这一项与公式 5.3 和 5.4 进行比较。其实该项就是考虑了后验的样本内离差（deviance）。因此，如果我们认为计算对数似然是衡量模型适合性的好方法，那么在贝叶斯方法中，从后验计算对数似然就顺理成章。观测数据的 lddp 是对未来数据 lppd 的高估（此处意指样本内离差通常小于样本外离差），因此引入第二项来修正这种过高的估计。第二项计算后验样本的对数似然方差，我们对每个数据点执行此方差计算，然后对所有数据点进行汇总。为什么方差会给出惩罚条件？这与贝叶斯因子内置奥卡姆剃须刀的原理相似。有效参数越多，后验分布越大。当向模型添加结构时（如具有信息性/正则化的先验或分层依赖），与非正则化的模型相比，我们约束了后验，进而减少了有效参数的数量。</p>
</div>
<div class="section" id="id22">
<h3>5.7.3 熵<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id23">
<h4>（1）熵的定义<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h4>
<p>从数学上讲，熵定义为：</p>
<div class="math notranslate nohighlight">
\[H(p)=-\sum_{i} p_i\text{log} (p_i) \tag{式5.15}  \label{式5.15}\]</div>
<p>直观地说，分布越分散，其熵越大。通过运行以下代码并查看图 5.15，可以看到这一点：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">912</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">true_distribution</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">200</span><span class="p">))</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="mi">200</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="n">q_pmf</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">r_pmf</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">dist</span><span class="p">,</span> <span class="n">label</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">zip</span><span class="p">([</span><span class="n">true_distribution</span><span class="p">,</span> <span class="n">q_pmf</span><span class="p">,</span> <span class="n">r_pmf</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;true_distribution&#39;</span><span class="p">,</span> <span class="s1">&#39;q&#39;</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">])):</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;entropy =</span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">handlelength</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<center>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210525131621_8e.webp" /></p>
<p>图 5.15</p>
</center>
<p>如图所示，图中的分布 <span class="math notranslate nohighlight">\(r\)</span> 是三种分布中较广的一个，也是熵最大的一个。建议使用代码并探索熵是如何变化的（参见练习10）。</p>
</div>
<div class="section" id="id24">
<h4>（2） 熵与方差<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h4>
<p>熵和方差概念上相关，以至于很多人将其声明为度量 ”数据分布的方差“ 的一种特殊形式。不过尽管两个概念相关，但本质上并不相同。在某些情况下，熵增加意味着方差增加，例如高斯分布。然而，也存在方差增加但熵不变的例子。例如混合了两个高斯分布的混合模型，当增加两个高斯分布的众数之间距离的时候，导致大部分点到平均值的距离增加了，即方差增大了。但此时，熵受到的影响极小，因为随着众数之间距离的增加，众数之间的点的概率越来越小，因此它们对总熵的贡献逐步可以忽略不计。从熵的角度来看，如果从两个重叠的高斯开始，将一个相对于另一个移动，则在某个点上，将有两个分离的高斯。</p>
</div>
<div class="section" id="id25">
<h4>（3） 最大熵原理<a class="headerlink" href="#id25" title="Permalink to this headline">¶</a></h4>
<p>熵与信息及其不确定性也有关。事实上，更分散或更平坦的先验是弱信息先验。这不仅直观上是正确的，而且有熵的理论支撑。事实上，在贝叶斯学派中有一个群体，在用熵来证明弱信息先验或正则化先验是合理的。这就是<code class="docutils literal notranslate"><span class="pre">最大熵原理</span></code>：<strong>我们总是期望在问题定义的约束下找到具有最大可能熵的分布</strong>。这是一个可用数学方法解决的优化问题，但本书不讲解细节。下面仅列出一些常见约束条件下的最大熵分布：</p>
<ul class="simple">
<li><p>无约束：均匀分布（连续或离散，取决于变量类型）</p></li>
<li><p>正均值：指数分布</p></li>
<li><p>给定方差：正态分布</p></li>
<li><p>只有 2 个非定序输出和 1 个常数均值：二项分布，如果有罕见事件，则为泊松分布（泊松可视为 <span class="math notranslate nohighlight">\(p\)</span> 很小，<span class="math notranslate nohighlight">\(n\)</span> 很大时的二项分布）</p></li>
</ul>
<p>有趣的是，许多传统的广义线性模型（如在 <code class="docutils literal notranslate"><span class="pre">第4章</span> <span class="pre">广义线性模型</span></code>中看到的模型），都是在给定的模型约束下，使用最大熵分布来定义的。</p>
</div>
</div>
<div class="section" id="kl">
<h3>5.7.4 KL 散度<a class="headerlink" href="#kl" title="Permalink to this headline">¶</a></h3>
<p>现在简单谈谈 <code class="docutils literal notranslate"><span class="pre">Kullback-Leibler(KL)散度</span></code>，或简称 <code class="docutils literal notranslate"><span class="pre">KL散度</span></code>。这是在阅读统计学、机器学习、信息论或统计力学文献时经常遇到的概念。你或许会说，<code class="docutils literal notranslate"><span class="pre">KL散度</span></code>、<code class="docutils literal notranslate"><span class="pre">熵</span></code>、<code class="docutils literal notranslate"><span class="pre">边缘似然</span></code>等概念反复出现的原因很简单，因为所有这些学科都在讨论同一组问题，只是观点略有不同。<code class="docutils literal notranslate"><span class="pre">KL散度</span></code> 非常有用，因为<strong>它是衡量两个分布接近程度的一种方法</strong>，其定义如下：</p>
<div class="math notranslate nohighlight">
\[D_{K L}(p \| q)=\sum_{i} p_{i} \log \frac{p_{i}}{q_{i}} \tag{式5.16}  \label{式5.16}\]</div>
<p>上式可读为 <span class="math notranslate nohighlight">\(q\)</span> 到 <span class="math notranslate nohighlight">\(p\)</span> 的 <code class="docutils literal notranslate"><span class="pre">Kullback-Leibler散度</span></code>（两者顺序不能相反，因为 <code class="docutils literal notranslate"><span class="pre">KL散度</span></code> 不符合交换率），其中 <span class="math notranslate nohighlight">\(p\)</span> 和 <span class="math notranslate nohighlight">\(q\)</span> 是两个概率分布。对于连续变量应该计算积分而非求和，但主要思想相同。</p>
<p>可以将 <span class="math notranslate nohighlight">\(D_{KL}({p||q})\)</span> 散度解释为 <strong>”通过使用概率分布 <span class="math notranslate nohighlight">\(q\)</span> 来近似真实分布 <span class="math notranslate nohighlight">\(p\)</span> 而引入的额外熵或不确定性“</strong>。事实上，<code class="docutils literal notranslate"><span class="pre">KL散度</span></code> 是两个熵之间的差值：</p>
<div class="math notranslate nohighlight">
\[D_{K L}(p \| q)=\underbrace{\sum_{i} p_{i} \log p_{i}}_{\text {entropy of p }}-\underbrace{\sum_{i} p_{i} \log q_{i}}_{\text {crossentropy of p,q }}=\sum_{i} p_{i}\left(\log p_{i}-\log q_{i}\right) \tag{式5.17}  \label{式5.17}\]</div>
<p>利用对数性质，可以重新排列式 5.17 以恢复式 5.16。从式 5.17 的角度来看，也可以将 <span class="math notranslate nohighlight">\(D_{KL}({p||q})\)</span> 理解为 <span class="math notranslate nohighlight">\(p\)</span> 相对于 <span class="math notranslate nohighlight">\(q\)</span> 的相对熵(这一次顺着念)。</p>
<p>作为一个简单例子，我们可以使用 KL 散度来评估哪个分布（ <span class="math notranslate nohighlight">\(q\)</span> 或 <span class="math notranslate nohighlight">\(r\)</span> ）更接近真实分布。使用Scipy，可以计算 <span class="math notranslate nohighlight">\(D_{KL}({真实分布||q})\)</span>  和 <span class="math notranslate nohighlight">\(D_{KL}({真实分布||r})\)</span> ：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">true_distribution</span><span class="p">,</span> <span class="n">q_pmf</span><span class="p">),</span> <span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">true_distribution</span><span class="p">,</span><span class="n">r_pmf</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>如果运行上段代码，您将获得 <span class="math notranslate nohighlight">\(\approx 0.0096,\approx 0.7394\)</span> 。因此可以判定，<span class="math notranslate nohighlight">\(q\)</span> 比 <span class="math notranslate nohighlight">\(r\)</span> 更接近于真实分布，因为它引入的额外不确定性更小。我希望您同意我的观点，即这个数值结果与您检查图 5.15 所预期的一致。</p>
<p>您可能很想将 KL 散度描述为距离，但它是不对称的，因此不是真实距离。如果运行下面代码，将获得 <span class="math notranslate nohighlight">\(\approx 2.7,\approx 0.7\)</span> 。由此可见，结果数字是不同的。在此例中，可以看到 <span class="math notranslate nohighlight">\(r\)</span> 是 <span class="math notranslate nohighlight">\(q\)</span> 的更好近似，但反之可能不成立：</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">r_pmf</span><span class="p">,</span> <span class="n">q_pmf</span><span class="p">),</span> <span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">q_pmf</span><span class="p">,</span> <span class="n">r_pmf</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><span class="math notranslate nohighlight">\(D_{KL}({p||q})\)</span> 表示 <span class="math notranslate nohighlight">\(q\)</span> 有多像 <span class="math notranslate nohighlight">\(p\)</span> 。也可从惊喜的角度来考虑，如果在预期 <span class="math notranslate nohighlight">\(p\)</span> 的时候突然看到了 <span class="math notranslate nohighlight">\(q\)</span> ，我们会有多惊讶。对一个事件的惊讶程度取决于用于判断该事件的信息。我在一个非常干旱的城市长大，每年可能会有一两场真正的暴风雨。然后。我搬到另一省份去上大学，我真的很震惊，至少在雨季，平均每周有一场真正的暴风雨！我的一些同学来自布宜诺斯艾利斯，这是阿根廷最潮湿多雨的省份之一。但对他们来说，降雨频率或多或少是意料之中的。更重要的是，他们可能因为空气不够潮湿，而认为天气可能会下多一点雨。</p>
<p>我们也可以使用 KL 散度来比较模型，因为它将给出哪个模型更接近真实分布的后验。但问题是我们并不知道真实分布。因此，KL 散度不能直接适用，但可用它作为论据来修正离差（式5.3）。如果假设真实分布存在（如下式所示），则其应当独立于任何模型和常数，并以同样方式影响 KL 散度，而与用于近似真实分布的后验分布无关。因此，可以使用离差（依赖于每个模型的部分）来估计我们离真实分布相对有多近，即使我们不知道它。对于公式 5.17 ，通过使用一些代数，可以得到：</p>
<p>\begin{align*}
D_{K L}(p | q)-D_{K L}(p | r) &amp;=\left(\sum_{i} p_{i} \log p_{i}-\sum_{i} p_{i} \log q_{i}\right)-\left(\sum_{i} p_{i} \log p_{i}-\sum_{i} p_{i} \log r_{i}\right) \tag{式5.18}  \label{式5.18}\
&amp;=\sum_{i} p_{i} \log q_{i}-\sum_{i} p_{i} \log r_{i} \notag
end{align*}</p>
<p>即使不知道 <span class="math notranslate nohighlight">\(p\)</span>，我们也可以得出结论，具有更大对数似然（或离差）的分布就是在 KL 散度中更接近真实分布的分布。实践中对数似然（或离差）是从有限样本拟合的模型中获得的。因此，还必须增加一个惩罚项，以纠正对离差的高估，这就引出了 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 等信息准则。</p>
</div>
</div>
<div class="section" id="id26">
<h2>5.8 总结<a class="headerlink" href="#id26" title="Permalink to this headline">¶</a></h2>
<p>后验预测性检查是一个通用概念和实践，它可以帮助我们了解模型捕获数据的能力，以及模型捕获我们感兴趣问题的各个方面的能力。我们可以只用一个模型进行后验预测性检查，也可以用多个模型进行后验预测性检查，因此也可以用它作为模型比较的一种方法。后验预测性检查大多是通过可视化完成的，但像 <code class="docutils literal notranslate"><span class="pre">贝叶斯p-value</span></code> 类似的数字摘要也很有帮助。</p>
<p>好的模型在复杂性和预测准确性之间有很好的平衡。我们用多项式回归的经典例子来说明这一特征。我们讨论了两种在不留数据的情况下估计样本外准确度的方法：交叉验证法和信息准则法。我们集中讨论了后者。从实践角度来看，信息准则是一系列平衡两种贡献的方法：一种是衡量模型与数据的拟合程度，另一种是惩罚复杂的模型。在众多可用信息准则中， <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 是贝叶斯模型中最有用的。另一个有用的方法是 <code class="docutils literal notranslate"><span class="pre">PSIS-LOO-CV</span></code> （或 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> ），它在实践中提供了与 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 非常相似的结果。 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 可用于模型选择，也可用于模型平均。模型平均不是选择单个最佳模型，而是通过对所有可用模型进行加权平均来组合所有可用模型。</p>
<p>模型选择、比较和模型平均的另一种方法是贝叶斯因子，它是两个模型的边缘似然之比。贝叶斯因子的计算很有挑战性。本章介绍了使用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 计算它们的两种方法：一种是直接尝试使用离散 index 估计每个模型的相对概率的分层模型，另一种是称为 <code class="docutils literal notranslate"><span class="pre">序贯蒙特卡罗（Sequential</span> <span class="pre">Monte</span> <span class="pre">Carlo，SMC）采样方法</span></code> ，我们建议使用后者。</p>
<p>贝叶斯因子对先验非常敏感，除了在计算上具有挑战性之外，使用贝叶斯因子也是有问题的。我们比较了贝叶斯因子和信息准则，并通过一个示例介绍了它们解决两个相关但不同的问题：一个侧重于确定正确的模型，另一个侧重于实现最佳预测或更低的泛化损失。所有这些方法都或多或少存在问题，但 <code class="docutils literal notranslate"><span class="pre">WAIC</span></code> 和 <code class="docutils literal notranslate"><span class="pre">LOO</span></code> 在实践中相对要健壮得多。</p>
</div>
<div class="section" id="id27">
<h2>5.9 习题<a class="headerlink" href="#id27" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210525214039_d1.webp" /></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="chapter04-GeneralizedLinearRegression.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">第 4 章 广义线性回归模型</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="chapter06-MixtureModels.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">第 6 章 混合模型</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Osvaldo Martin<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>