
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>第1章 概率思维 &#8212; My Jupyter Book</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="第 2 章 概率编程" href="chapter02-ProgrammingProbabilistically.html" />
    <link rel="prev" title="封面" href="preface.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      <h1 class="site-logo" id="site-title">My Jupyter Book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   封面
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   第1章 概率思维
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter02-ProgrammingProbabilistically.html">
   第 2 章 概率编程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter03-ModellingwithLinearRegression.html">
   第 3 章 线性回归模型的贝叶斯视角
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter04-GeneralizedLinearRegression.html">
   第 4 章 广义线性回归模型与分类任务
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter05-ModelComparison.html">
   第 5 章 模型比较
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter06-MixtureModels.html">
   第6章 混合模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter07-GaussianProcesses.html">
   第 7 章 高斯过程
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/chapter01-ThinkingProbabilistically.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   1.1 统计、模型与本书途径
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id3">
     （1）与数据共舞
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     （2）贝叶斯建模
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id5">
   1.2 概率与不确定性
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     1.2.1 解释概率
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     1.2.2 定义概率
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id8">
       （1）概率分布
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id9">
       （2）独立同分布变量
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id10">
       （3）贝叶斯定理与统计推断
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id11">
   1.3 以单参数的贝叶斯推断为例
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     （1）抛硬币问题
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id13">
     （2）通用模型
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     （3）选择似然
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     （4）选择先验
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     （5）计算后验
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id17">
     （6）后验诊断
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     （7）先验的影响以及选择
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id19">
   1.4 报告贝叶斯分析结果
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id20">
     1.4.1 模型注释和可视化
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id21">
     1.4.2 总结后验
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id22">
   1.5 后验预测检验
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id23">
   1.6 总结
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id24">
   1.7 习题
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1>第1章 概率思维<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<blockquote>
<div><p>归根到底，概率论不过是把常识化作计算而已。                                ——皮埃尔—西蒙•拉普拉斯</p>
</div></blockquote>
<p>本章我们将学习贝叶斯统计中的核心概念以及一些用于贝叶斯分析的基本工具。大部分内容都是一些理论介绍，其中会涉及一些Python代码，绝大多数概念会在本书其余章节中反复提到。尽管本章内容有点偏理论，可能会让习惯代码的你感到有点不安，不过这会让你在后面应用贝叶斯统计方法解决问题时容易一些。</p>
<p>本章包含以下主题：</p>
<ul class="simple">
<li><p>统计模型；</p></li>
<li><p>概率及不确定性；</p></li>
<li><p>贝叶斯理论及统计推断；</p></li>
<li><p>单参数推断以及经典的抛硬币问题；</p></li>
<li><p>如何选择先验以及人们为什么不喜欢它；</p></li>
<li><p>如何报告贝叶斯分析结果。</p></li>
</ul>
<div class="section" id="id2">
<h2>1.1 统计、模型与本书途径<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>统计学主要是收集、组织、分析并解释数据，因此，统计学基础知识对数据分析至关重要。数据分析主要使用两种统计方法：</p>
<ul class="simple">
<li><p><strong>探索性数据分析(Exploratory Data Analysis，EDA)</strong>：利用一些数据汇总信息辅助分析的统计方法，主要包括可视化图表、统计摘要两种手段。常见统计摘要信息有均值、众数、标准差等，此类信息也被称为描述性统计；常见可视化手段包括直方图、散点图等。</p></li>
</ul>
<blockquote>
<div><p>假设我们已经有了数据集，通常做法是先探索并可视化，进而能对数据有直观的认识。可通过两步完成探索式数据分析过程：描述性统计、数据可视化。其中：描述性统计指用一些指标或统计值来定量地总结或刻画数据，例如如何用均值、众数、标准差、四分位差等指标来描述数据。数据可视化用生动形象的方式表述数据，直方图、散点图等是常见表现形式。乍一看，探索式数据分析似乎是复杂分析前做的准备工作而不是真正的分析，但它却在理解、解释、检查、总结及交流贝叶斯分析结果过程中非常有用。</p>
</div></blockquote>
<ul class="simple">
<li><p>**推断统计（Inferential Statistics）：**指在当前数据之外做出陈述的统计方法。人们可能想要了解一些特定现象，或者想要对未来(尚未观测到的)数据点做出预测，或者需要对同一观测在几种相互矛盾的解释中做出决策。推断统计提供一套分丰富的方法和工具，帮助我们回答上述类型的问题。</p></li>
</ul>
<blockquote>
<div><p>有时画画图、对数据做些简单计算就够了。但另外一些时候，希望从数据中挖掘出更一般性的结论。例如：人们可能希望了解数据是怎么生成的，也可能想对未来还未观测到的数据做出预测，又或者是希望从多个对观测值的解释中找出最合理的一个，这些正是统计推断所做的事情。统计推断模型分许多种，但依赖的是概率模型，许多科学研究也都基于模型，大脑不过是对现实进行建模的一台机器。</p>
</div></blockquote>
<p>本书的重点是：如何执行贝叶斯推断统计，然后使用EDA来总结、解释、检查和交换贝叶斯推断结果。</p>
<p>大多数统计学课程是作为“食谱集合”来讲授的。大致情形是这样：去 “统计” 食品屋，拿出并打开一个罐头，加些 “数据” 来品尝，搅拌直到你获得一致的<span class="math notranslate nohighlight">\(p\)</span>值，最好在<span class="math notranslate nohighlight">\(0.05\)</span>以下。此类课程主要目标是教你如何挑选罐头。我不喜欢这种方法，因为常让读者更加困扰了。即使在概念上也很难统一学习方法。</p>
<p>本书采取了一种不同方法：同样还将学习一些食谱，但不是速食罐头，而是自制食品；我们将学习如何混合适合不同美食场景的新鲜食材，以便让你理解和应用超出本书示例的大量概念。</p>
<p>采取这种方法是可能的，有两个原因：</p>
<ul class="simple">
<li><p>本体论层面：</p>
<ul>
<li><p>统计学是在概率论数学框架下一种统一的建模形式。使用概率方法为不同方法提供了一个统一视图，统计方法和机器学习方法在概率视角下看起来更相似。</p></li>
</ul>
</li>
<li><p>技术层面：</p>
<ul>
<li><p>像 PyMC3 这样的现代软件，允许实践者以相对简单的方式定义和求解模型。而就在几年前，许多模型还无法求解，或需要非常高的数学和技术复杂度。</p></li>
</ul>
</li>
</ul>
<div class="section" id="id3">
<h3>（1）与数据共舞<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h3>
<p>数据是统计学和数据科学的重要组成部分。数据的主要来源包括：实验、计算机模拟、调查和实地观测等。如果我们是负责产生或收集数据的人，首先应当仔细考虑想要回答的问题和将使用的方法，然后才着手获取数据。事实上，有完整的统计学分支来应对数据收集工作，即所谓实验设计。在数据泛滥的时代，人们有时会忘记，收集数据并不总是便宜的。例如：虽然大型强子对撞机确实每天产生数百太字节，但它的建造需要多年体力和脑力劳动。</p>
<p>一般可以认为数据生成的过程是随机的，因为存在本体论、技术和/或认知上的不确定性。即系统本质上是随机的，存在噪声或者存在限制高精度测量的技术问题，和/或存在一些隐私限制。鉴于此，人们需要在模型的上下文对数据做出解释，包括心理模型和形式模型。<strong>数据不会说话，而是通过模型说话。</strong></p>
<p>本书将假设已经收集了数据，而且数据干净整洁，以便将注意力集中在主题上。但需要强调，虽然本书没有介绍此方面的技能，但读者应当自行学习和练习，以便成功获取和使用数据。</p>
<p>分析数据时的一项非常有用的技能是懂得如何用编程语言（如Python）编写代码。考虑到我们生活在一个数据混乱的世界，处理数据通常是必要的，而编码有助于完成该任务。即使您的数据非常干净整洁，编码仍然非常有用，因为现代贝叶斯统计主要是通过Python或R等编程语言完成。</p>
<p>如果想了解如何使用Python清理和操作数据，推荐您阅读杰克·范德普拉斯的《Python数据科学手册》。</p>
</div>
<div class="section" id="id4">
<h3>（2）贝叶斯建模<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p><strong>模型是对给定系统或流程的简化描述</strong>。这些描述被故意设计成只捕获系统中最相关的方面，而不是解释每个细节。这就是为什么更复杂模型并不总是更好的原因。存在许多不同类型的模型，本书中将仅介绍贝叶斯模型。可以使用三个步骤总结贝叶斯建模过程：</p>
<p>（1）设计模型：给定一些数据以及关于数据生成的假设，通过组合以概率分布为主要内容的模块来设计一个模型。模型是粗略近似的，但通常能够满足需要。</p>
<p>（2）拟合模型：利用贝叶斯理论将数据和模型结合起来，根据数据和假设推导出逻辑结论，人们称之为经数据拟合后的模型。</p>
<p>（3）诊断模型：根据多种标准（包括真实数据和领域专业知识），判断模型拟合得是否合理。</p>
<p>当然，你会发现实际建模过程并非严格按照该顺序进行，有时可能会跳到其中任何一步，原因可能是程序编写错误，也可能是有了新的改进模型，又或者是需要增加更多数据。</p>
<p>**贝叶斯模型是基于概率构建的，因此也称概率模型。**为什么基于概率呢？因为概率能够很好地描述数据中的不确定性，并让我们在满是岔路的花园里不至于迷路。</p>
</div>
</div>
<div class="section" id="id5">
<h2>1.2 概率与不确定性<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h2>
<p>本节不想总结概率论知识体系，只介绍几个对理解贝叶斯方法非常重要的一般性概念。在本书后面部分，将根据需要适度扩展或引入与概率相关的一些概念。要详细学习概率论，建议阅读专门的书籍。推荐约瑟夫·K·布利茨坦（Joseph K Blitzstein）和杰西卡·黄 （Jessica Hwang） 写的《Introduction to Probability》。另一本是渡部住夫的《Mathematical Theory of Bayesian Statistics》，后者比前者更注重贝叶斯，数学方面也更重。</p>
<div class="section" id="id6">
<h3>1.2.1 解释概率<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>虽然概率论是成熟且久负盛名的数学分支，但对概率的解释不止一种。从贝叶斯角度看，概率是一种量化命题不确定性水平的度量。根据这个定义，询问火星上有生命的概率、电子质量为 <span class="math notranslate nohighlight">\(9.1 \times 10^{-31}\)</span> 千克的概率或布宜诺斯艾利斯1816 年 7 月 9 日为晴天的概率是完全有效和自然的。注意，火星上存在或不存在生命，原本是一个结果为“是 ” 或“ 否 ”的二元问题。但考虑到不能完全确定这一事实，明智做法是找出火星上有生命的可能性有多大。</p>
<p>由于概率的这个定义与人类认知心理状态有关，所以也被称为概率的主观定义。但任何有科学头脑的人都不会用自己的信仰来回答这样的问题，而是会使用所有有关火星的地球物理数据、有关生命必备条件的生化知识等。因此，贝叶斯统计和人们拥有的任何其他成熟科学方法一样都既是主观又是客观的。</p>
<p>如果没有关于一个问题的信息，那么所有可能的事件对我们来说有相同的可能性，形式上，这等同于为每个可能的事件分配相同概率。在没有信息的情况下，不确定性是最大的。相反，如果我们知道某些事件更有可能发生，那么可以通过给其分配更高的概率（或给其他事件分配更少的概率）来形式化表示。此外，用统计语言谈论事件时，通常并不局限于可能发生的事情，例如：小行星撞向地球或姑姑60岁的生日派对。事件是一个可以接受任何可能值(或值的集合)的变量，例如：30岁以上的事件，或去年在世界各地售出的自行车数量。</p>
<p>概率的概念与逻辑学学科有关。在亚里士多德或古典逻辑中，只有取值为真或假的语句。在概率贝叶斯定义下，确定性只是一个特例：真陈述的概率为 1，假陈述的概率为 0。只有在有确凿数据表明某些东西正在生长、繁殖和其他生物相关活动后，才会将火星生命概率定为 1。然而，将概率指定为 0 更难，因为几乎总是可以认为：由于实验或观测的问题，导致了得出火星上没有生命这个可能错误的结论。与此相关的是克伦威尔规则，它指出，我们应保留使用先验概率 0 或 1 来处理逻辑上正确或错误的陈述。有趣的是，理查德·考克斯（Richard Cox）在数学上证明，如果想要将逻辑扩展到包含不确定性，必须使用概率和概率理论。贝叶斯定理只是概率规则的一个逻辑推论。因此，贝叶斯统计的另一种思维方式是在处理不确定性时将其作为逻辑的延伸，当然这与贬义意义上的 “主观推理” 无关。</p>
<p>综上所述，用概率来模拟不确定性并不一定与自然界是“确定性”还是“随机性”的争论有关，也不一定与主观个人信念有关。这是一种纯粹用来建模不确定性的方法论。我们认识到大多数现象很难理解，因为通常不得不处理不完整和（或）有噪声的数据，而本质上人类会受到大脑限制。因此，我们使用了一种明确考虑不确定性的建模方法。</p>
</div>
<div class="section" id="id7">
<h3>1.2.2 定义概率<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>概率值介于 [0,1] 之间（包括0和1），其计算遵循一些法则，其中之一是乘法法则：</p>
<div class="math notranslate nohighlight">
\[
p(A, B)=p(A \mid B) p(B) \tag{1.1}
\]</div>
<p>上式的读法是：<span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 同时发生的概率等于 <span class="math notranslate nohighlight">\(B\)</span> 发生的概率乘以在 <span class="math notranslate nohighlight">\(B\)</span> 发生条件下 <span class="math notranslate nohighlight">\(A\)</span> 也发生的概率。其中，<span class="math notranslate nohighlight">\(p(A,B)\)</span> 表示 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 的联合概率，指 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 同时发生的概率； <span class="math notranslate nohighlight">\(p(A|B)\)</span> 表示条件概率 ，指在知识（或证据、事件） <span class="math notranslate nohighlight">\(B\)</span> 支持下，<span class="math notranslate nohighlight">\(A\)</span> 发生的概率。二者的现实意义不同，例如：“路面是湿的” 与 “下雨时路面是湿的” 两个事件的概率截然不同，后者是典型的条件概率。</p>
<p>条件概率 <span class="math notranslate nohighlight">\(p(A|B)\)</span> 可能比原概率 <span class="math notranslate nohighlight">\(p(A)\)</span> 高，也可能低或者相等。从贝叶斯角度，条件概率可以做如下理解：</p>
<ul class="simple">
<li><p>如果 <span class="math notranslate nohighlight">\(B\)</span> 并不能提供任何关于 <span class="math notranslate nohighlight">\(A\)</span> 的信息，那么 <span class="math notranslate nohighlight">\(p(A|B)=p(A)\)</span> ，即暗示 <span class="math notranslate nohighlight">\(A\)</span> 和 <span class="math notranslate nohighlight">\(B\)</span> 是相互独立的。</p></li>
<li><p>如果事件 <span class="math notranslate nohighlight">\(B\)</span> 能够给出有关事件 <span class="math notranslate nohighlight">\(A\)</span> 的信息，那么根据信息不同，事件 <span class="math notranslate nohighlight">\(A\)</span> 可能发生的概率会变得更高或更低。</p></li>
</ul>
<p>以公平六边形骰子为例，如果我们掷骰子，得到数字 3 的可能性有多大？将是六分之一，因为六个数字中的每一个都有相同机会；假设已经知道点数是奇数，得到数字 3 的概率又是多少？是三分之一，因为唯一可能的奇数数字是 <span class="math notranslate nohighlight">\(\{1,3,5\}\)</span> ，每个数字都有相同几率；如果知道出现的是偶数，得到数字 3 的可能性又是多少呢？是零，因为如果一旦知道数字是偶数，那么可能的数字就是 <span class="math notranslate nohighlight">\(\{2,4,6\}\)</span> ，因此得到 3 是不可能的。</p>
<p>上例表明，通过观测数据的条件作用，可以有效地改变事件发生的概率，即事件的不确定性。条件概率是统计学的核心，不管你的问题是掷骰子还是制造自动驾驶汽车。</p>
<div class="section" id="id8">
<h4>（1）概率分布<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h4>
<p>概率分布是一个数学对象，用来描述不同事件发生的可能性有多大。通常这些事件以某种方式被限制为一组可能的事件中，例如：骰子的 <span class="math notranslate nohighlight">\(\{1,2,3,4,5\}\)</span> 。统计学中一个常见且有用的概念是：数据是从某个具有未知参数的真实概率分布中生成的。而<u>推断是从符合真实概率分布的样本（也称为数据集）中找出这些参数值的过程</u>。一般来说，我们无法获得真实的概率分布，因此我们必须想办法以某种方式创建一个具有近似分布的模型。概率模型是通过合理组合概率分布来建立的。</p>
<blockquote>
<div><p>请注意，一般来说不能确定我们的模型是否正确，因此需要评估和批判这些模型，以便获得信心并说服他人相信模型适用于想要探索或解决的问题。</p>
</div></blockquote>
<p>如果一个变量 <span class="math notranslate nohighlight">\(X\)</span> 可以用概率分布来描述，即可以称之为随机变量 <span class="math notranslate nohighlight">\(X\)</span> 。一般来说，使用大写字母（例如 <span class="math notranslate nohighlight">\(X\)</span>） 来表示一个随机变量，并用小写 <span class="math notranslate nohighlight">\(x\)</span> 表示该随机变量的一个实例。<span class="math notranslate nohighlight">\(x\)</span> 可以是包含许多元素或单个值的向量 <span class="math notranslate nohighlight">\(x=\{x_1,x_2,...,x_n\}\)</span> 。</p>
<p>让我们看一个使用 Python 的示例：真实概率分布是均值 <span class="math notranslate nohighlight">\(\mu=0\)</span> 和方差 <span class="math notranslate nohighlight">\(\sigma=1\)</span> 的正态（或高斯）分布；这两个参数完全明确地定义了正态分布。使用 SciPy，可以通过编写 <code class="docutils literal notranslate"><span class="pre">stats.Norm(μ，σ)</span></code> 来定义随机变量，并且使用 <code class="docutils literal notranslate"><span class="pre">rvs</span></code> （random variates的缩写）方法获得一个实例 <span class="math notranslate nohighlight">\(x\)</span> 。而下面示例中，代码要求提供三个值：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">μ</span> <span class="o">=</span> <span class="mf">0.</span>
<span class="n">σ</span> <span class="o">=</span> <span class="mf">1.</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">μ</span><span class="p">,</span> <span class="n">σ</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
</pre></div>
</div>
<p>您会注意到，每次执行此代码（统计术语为：一次试验）时，您将获得不同随机结果。请注意，一旦知道概率分布参数的值，则该分布中每个值的概率就是已知的，随机的是在每次试验中获得的精确值。</p>
<p>关于随机的含义，一个常见误解是：你可以从随机变量中得到任何可能的值，或者认为所有值都是等可能的。</p>
<p>随机变量的可能值及其概率是由概率分布严格控制的，随机性只是因为我们不能预测在每次试验中将要得到的精确值。每次执行前面的代码时，我们都会得到三个不同数字，但如果我们重复代码数千次，将能够经验性地检查采样值的平均值是否在零附近，以及 95% 的样本值是否在 [-1.96，+1.96] 范围内。请不要相信我，用你的 Python 力量自己去验证吧。如果我们研究正态分布的数学性质，我们可以得出同样的结论。</p>
<p>统计学中表示变量随参数 <span class="math notranslate nohighlight">\(\mu,\sigma\)</span> 呈正态分布的形式为：</p>
<div class="math notranslate nohighlight">
\[
x \sim \mathcal{N}(\mu,\sigma) \tag{1.2}
\]</div>
<p>在本书中，当您找到（波浪号）<span class="math notranslate nohighlight">\(\sim\)</span> 符号时，读为：<span class="math notranslate nohighlight">\(x\)</span> 服从…分布。</p>
<blockquote>
<div><p>在许多文本中，常用方差而不是标准差来表示正态分布，记作 <span class="math notranslate nohighlight">\(\mathcal{N}(\mu,\sigma^2)\)</span> 。本书中，我们将使用标准差来做正态分布的参数，首先是因为它更容易解释，其次是因为 PyMC3 是这样工作的。</p>
</div></blockquote>
<p>在本书后面将遇到几个概率分布；每次出现一个概率分布，都会花一点时间来描述它。首先从正态分布开始，因为它类似于概率分布的宙斯。如果变量的值由以下表达式规定，则变量服从高斯分布：</p>
<div class="math notranslate nohighlight">
\[
p(x \mid \mu, \sigma)=\frac{1}{\sigma \sqrt{2 \pi}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}\tag{1.3}
\]</div>
<p>这是正态分布的概率密度函数。不需要记住公式 1.3，此处只想展示给大家看，这样就能知道数字是从哪里来的。正如已经提到的，<span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\sigma\)</span> 是该分布的参数，通过指定这些参数值，我们完全可以定义分布；可以从表达式 1.3 中看到这一点，因为其他项都是常量。<span class="math notranslate nohighlight">\(\mu\)</span> 可以取任何实值，即 <span class="math notranslate nohighlight">\(\mu \in \mathbf{R}\)</span> ，并指定分布的平均值 （以及中位数和模式，它们都是相等的）。<span class="math notranslate nohighlight">\(\sigma\)</span> 是标准差，它只能是正数，表示概率分布的散布情况，值越大分布越分散。因为 <span class="math notranslate nohighlight">\(\mu\)</span> 和 <span class="math notranslate nohighlight">\(\sigma\)</span> 有无限多个可能组合，所以有无限多个高斯分布的实例，并且所有这些实例都属于相同的高斯族。</p>
<p>数学公式简明扼要而且不存在二义性，有人甚至说很漂亮。但必须承认，第一次遇到它可能会让人害怕，特别是那些对数学不太感兴趣的人；打破僵局的一个好方法是使用 Python 来探索它们：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">mu_params</span> <span class="o">=</span> <span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="n">sd_params</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mu_params</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">sd_params</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
        <span class="n">mu</span> <span class="o">=</span> <span class="n">mu_params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">sd</span> <span class="o">=</span> <span class="n">sd_params</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span> <span class="n">sd</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">([],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;μ = </span><span class="si">{:3.2f}</span><span class="se">\n</span><span class="s2">σ = </span><span class="si">{:3.2f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">mu</span><span class="p">,</span>
                     <span class="n">sd</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;p(x)&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">labelpad</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
</pre></div>
</div>
<p>前面的大部分代码用于绘图，概率部分由 <code class="docutils literal notranslate"><span class="pre">y=stats.norm(u，sd).pdf(X)</span></code> 行执行。在给定一组 <span class="math notranslate nohighlight">\(x\)</span> 值的 <span class="math notranslate nohighlight">\(µ\)</span> 和 <span class="math notranslate nohighlight">\(sd\)</span> 参数的情况下，通过这条线，我们可以评估正态分布的概率密度函数。前面的代码生成图1.1。在每个子图中，我们有一条蓝色(深灰色)曲线，表示具有特定参数的高斯分布，并且包含在每个子图的图例中：</p>
<blockquote>
<div><p>本书中大多数数字都是直接从它们前面的代码生成的，很多时候甚至没有将代码与数字联系起来的前导短语。使用 Jupyter 笔记本或 Jupyter Lab 的人应该熟悉这种模式。</p>
</div></blockquote>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210430230432_cf.webp" style="zoom:67%;" />
<p>随机变量有两种类型：连续型和离散型。连续变量可以取某个区间中的任何值(我们可以使用 Python 浮点数来表示它们)，而离散变量只能取某些值（我们可以使用 Python 整数来表示它们）。正态分布是一种连续分布。</p>
<p>请注意，前图中 <code class="docutils literal notranslate"><span class="pre">ytick</span></code> 被省略了；这是一个特性，而不是一个 <code class="docutils literal notranslate"><span class="pre">bug</span></code>。省略它们的原因是，这些值不会添加太多相关信息，但可能会让人感到困惑。解释一下：<span class="math notranslate nohighlight">\(y\)</span> 轴上的实际数字其实并不重要，重要的是它们之间的相对值。如果从中取两个值，比方说和，你会发现(在曲线图中高出两倍)，你可以有把握地说，的值的概率是的概率的两倍。这是大多数人会凭直觉理解的，幸运的是这是正确的解释。在处理连续分布时，唯一棘手的部分是在y轴上绘制的值不是概率，而是概率密度。为了得到一个合适的概率，你必须在给定的区间内积分，也就是说，你必须计算曲线下方的面积(对于该区间)。虽然概率不能大于1，但概率密度可以，并且是概率密度曲线下的总面积限制为1。从数学角度理解概率和概率密度之间的差异是至关重要的。对于本书中使用的实用方法，我们可以稍微草率一点，因为只要你对如何用相对值来解释前面的情节有正确的直觉，这种差别就不那么重要了。</p>
</div>
<div class="section" id="id9">
<h4>（2）独立同分布变量<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h4>
</div>
<div class="section" id="id10">
<h4>（3）贝叶斯定理与统计推断<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h4>
<p>到目前为止，我们已经学习了一些统计学中的基本概念和词汇，接下来让我们首先看看神奇的贝叶斯定理：</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y)=\frac{p(y \mid \theta) p(\theta)}{p(y)}\tag{1.4}
\]</div>
<p>看起来稀松平常，似乎跟小学课本里的公式差不多，不过这就是关于贝叶斯统计你所需要掌握的全部。首先看看贝叶斯定理是怎么来的，这对我们理解它会很有帮助。事实上，我们已经掌握了如何推导它所需要的全部概率论知识。</p>
<p>根据前面提到的概率论中的乘法准则，我们有以下式子：</p>
<div class="math notranslate nohighlight">
\[
p(\theta, y)=p(\theta \mid y) p(y)\tag{1.5}
\]</div>
<p>也可以写为：</p>
<div class="math notranslate nohighlight">
\[
p(\theta, y)=p(y \mid \theta) p(\theta)\tag{1.6}
\]</div>
<p>由于以上式子的左边相等，于是可以得到：</p>
<div class="math notranslate nohighlight">
\[
p ( \theta | y ) p ( y ) = p ( y | \theta ) p ( \theta )\tag{1.7}
\]</div>
<p>对上式调整下顺序，便得到了公式 1.4的贝叶斯定理。</p>
<p>现在，让我们看看这个式子的含义及其重要性。首先，上式表明 <span class="math notranslate nohighlight">\(p(\theta|y)\)</span> 和 <span class="math notranslate nohighlight">\(p(y|\theta)\)</span> 并不一定相等，这一点非常重要，日常分析中即使系统学习过统计学和概率论的人也很容易忽略这点。我们举个简单例子来说明为什么二者不一定相等：有两条腿的动物就是人的概率和人有两条腿的概率显然是不同的。几乎所有人都有两条腿（除了某些人因为先天性原因或者意外导致没有两条腿），但是有两条腿的动物中很多都不是人类，比如鸟类。</p>
<p>在前面的式子中，如果我们将 <span class="math notranslate nohighlight">\(\theta\)</span> 理解为<strong>假设</strong>，将 <span class="math notranslate nohighlight">\(y\)</span> 理解为<strong>数据</strong>，那么贝叶斯定理告诉我们的就是，在给定数据的条件下如何计算假设成立的概率。<strong>如何把假设融入贝叶斯定理中去呢？答案是概率分布</strong>。换句话说，此处假设是一种狭义上的假设，我们所做的实际上是寻找模型的参数（更准确地说是参数的分布）。</p>
<p>贝叶斯定理是贝叶斯统计的核心，正如我们将在第二章中看到的那样，使用 PyMC3 等工具进行概率编程使我们不必在每次构建贝叶斯模型时显式地编写贝叶斯定理。尽管如此，知道其各部分的名称很重要，因为我们将不断引用它们，理解每个部分的含义也很重要，因为这将帮助我们概念化模型，所以让我们这样做：</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(\theta)\)</span>：先验（Prior）；</p></li>
<li><p><span class="math notranslate nohighlight">\(p(y|\theta)\)</span> ：似然（Likelihood）；</p></li>
<li><p><span class="math notranslate nohighlight">\(p(\theta|y)\)</span>：后验（Posterior）；</p></li>
<li><p><span class="math notranslate nohighlight">\(p(y)\)</span>：证据或边际似然（Marginal likelihood）。</p></li>
</ul>
<p>先验分布反映的是在观测到数据之前我们对参数的了解，如果对参数一无所知，那么可以用一个不包含太多信息的均匀分布来表示。由于引入了先验，有些人会认为贝叶斯统计是偏主观的，然而，这些先验不过是构建模型时的一些假设罢了，其主观性跟似然差不多。</p>
<p>似然是指如何在实验分析中引入观测数据，反映的是在给定参数下得到某组观测数据的可信度。</p>
<p>后验分布是贝叶斯分析的结果，反映的是在给定数据和模型的条件下我们对问题的全部认知。需要注意，后验指的是我们模型中参数的概率分布而不是单个值，该分布正比于先验乘以似然。有这么个笑话：贝叶斯学派就像是这样一类人，心里隐约期待着一匹马，偶然间瞥见了一头驴，结果坚信他看到的是一头骡子。当然，如果要刻意纠正这个笑话的话，在先验和似然都比较含糊的情况下，我们会得到一个（模糊的）”骡子”后验。不过，这个笑话也讲出了这样一个道理，后验其实是对先验和似然的某种折中。从概念上讲，后验可以看做是在观测到数据之后对先验的更新。事实上，一次分析中的后验，在收集到新的数据之后，也可以看做是下一次分析中的先验。这使得贝叶斯分析特别适合于序列化的数据分析，比如通过实时处理来自气象站和卫星的数据从而提前预警灾害，更详细的内容可以阅读在线机器学习方面的算法。</p>
<p>最后一个概念是证据，也称作边际似然。正式地讲，证据是在模型参数取遍所有可能值的条件下得到指定观测值的概率的平均。不过，本书大部分内容并不关心这个概念，可以简单地把它当作归一化系数。我们只关心参数的相对值而非绝对值。把证据这一项忽略掉之后，贝叶斯定理可以表示成如下正比例形式：</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y) \propto p(y \mid \theta) p(\theta) \tag{1.8}
\]</div>
<p>理解其中的每个概念可能需要时间和更多的例子，本书也将围绕这些内容展开。</p>
</div>
</div>
</div>
<div class="section" id="id11">
<h2>1.3 以单参数的贝叶斯推断为例<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h2>
<p>前面学习了几个重要概念，其中有两个是贝叶斯统计的核心概念，这里我们用一句话再重新强调下：<strong>概率是用来衡量我们对参数的不确定性的，而贝叶斯定理是根据新数据正确更新这些概率的机制，有望减少不确定性。</strong></p>
<p>现在已经知道什么是贝叶斯统计了，接下来从简单例子入手，通过推断单个未知参数来学习如何进行贝叶斯统计。</p>
<div class="section" id="id12">
<h3>（1）抛硬币问题<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p>抛硬币是统计学中的一个经典问题，其描述如下：我们随机抛一枚硬币，重复一定次数，记录正面朝上和反面朝上的次数，根据这些数据，我们需要回答诸如这枚硬币是否公平，以及更进一步这枚硬币有多不公平等问题。抛硬币是一个学习贝叶斯统计非常好的例子，一方面是因为几乎人人都熟悉抛硬币这一过程，另一方面是因为这个模型很简单，我们可以很容易计算并解决这个问题。此外，许多真实问题都包含两个互斥的结果，例如0或者1、正或者负、奇数或者偶数、垃圾邮件或者正常邮件、安全或者不安全、健康或者不健康等。因此，即便我们讨论的是硬币，该模型也同样适用于前面这些问题。</p>
<p>为了估计硬币的偏差，或者更广泛地说，想要用贝叶斯学派理论解决问题，我们需要数据和一个概率模型。对于抛硬币这个问题，假设我们已试验了一定次数并且记录了正面朝上的次数，也就是说数据部分已经准备好了，剩下的就是模型部分了。考虑到这是第一个模型，我们会列出所有必要的数学公式，并且一步一步推导。下一章中，我们会重新回顾这个问题，并借用PyMC3从数值上解决它（也就是说那部分不需要手动推导，而是利用PyMC3和计算机来完成）。</p>
</div>
<div class="section" id="id13">
<h3>（2）通用模型<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h3>
<p>首先，我们要抽象出偏差的概念。我们称，如果一枚硬币总是正面朝上，那么它的偏差就是 1，反之，如果总是反面朝上，那么它的偏差就是 0，如果正面朝上和反面朝上的次数各占一半，那么它的偏差就是0.5。这里用参数 <span class="math notranslate nohighlight">\(θ\)</span> 来表示偏差，用 <span class="math notranslate nohighlight">\(y\)</span> 表示 <span class="math notranslate nohighlight">\(N\)</span> 次抛硬币实验中正面朝上的次数。根据贝叶斯定理，需要指定先验 <span class="math notranslate nohighlight">\(p(\theta)\)</span> 和似然 <span class="math notranslate nohighlight">\(p(y|\theta)\)</span> 分别是什么。让我们首先从似然开始。</p>
</div>
<div class="section" id="id14">
<h3>（3）选择似然<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p>假设多次抛硬币的结果相互之间没有影响，也就是说每次抛硬币都是相互独立的，同时还假设结果只有两种可能：正面朝上或者反面朝上。基于这些假设，一个不错的似然候选是二项分布：</p>
<div class="math notranslate nohighlight">
\[
p(y \mid \theta, N)=\frac{N !}{y !(N-y) !} \theta^{y}(1-\theta)^{N-y} \tag{1.9}
\]</div>
<p>这是一个离散分布，表示给定某个参数值 <span class="math notranslate nohighlight">\(\theta\)</span> ， <span class="math notranslate nohighlight">\(N\)</span> 次抛硬币实验中 <span class="math notranslate nohighlight">\(y\)</span> 次正面朝上的概率（或者更通俗地描述是，<span class="math notranslate nohighlight">\(N\)</span> 次实验中，<span class="math notranslate nohighlight">\(y\)</span> 次成功的概率）。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">n_params</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">4</span><span class="p">]</span>  <span class="c1"># Number of trials</span>
<span class="n">p_params</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.25</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">]</span>  <span class="c1"># Probability of success</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="p">(</span><span class="n">n_params</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span><span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_params</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">p_params</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">n_params</span><span class="p">)):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">p_params</span><span class="p">)):</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">n_params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">p_params</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">colors</span><span class="o">=</span><span class="s1">&#39;C0&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;N = </span><span class="si">{:3.2f}</span><span class="se">\n</span><span class="s2">θ =</span>
                           <span class="p">{:</span><span class="mf">3.2</span><span class="n">f</span><span class="p">}</span><span class="s2">&quot;.format(n,p), alpha=0)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;p(y | θ, N)&#39;</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505153120_1a.webp" style="zoom:67%;" />
<p>二项分布是似然的一个合理选择，直观上讲，<span class="math notranslate nohighlight">\(θ\)</span> 可以看作抛一次硬币时正面朝上的可能性，并且该过程发生了 <span class="math notranslate nohighlight">\(y\)</span> 次。类似地，可以把 <span class="math notranslate nohighlight">\(1−θ\)</span> 看作抛一次硬币时反面朝上的概率，并且该过程发生了 <span class="math notranslate nohighlight">\(N−y\)</span> 次。</p>
<p>假如知道了 <span class="math notranslate nohighlight">\(θ\)</span>，就可以从二项分布得出硬币正面朝上的分布。如果我们不知道 <span class="math notranslate nohighlight">\(θ\)</span> 也别灰心，在贝叶斯统计中，当不知道某个参数的时候，就对其赋予一个先验。</p>
</div>
<div class="section" id="id15">
<h3>（4）选择先验<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>这里我们选用贝叶斯统计中最常见的贝塔分布作为先验，其数学形式如下：</p>
<div class="math notranslate nohighlight">
\[
p(\theta)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1} \tag{1.10}
\]</div>
<p>仔细观察上面的式子可以看出，除了 <span class="math notranslate nohighlight">\(\Gamma\)</span> 部分之外，贝塔分布和二项分布看起来很像。 <span class="math notranslate nohighlight">\(\Gamma\)</span> 是希腊字母中大写的伽马，用来表示伽马函数。现在只需要知道，用分数表示的第一项是一个正则化常量，用来保证该分布的积分为1，此外，<span class="math notranslate nohighlight">\(\alpha\)</span> 和 <span class="math notranslate nohighlight">\(\beta\)</span> 两个参数用来控制具体分布形态。贝塔分布是到目前为止见到的第3个分布，利用下面的代码，我们可以深入了解其形态：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">params</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">),</span> <span class="n">sharex</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                     <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
        <span class="n">a</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">b</span> <span class="o">=</span> <span class="n">params</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;α = </span><span class="si">{:2.1f}</span><span class="se">\n</span><span class="s2">β = </span><span class="si">{:2.1f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">a</span><span class="p">,</span>
                     <span class="n">b</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">ax</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
        
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_yticks</span><span class="p">([])</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xticks</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="n">f</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="s1">&#39;θ&#39;</span><span class="p">,</span> <span class="n">ha</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">)</span>
<span class="n">f</span><span class="o">.</span><span class="n">text</span><span class="p">(</span><span class="mf">0.07</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="s1">&#39;p(θ)&#39;</span><span class="p">,</span> <span class="n">va</span><span class="o">=</span><span class="s1">&#39;center&#39;</span><span class="p">,</span> <span class="n">rotation</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505153642_8f.webp" style="zoom:67%;" />
<p>为什么要在模型中使用贝塔分布呢？在抛硬币以及一些其他问题中使用贝塔分布的原因之一是：贝塔分布的范围限制在0 到 1 之间，这跟我们的参数一样；另一个原因是其通用性，从前面的图可以看出，该分布可以有多种形状，包括均匀分布、类高斯分布、U型分布等。第3个原因是：贝塔分布是二项分布的共轭先验。似然的共轭先验是指，将该先验分布与似然组合在一起之后，得到的后验分布与先验分布的表达形式仍然是一样的。简单说，就是每次用贝塔分布作为先验、二项分布作为似然时，我们会得到一个贝塔分布的后验。除贝塔分布之外还有许多其他共轭先验，例如：高斯分布的共轭先验就是自己。共轭确保了后验的数学可处理性，这一点很重要，因为贝叶斯统计学中的一个常见问题是无法分析解决后验。在开发合适的计算方法来解决后验概率分布问题之前，共轭先验是一项重大突破。从第二章概率编程开始，我们将学习如何使用现代计算方法来解决贝叶斯问题，无论我们是否选择共轭先验。</p>
</div>
<div class="section" id="id16">
<h3>（5）计算后验<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>首先回忆一下贝叶斯定理：后验正比于似然乘以先验。对于我们的问题，必须将二项分布和贝塔分布相乘：</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y) \propto \frac{N !}{y !(N-y) !} \theta^{y}(1-\theta)^{N-y} \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} \theta^{\alpha-1}(1-\theta)^{\beta-1}\tag{1.11}
\]</div>
<p>可以简化该表达式。为了实际考虑，可以去掉所有不依赖的元素，结果仍然有效。相应地，可以这样写：</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y) \propto \theta^{y}(1-\theta)^{N-y} \theta^{\alpha-1}(1-\theta)^{\beta-1}\tag{1.12}
\]</div>
<p>重新整理之后得到：</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y) \propto \theta^{y+\alpha-1}(1-\theta)^{N-y+\beta-1}\tag{1.13}
\]</div>
<p>如果注意会发现，上述表达式与贝塔分布具有相同的函数形式（除了归一化项），其中，参数 <span class="math notranslate nohighlight">\( \alpha_{\text {posterior }}=\alpha_{\text {prior }}+y \)</span>  ， 参数 <span class="math notranslate nohighlight">\(\beta_{\text {posterior}}=\beta_{\text {prior }}+N-y\)</span> 。事实上，问题的后验分布依然是贝塔分布：</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y) \propto \operatorname{Beta}\left(\alpha_{\text {prior }}+y, \beta_{\text {prior }}+N-y\right)\tag{1.14}
\]</div>
</div>
<div class="section" id="id17">
<h3>（6）后验诊断<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>现在已经有了后验的表达式，我们可以用 Python 对其计算并画出结果。下面的代码中，其实只有一行是用来计算后验结果的，其余的代码都是用来画图的：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">n_trials</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">50</span><span class="p">,</span> <span class="mi">150</span><span class="p">]</span>
<span class="n">data</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">13</span><span class="p">,</span> <span class="mi">48</span><span class="p">]</span>
<span class="n">theta_real</span> <span class="o">=</span> <span class="mf">0.35</span>
<span class="n">beta_params</span> <span class="o">=</span> <span class="p">[(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)]</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">200</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">N</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">n_trials</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">idx</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;θ&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">idx</span><span class="o">+</span><span class="mi">3</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">for</span> <span class="p">(</span><span class="n">a_prior</span><span class="p">,</span> <span class="n">b_prior</span><span class="p">)</span> <span class="ow">in</span> <span class="n">beta_params</span><span class="p">:</span>
        <span class="n">p_theta_given_y</span> <span class="o">=</span> <span class="n">dist</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a_prior</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">b_prior</span> <span class="o">+</span> <span class="n">N</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span>
        <span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">p_theta_given_y</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">theta_real</span><span class="p">,</span> <span class="n">ymax</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">N</span><span class="si">:</span><span class="s1">4d</span><span class="si">}</span><span class="s1"> trials</span><span class="se">\n</span><span class="si">{</span><span class="n">y</span><span class="si">:</span><span class="s1">4d</span><span class="si">}</span><span class="s1"> heads&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">12</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505154649_0f.webp" /></p>
<p>在上图的第一行中，实验的次数为0，因此第一个图中的曲线描绘的是先验分布，其中有3条曲线，每条曲线分别表示一种先验。</p>
<ul class="simple">
<li><p>蓝色的线是一个均匀分布先验，其含义是：偏差的所有可能取值都是等概率的。</p></li>
<li><p>红色的线与均匀分布有点类似，对抛硬币这个例子而言可以理解为：偏差等于0或者1的概率要比其他值更大一些。</p></li>
<li><p>绿色的线集中在中间值0.5附近，该分布反映了通常硬币正面朝上和反面朝上的概率大致是差不多的。我们也可以称，该先验与大多数硬币都是公平的这一信念是兼容的。”兼容”这个词在贝叶斯相关的讨论中会经常用到，特别是在提及受到数据启发的模型时。</p></li>
</ul>
<p>剩余的子图描绘了后续实验的后验分布，回想一下，后验可以看做是在给定数据之后更新了的先验。实验（抛硬币）的次数和正面朝上的次数分别标注在每个子图中。此外每个子图中在横轴 0.35 附近还有一个黑色的竖线，表示的是真实的 <span class="math notranslate nohighlight">\(θ\)</span> ，显然，在真实情况下，我们并不知道该值，在这里标识出来只是为了方便理解。从这幅图中可以学到很多贝叶斯分析方面的知识：</p>
<ul class="simple">
<li><p>贝叶斯分析的结果是后验分布而不是某个值，该分布描述了根据给定数据和模型得到的不同数值的可能性。</p></li>
<li><p>后验最可能的值是根据后验分布的形态决定的（也就是后验分布的峰值）。</p></li>
<li><p>后验分布的离散程度与我们对参数的不确定性相关；分布越离散，不确定性越大。尽管1/2=4/8=0.5，但从图中可以看出，前者的不确定性要比后者大。这是因为我们有了更多的数据来支撑我们的推断，该直觉也同时反映在了后验分布上。</p></li>
<li><p>在给定足够多的数据时，两个或多个不同先验的贝叶斯模型会趋近于收敛到相同的结果。在极限情况下，如果有无限多的数据，不论我们使用的是怎样的先验，最终都会得到相同的后验。注意这里说的无限多是指某种程度而非某个具体的数量，也就是说，从实际的角度来讲，某些情况下无限多的数据可以通过比较少量的数据近似。</p></li>
<li><p>不同后验收敛到相同分布的速度取决于数据和模型。从前面的图中可以看出，蓝色和红色的后验在经过8次实验之后就很难看出区别了，而红色的曲线则一直到150次实验之后才与另外两个后验看起来比较接近。</p></li>
<li><p>有一点从前面的图中不太容易看出来，如果我们一步一步地更新后验，最后得到的结果跟一次性计算得到的结果是一样的。换句话说，可以对后验分150次计算，每次增加一个新的观测数据并将得到的后验作为下一次计算的先验，也可以在得到150次抛硬币的结果之后一次性计算出后验，而这两种计算方式得到的结果是完全一样的。这个特点非常有意义，当我们获得新数据时，这会导致我们以一种自然的方式更新估计，这种情况在许多数据分析问题中都很常见。</p></li>
</ul>
</div>
<div class="section" id="id18">
<h3>（7）先验的影响以及选择<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>从前面的例子可以清楚地看出，先验可以影响推断。这完全没问题，前科应该这么做的。贝叶斯分析的新手(以及这种范式的批评者)通常对如何选择先验感到有点紧张，因为他们不想让先验充当一个不让数据自己说话的审查员！这没问题，但我们必须记住，数据并不能真正说话；充其量，数据只是杂音。只有在我们的模型(包括数学模型和心理模型)的上下文中，数据才有意义。在科学史上，同样的数据导致人们对相同的主题有不同的思考的例子很多，即使你的观点建立在正式的模型上，这种情况也可能发生。</p>
<p>有些人青睐于使用没有信息量的先验（也称作均匀的、含糊的或者发散的先验），这类先验对分析过程的影响最小。本书将遵循 Gelman、McElreath和Kruschke 三人的建议，更倾向于使用带有较弱信息量的先验。在许多问题中，我们对参数可以取的值一般都会有些了解，比如，参数只能是正数，或者知道参数近似的取值范围，又或者是希望该值接近 0 或大于/小于某值。这种情况下，可以给模型加入一些微弱的先验信息而不必担心该先验会掩盖数据本身的信息。由于这类先验会让后验近似位于某一合理的边界内，因此也被称作正则化先验。</p>
<p>当然，使用带有较多信息量的强先验也是可行的。视具体的问题不同，有可能很容易或者很难找到这类先验，例如在我工作的领域（结构生物信息学），人们会尽可能地利用先验信息，通过贝叶斯或者非贝叶斯的方式来了解和预测蛋白质的结构。这样做是合理的，原因是我们在数十年间已经从上千次精心设计的实验中收集了数据，因而有大量可信的先验信息可供使用。如果你有可信的先验信息，完全没有理由不去使用。试想一下，如果一个汽车工程师每次设计新车的时候，他都要重新发明内燃机、轮子乃至整个汽车，这显然不是正确的方式。</p>
<p>现在我们知道了先验有许多种，不过这并不能缓解我们选择先验时的焦虑。或许，最好是没有先验，这样事情就简单了。不过，不论是否基于贝叶斯，模型都在某种程度上拥有先验，即使这里的先验并没有明确表示出来。事实上，许多频率统计学方面的结果可以看做是贝叶斯模型在一定条件下的特例，比如均匀先验。让我们再仔细看看前面那幅图，可以看到蓝色后验分布的峰值与频率学分析中 <span class="math notranslate nohighlight">\(θ\)</span> 的期望值是一致的：</p>
<div class="math notranslate nohighlight">
\[
\hat \theta = \frac { y } { N }\tag{1.15}
\]</div>
<p>注意，这里 <span class="math notranslate nohighlight">\(\hat \theta\)</span>是点估计而不是后验分布（或者其他类型的分布）。由此看出，你没办法完全避免先验，不过如果你在分析中引入先验，得到的会是概率分布而不只是最可能的一个值。</p>
<p>明确引入先验的另一个好处是，我们会得到更透明的模型，这意味着更容易评判、调试以及优化。构建模型是一个循序渐进的过程，有时候可能只需要几分钟，有时候则可能需要数年；有时候整个过程可能只有你自己，有时候则可能包含你不认识的人。而且，模型复现很重要，而模型中透明的假设能有助于复现。</p>
<p>在特定分析任务中，如果我们对某个先验或者似然不确定，可以自由使用多个先验或者似然进行尝试。模型构建过程中的一个环节就是质疑假设，而先验就是质疑的对象之一。不同的假设会得到不同的模型，根据数据和与问题相关的领域知识，我们可以对这些模型进行比较，本书第5章模型比较部分会深入讨论该内容。</p>
<p>由于先验是贝叶斯统计中的一个核心内容，在接下来遇到新的问题时我们还会反复讨论它，因此，如果你对前面的讨论内容感到有些疑惑，别太担心，要知道人们在这个问题上已经困惑了数十年并且相关的讨论一直在继续。</p>
</div>
</div>
<div class="section" id="id19">
<h2>1.4 报告贝叶斯分析结果<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h2>
<p>创建报告和交流结果是统计和数据科学实践的核心。在本节中，我们将简要讨论使用贝叶斯模型时此任务的一些特点。在接下来的章节中，我们将继续看有关这一重要问题的例子。</p>
<div class="section" id="id20">
<h3>1.4.1 模型注释和可视化<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h3>
<p>根据受众不同，你可能在交流分析结果的同时还需要交流模型。以下是一种简单表示概率模型的常见方式：</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{equation}
\begin{aligned}
\theta &amp;\sim \operatorname{Beta}(\alpha, \beta) \\
y &amp;\sim \operatorname{Bin}(n=1, p=\theta) 
\end{aligned}
\end{equation}
\end{split}\]</div>
<p>这是抛硬币例子中用到的模型。符号～表示左边随机变量的分布服从右边的分布形式，也就是说，这里 <span class="math notranslate nohighlight">\(θ\)</span> 服从于参数为<span class="math notranslate nohighlight">\(α\)</span> 和 <span class="math notranslate nohighlight">\(β\)</span> 的贝塔分布，而 <span class="math notranslate nohighlight">\(y\)</span> 服从于参数为 <span class="math notranslate nohighlight">\(n=1\)</span> 和 <span class="math notranslate nohighlight">\(p=θ\)</span> 的二项分布。该模型还可以用类似 Kruschke 书中的图表示成如下形式：</p>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505155942_31.webp" style="zoom:67%;" />
<p>在第一层，根据先验生成了 <span class="math notranslate nohighlight">\(θ\)</span> ，然后通过似然生成最下面的数据。图中的箭头表示变量之间的依赖关系，符号～表示变量的随机性。</p>
</div>
<div class="section" id="id21">
<h3>1.4.2 总结后验<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h3>
<p>贝叶斯分析的结果是后验分布，该分布包含了有关参数在给定数据和模型下的所有信息。如果可能的话，我们只需要将后验分布展示给观众即可。通常，一个不错的做法是：同时给出后验分布的均值（或者众数、中位数），这样能让我们了解该分布的中心，此外还可以给出一些描述该分布的衡量指标，如标准差，这样人们能对我们估计的离散程度和不确定性有一个大致的了解。拿标准差衡量类似正态分布的后验分布很合适，不过对于一些其他类型的分布（如偏态分布）却可能得出误导性结论，因此，我们还可以采用下面的方式衡量。</p>
<p>一个经常用来描述后验分布分散程度的概念是最大后验密度（Highest Posterior Density，HPD）区间。一个HPD区间是指包含一定比例概率密度的最小区间，最常见的比例是 95%HPD 或 98%HPD，通常还伴随着一个 50%HPD。如果我们说某个分析的HPD区间是 [2,5]，其含义是指：根据我们的模型和数据，参数位于2～5的概率是0.95。</p>
<blockquote>
<div><p>这是一个非常直观的解释，以至于人们经常会将频率学中的置信区间与贝叶斯方法中的可信区间弄混淆。如果你对频率学的范式比较熟悉，请注意这两种区间的区别。贝叶斯学派的分析告诉我们的是参数取值的概率，这在频率学的框架中是不可能的，因为频率学中的参数是固定值，频率学中的置信区间只能包含或不包含参数的真实值。在继续深入之前，有一点需要注意：选择95%还是50%或者其他什么值作为HPD区间的概率密度比例并没有什么特殊的地方，这些不过是经常使用的值罢了。比如，我们完全可以选用比例为91.37%的HPD区间。如果你选的是95%，这完全没问题，只是要记住这只是个默认值，究竟选择多大比例仍然需要具体问题具体分析。</p>
</div></blockquote>
<p>ArviZ是一个Python包，用于贝叶斯模型的探索性数据分析。ArviZ有许多函数可以帮助我们总结后验结果，例如，az.plot_posterior 可以用来生成具有分布平均值和HPD的曲线图。在下面的示例中，我们从贝塔分布生成随机样本，而不是真实分析的后验结果。注意，图中报告的HPD为94%。ArviZ每次计算和报告HPD时，默认情况下将使用值0.94(对应于94%)。可以通过向 trusted_interval 参数传递不同值来更改此设置。</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">az</span><span class="o">.</span><span class="n">plot_posterior</span><span class="p">({</span><span class="s1">&#39;θ&#39;</span><span class="p">:</span><span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">)})</span>
</pre></div>
</div>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505160529_37.webp" style="zoom:67%;" />
</div>
</div>
<div class="section" id="id22">
<h2>1.5 后验预测检验<a class="headerlink" href="#id22" title="Permalink to this headline">¶</a></h2>
<p>获取特定模型的后验分布后，可以用它模拟基于该分布的新数据，这有助于评估模型是否提供了有效预测，对未来事件进行推断。这些模拟可用于多种目的，比如：通过对比观测数据和模拟数据的核密度估计值来检验模拟数据是否类似于观测数据。在评估模型是否与数据生成机制有较好的拟合时，需要更正式的后验预测检验方法。任何参数依赖的统计或差异都可用于后验预测检验。这与先验预测检验的使用方式类似，但在对比观测数据和模拟数据时要更加严苛。</p>
<p>###（1） 后验预测分布
贝叶斯方法的一个优势是：一旦得到了后验分布 <span class="math notranslate nohighlight">\(p(\theta|y)\)</span> ，就可以根据该后验生成未来的数据 <span class="math notranslate nohighlight">\( \hat y \)</span> ，即用来做预测。而后验预测分布是条件预测相对于后验分布的平均值。
$<span class="math notranslate nohighlight">\(
p(\hat{y} \mid y)=\int p(\hat{y} \mid \theta) p(\theta \mid y) d \theta \tag{1.17}
\)</span>$
从概念上(和计算上)，我们将此积分1.17近似为迭代的两步过程：</p>
<ul class="simple">
<li><p>从后验 <span class="math notranslate nohighlight">\(p(\theta|y)\)</span>  取样 <span class="math notranslate nohighlight">\(\theta\)</span> 值，</p></li>
<li><p>将 <span class="math notranslate nohighlight">\(\theta\)</span> 值馈给似然（如果愿意，也可以是抽样分布），获得一个数据点 <span class="math notranslate nohighlight">\(\hat y\)</span> 。</p></li>
</ul>
<p>请注意该过程如何组合两个不确定性来源：参数不确定性（由后验捕获）；以及采样不确定性（由似然捕获）。</p>
<p>###（2）后验预测检验
当我们需要进行预测时，可以根据上面两个过程获得预测。但也可以用其来比较观测数据和预测数据，进而对模型做出评判，以找出两组数据间的差异，此即为后验预测检验，检验的主要目标是两者的一致性。</p>
<p>生成的数据和观测的数据应该看起来差不多，否则有可能是建模出现了问题或者输入数据到模型时出了问题。不过就算没有出错，两个集合仍然可能出现不同。尝试去理解其中的偏差有助于我们改进模型，或者至少能知道模型的极限。即使不知道如何改进模型，能够理解模型或数据捕获或未捕获的问题也非常有价值。模型也许能够捕获到均值但却无法预测异常值，这可能是个问题，但当我们只关心均值时，模型对我们而言还是可用的。
换句话说，总体目标不是宣布某个模型是错误的，而是想了解模型哪一部分是可信任的，并尝试检验该模型是否适合特定目的。一个人对一个模型有多大信心，在不同学科间肯定不一样。物理学可以使用高级理论在高度受控条件下研究系统，因此模型通常被视为对现实的很好描述；而其他学科研究复杂的、难以孤立的系统时，模型通常具有较弱认识论地位。尽管如此，无论从事哪个学科，都应始终对模型做检验，后验预测检验和探索性数据分析的想法是检验模型的好方法。</p>
</div>
<div class="section" id="id23">
<h2>1.6 总结<a class="headerlink" href="#id23" title="Permalink to this headline">¶</a></h2>
<p>我们的贝叶斯之旅首先围绕统计建模、概率论和贝叶斯定理做了一些简短讨论，然后用抛硬币的例子介绍了贝叶斯建模和数据分析，借用这个经典例子传达了贝叶斯统计中的一些最重要的思想，比如用概率分布构建模型并用概率分布来表示不确定性。此外我们尝试揭示了如何选择先验，并将其与数据分析中的一些其他问题置于同等地位（怎么选择似然，为什么要解决该问题等）。本章的最后讨论了如何解释和报告贝叶斯分析的结果。本章我们对贝叶斯分析的一些主要方面做了简要总结，后面还会重新回顾这些内容，从而充分理解和吸收，并为后面理解更高级的内容打下基础。</p>
<p>下图以渡边住友的工作流程为基础，总结了本章所述的贝叶斯工作流程：</p>
<img src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505163234_92.webp" style="zoom:67%;" />
<p><img alt="img" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210507110636_90.webp" /></p>
<p>我们假设存在一个总体未知的真分布，从该分布中可以通过实验、调查、观察或模拟获得有限样本。为了从真分布中学到一些东西，假设我们只观察到一个样本，建立了一个概率模型。概率模型由两个部分组成：先验和似然。我们使用模型和样本进行贝叶斯推断，得到一个后验分布；这个分布封装了给定模型和数据的所有关于问题的信息。从贝叶斯的角度来看，后验分布是主要感兴趣的对象，其他一切都是从它衍生出来的，包括后验预测分布形式的预测。由于后验分布（以及由此得出的任何其他量）是模型和数据的结果，因此贝叶斯推断的有用性受到模型和数据质量的限制。评估模型的一种方法是将后验预测分布与之前获得的有限样本进行比较。请注意，后验分布是模型中参数的分布（以观测样本为条件），而后验预测分布是预测样本的分布（在后验分布上平均）。模型验证过程至关重要，不是因为想要确保拥有正确模型，而是因为我们知道几乎从来没有正确的模型。我们检查模型以评估它们在特定上下文中是否足够有用，如果不是，则深入了解如何改进它们。</p>
<p>在本章中，我们简要总结了进行贝叶斯数据分析的主要方面。在本书的其余部分，我们将重新审视这些想法，以便真正吸收它们，并将它们用作更高级概念的脚手架。在下一章中，我们将介绍 PyMC3，这是一个用于贝叶斯建模和概率机器学习的Python库，以及ArviZ，它是一个用于贝叶斯模型探索性分析的Python库。</p>
</div>
<div class="section" id="id24">
<h2>1.7 习题<a class="headerlink" href="#id24" title="Permalink to this headline">¶</a></h2>
<p>我们尚不清楚大脑是如何运作的，是按照贝叶斯方式？还是类似贝叶斯的某种方式？又或许是进化过程中形成的某种启发式的方式？不管如何，我们至少知道自己是通过数据、例子和练习来学习的，尽管你可能对此有不同的意见，不过我仍然强烈建议你完成以下练习。</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505165349_d4.webp" /></p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/spatialPresent_20210505165408_07.webp" /></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="preface.html" title="previous page">封面</a>
    <a class='right-next' id="next-link" href="chapter02-ProgrammingProbabilistically.html" title="next page">第 2 章 概率编程</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By The Jupyter Book community<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>