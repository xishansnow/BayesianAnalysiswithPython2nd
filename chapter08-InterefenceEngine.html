
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>第 8 章 推断引擎 &#8212; Python贝叶斯分析(中文)</title>
    
  <link href="_static/css/theme.css" rel="stylesheet" />
  <link href="_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="第 9 章 下一步去哪儿？" href="chapter09-WheretoGoNext.html" />
    <link rel="prev" title="第 7 章 高斯过程" href="chapter07-GaussianProcesses.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Python贝叶斯分析(中文)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="preface.html">
   封面
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="chapter01-ThinkingProbabilistically.html">
   第 1 章 概率思维
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter02-ProgrammingProbabilistically.html">
   第 2 章 概率编程
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter03-ModellingwithLinearRegression.html">
   第 3 章 线性回归模型的贝叶斯视角
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter04-GeneralizedLinearRegression.html">
   第 4 章 广义线性回归模型与分类任务
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter05-ModelComparison.html">
   第 5 章 模型比较
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter06-MixtureModels.html">
   第 6 章 混合模型
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter07-GaussianProcesses.html">
   第 7 章 高斯过程
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   第 8 章 推断引擎
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="chapter09-WheretoGoNext.html">
   第 9 章 下一步去哪儿？
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ref-chapter08-VI-VIforDummies.html">
   变分推断傻瓜书
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ref-chapter08-VI-ADVI.html">
   自动微分变分推断
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ref-chapter08-MC-Integration.html">
   蒙特卡洛积分
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ref-chapter08-MC-MCMCforDummies.html">
   MCMC采样的傻瓜书
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/chapter08-InterefenceEngine.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   8.1 几类推断引擎
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   8.2 非马尔可夫方法
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     8.2.1 网格计算法
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     8.2.2 二次近似法
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     8.2.3 变分方法
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id7">
     8.2.4 自动微分变分推断方法
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id8">
   8.2 马尔科夫方法
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     8.3.1 蒙特卡洛
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id10">
     8.3.2 马尔科夫链
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#metropolis-hastings">
     8.3.3 Metropolis-Hastings 算法
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id11">
     8.3.4 汉密尔顿蒙特卡洛方法/不掉向采样
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id12">
     8.3.5 序贯蒙特卡洛
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id13">
   8.4 诊断样本
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id14">
     8.4.1 收敛性
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id15">
     8.4.2 蒙特卡洛误差
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id16">
     8.4.3 自相关
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id17">
     8.4.4 有效样本量
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id18">
     8.4.5 发散性
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id19">
     8.4.6 非居中参数化
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id20">
   8.5 小结
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id21">
   练习
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="id1">
<h1>第 8 章 推断引擎<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<style>p{text-indent:2em;2}</style>
<p>到目前为止，我们的重点是建立模型、解释结果和批判模型。依靠 <code class="docutils literal notranslate"><span class="pre">pm.sample</span></code> 函数的魔力计算后验分布。现在，我们将重点学习此函数背后的推断引擎。概率编程工具（如 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> ）的目的是使用户不用关心如何进行采样，但了解如何从后验获取样本对于全面理解推断过程很重要，还可以帮助我们了解这些方法何时会失败、为什么失败、如何处理。如果您对了解近似后验的方法原理不感兴趣，可以跳过本章的大部分内容，但强烈建议您至少阅读样本诊断一节，因为这一节提供了一些指导原则，可以帮助您检查后验样本是否可靠。</p>
<p>计算后验分布的方法有很多。本章将讨论一些基本思想，并将重点介绍在 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 中实现的最重要的方法。</p>
<p>在本章中，我们将学习：</p>
<ul class="simple">
<li><p>变分方法</p></li>
<li><p>Metropolis-Hastings</p></li>
<li><p>汉密尔顿蒙特卡洛</p></li>
<li><p>序贯蒙特卡洛</p></li>
<li><p>样本诊断</p></li>
</ul>
<hr class="docutils" />
<div class="section" id="id2">
<h2>8.1 几类推断引擎<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h2>
<p>虽然概念上很简单，但贝叶斯方法在数学和数值上都极具挑战性。主要原因是：贝叶斯定理（公式 1.4）中的分母边缘似然通常采用难以处理或计算昂贵的积分形式来求解。为此，后验估计通常使用马尔可夫链蒙特卡洛 (MCMC) 家族的算法或变分算法进行数值估计。这些方法有时被称为推断引擎，因为它们能够近似任何概率模型的后验分布。虽然在实践中推断并不总是那么好用，但这些方法也推动了概率编程语言（如 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code>) 的发展。</p>
<p>概率编程语言的目标是将建模过程与推断过程分开，以促进模型构建、评估和修改/扩展的迭代步骤。通过将推断过程视为黑匣子，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 等概率编程语言的用户可以将注意力放在他们关心的具体问题上，而让 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 为他们处理计算细节。这也是本书到目前为止一直在做的事情。因此，你可能会认为这是显而易见的方法。实际上，在概率编程语言出现之前，做概率模型的人习惯于编写自己的采样方法，通常是根据其模型量身定做的，或者习惯于简化模型，使其适合于某些数学近似。事实上，在一些学术界，这仍然是正确的。这种量身定制的方法更优雅，甚至可以提供一种更有效的后验计算方法，但也容易出错和耗时，即便对专家来说也是如此。</p>
<p>此外，定制方法不适合大多数用概率模型解决问题的从业者。像 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 这样的软件欢迎各种背景的人使用概率模型，从而降低了数学和计算的入门门槛。前面的章节主要是关于学习贝叶斯建模的基础知识；现在将在概念层面上学习如何实现自动推断，何时以及为什么失败，以及当失败时该怎么做。</p>
<p>目前有几种数值计算后验分布的方法。我把它们分成两大类：</p>
<p><strong>非马尔可夫方法</strong></p>
<ul class="simple">
<li><p>网格计算法</p></li>
<li><p>二次逼近法或拉普拉斯近似法</p></li>
<li><p>变分方法</p></li>
<li><p>集成嵌入拉普拉斯近似（INLA）方法</p></li>
</ul>
<p><strong>马尔可夫方法</strong></p>
<ul class="simple">
<li><p>Metropolis-Hastings法</p></li>
<li><p>哈密顿蒙特卡洛法</p></li>
<li><p>序贯蒙特卡洛法</p></li>
</ul>
</div>
<div class="section" id="id3">
<h2>8.2 非马尔可夫方法<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h2>
<p>首先讨论非马尔科夫方法的推断引擎。对于某些问题，此类方法非常有效，而对另外一些问题，此类方法只能提供真实后验的粗略近似。</p>
<div class="section" id="id4">
<h3>8.2.1 网格计算法<a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h3>
<p>网格计算法是一种暴力穷举的方法。即便你无法计算出整个后验，也可以根据一些点计算出先验和似然。假设我们要计算某个单参数模型的后验，网格近似法可以按照如下方式进行：</p>
<ul class="simple">
<li><p>确定参数的一个合理区间（先验会给你点提示）；</p></li>
<li><p>在以上区间确定一些网格点（通常是等距离的）；</p></li>
<li><p>对于网格中的每个点计算先验和似然。</p></li>
</ul>
<p>视情况，可能会对计算结果进行归一化（把每个点的计算结果除以所有点的计算结果之和）。很容易看出，选的点越多（网格越密）近似的结果就越好。事实上，如果使用无限多点，可以得到准确的后验。</p>
<p>网格计算法对于多参数的场景不太适用，因为随着参数增加，采样空间相比后验空间会急剧增加，换言之，我们花费了大量
时间计算后验值，但对于估计后验却几乎没有帮助（计算后验的时间比用后验做预测和推断的时间还长很多），因而该方法对于大多数统计学和数据科学问题不太实用。</p>
<p>下面的代码用网格计算法解决第一章中的抛硬币问题：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">posterior_grid</span><span class="p">(</span><span class="n">grid_points</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">heads</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">tails</span><span class="o">=</span><span class="mi">9</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    A grid implementation for the coin-flipping problem</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">grid</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">grid_points</span><span class="p">)</span>
    <span class="n">prior</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="o">/</span><span class="n">grid_points</span><span class="p">,</span> <span class="n">grid_points</span><span class="p">)</span>  <span class="c1"># uniform prior</span>
    <span class="n">likelihood</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">heads</span><span class="p">,</span> <span class="n">heads</span><span class="o">+</span><span class="n">tails</span><span class="p">,</span> <span class="n">grid</span><span class="p">)</span>
    <span class="n">posterior</span> <span class="o">=</span> <span class="n">likelihood</span> <span class="o">*</span> <span class="n">prior</span>
    <span class="n">posterior</span> <span class="o">/=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">grid</span><span class="p">,</span> <span class="n">posterior</span>
</pre></div>
</div>
<p>假设我们抛硬币 13 次，观察到 3 个头：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">points</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">h</span> <span class="o">=</span> <span class="n">data</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">t</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">h</span>
<span class="n">grid</span><span class="p">,</span> <span class="n">posterior</span> <span class="o">=</span> <span class="n">posterior_grid</span><span class="p">(</span><span class="n">points</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">grid</span><span class="p">,</span> <span class="n">posterior</span><span class="p">,</span> <span class="s1">&#39;o-&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;heads = </span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s1">, tails = </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;θ&#39;</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021060716063392.webp" /></p>
<p>很容易注意到，点数量越多（或者等效地：网格越小），可以得到更好的近似值。事实上，在无限个点的限制下，我们会以增加计算资源为代价得到精确的后验结果。</p>
<p>网格法最大的问题是：此方法不能很好地随参数（也称为维度）数量进行调整。可以通过一个简单的例子来了解这一点。假设我们想要采样一个单位区间（参见图 8.2)，就像抛硬币问题一样，使用四个等距点。这意味着分辨率为 0.25 个单位。现在假设有一个 2D 问题（图 8.2 中的正方形），并且想使用具有相同分辨率的网格，将需要 16 个点，而对于 3D 问题，我们将需要 64 个点（参见图 8.2 中的立方体）。本例中从边场为 1 的立方体采样所需的资源是长度为 1、分辨率为 0.25 的线采样的 16 倍。如果决定需要 0.1 个单位的分辨率，则必须对直线采样 10 个点，对立方体采样 1000 个点：</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210607160809ce.webp" /></p>
<p>除了点数增加，还有另一种现象，它不是网格法的属性，也不是其他方法的属性，而是高维空间的特性。随着<code class="docutils literal notranslate"><span class="pre">参数数量</span></code>增加，与采样体积相比，大部分后验集中在参数空间中越来越小的区域。这是统计学和机器学习中的普遍现象，通常被称为<code class="docutils literal notranslate"><span class="pre">维度诅咒</span></code>，或者数学家更喜欢将其称为 <code class="docutils literal notranslate"><span class="pre">量度集中</span></code>。</p>
<p>名称中的<code class="docutils literal notranslate"><span class="pre">维度诅咒</span></code>用来讨论各种相关现象，这些现象在低维空间中不存在，但在高维空间中存在。以下是这些现象的一些例子：</p>
<ul class="simple">
<li><p>随着维数增加，任意一对样本之间的欧几里得距离变得越来越近。也就是说，在高维空间中，大多数点彼此之间的距离基本相同。</p></li>
<li><p>对于超立方体，大部分体积在其角落，而不是在中间。对于超球体，大部分体积在其表面，而不在中间。</p></li>
<li><p>在高维中，多变量高斯分布的大部分质量并不非聚集在平均值或众数，而是在其周围的壳层中，而且随着维数增加，壳层从高斯分布的平均值向尾部移动。</p></li>
</ul>
<p>有关其中一些事实的代码示例，请查看 <a class="reference external" href="https:///%E2%80%8B/%E2%80%8Bgihub.%E2%80%8Bcom/aloctavodia/%E2%80%8BBAP">链接</a></p>
<p>所有这些事实都意味着，如果不明智地选择在哪里评估后验，我们将花费大部分时间来计算对后验贡献几乎为零的值，从而浪费宝贵的资源。网格法不是一种选择在哪里评估后验分布的明智方法，特别是对于高维问题。</p>
</div>
<div class="section" id="id5">
<h3>8.2.2 二次近似法<a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h3>
<p>二次近似法，也称为 <code class="docutils literal notranslate"><span class="pre">拉普拉斯方法</span></code> 或 <code class="docutils literal notranslate"><span class="pre">正态近似法</span></code>，包括用高斯分布 <span class="math notranslate nohighlight">\(q(x)\)</span> 来近似后验分布 <span class="math notranslate nohighlight">\(p(x)\)</span>。</p>
<p>此方法由两个步骤组成：</p>
<ul class="simple">
<li><p>找出真实后验分布的众数，并将其作为 <span class="math notranslate nohighlight">\(q(x)\)</span> 的均值。</p></li>
<li><p>计算 Hessian 矩阵。由此可以计算出的 <span class="math notranslate nohighlight">\(q(x)\)</span> 的标准差。</p></li>
</ul>
<p>第一步可以使用最优化方法进行数值计算，也就是找出函数的最大值或最小值，有许多现成的方法。对于高斯分布，众数和均值相等，所以可以使用众数作为近似分布 <span class="math notranslate nohighlight">\(q(x)\)</span> 的平均值。第二步稍微复杂些，通过计算众数/平均值处的曲率来近似计算 <span class="math notranslate nohighlight">\(q(x)\)</span> 的标准差。这可以通过计算海森矩阵的平方根的倒数来实现。海森矩阵是函数二阶导数的矩阵，其逆矩阵为协方差矩阵。使用 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code>，可以执行以下操作：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">normal_approximation</span><span class="p">:</span>
     <span class="n">p</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Beta</span><span class="p">(</span><span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>
     <span class="n">w</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">,</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">observed</span><span class="o">=</span><span class="n">data</span><span class="p">)</span>
     <span class="n">mean_q</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">find_MAP</span><span class="p">()</span>
     <span class="n">std_q</span> <span class="o">=</span> <span class="p">((</span><span class="mi">1</span><span class="o">/</span><span class="n">pm</span><span class="o">.</span><span class="n">find_hessian</span><span class="p">(</span><span class="n">mean_q</span><span class="p">,</span> <span class="nb">vars</span><span class="o">=</span><span class="p">[</span><span class="n">p</span><span class="p">]))</span><span class="o">**</span><span class="mf">0.5</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="n">mean_q</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">],</span> <span class="n">std_q</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>如果您尝试在 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 中使用 <code class="docutils literal notranslate"><span class="pre">pm.find_map</span></code> 函数，您将收到一条警告消息。由于维数灾难，使用最大后验概率 (MAP) 来表示后验，甚至初始化采样方法通常不是一个好主意。</p>
</div>
<p>让我们看看贝塔-二项模型的二次近似是什么样子：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># analytic calculation</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span> <span class="p">,</span> <span class="n">h</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span>
         <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True posterior&#39;</span><span class="p">)</span>
<span class="c1"># quadratic approximation</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">mean_q</span><span class="p">[</span><span class="s1">&#39;p&#39;</span><span class="p">],</span> <span class="n">std_q</span><span class="p">),</span><span class="n">label</span><span class="o">=</span><span class="s1">&#39;Quadratic</span>
         <span class="n">approximation</span><span class="s1">&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">13</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;heads = </span><span class="si">{</span><span class="n">h</span><span class="si">}</span><span class="s1">, tails = </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;θ&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([]);</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021060716150097.webp" /></p>
<p>图 8.3 显示，二次近似并没有那么差。严格地说，我们只能将拉普拉斯法应用于无界变量，也就是 <span class="math notranslate nohighlight">\(\mathcal{R}^N\)</span> 空间中的变量，因为高斯分布是一个无界分布。所以用它来近似一个有界分布（比如贝塔分布），会将实际密度为 0 点估计为正密度（如：贝塔分布的 [0，1] 区间之外）。可以考虑将有界变量变换为无界变量，进而使用拉普拉斯方法。例如，我们通常使用半正态分布来精确地模拟标准差，因为它被限制在 [0，∞) 区间内，可以通过取其对数来使半正态分布变量无界。</p>
<p>拉普拉斯方法的作用是有限的，但对某些模型可以很好地工作，并且可用来获得近似后验的解析解，进而提升推断的效率。拉普拉斯方法也是最近一种高级 <code class="docutils literal notranslate"><span class="pre">积分嵌套拉普拉斯近似</span> <span class="pre">(INLA)方法</span></code> 的构建块之一。</p>
<p>下一节将讨论变分方法，其在某种程度上类似于拉普拉斯方法，但更灵活、更强大，其中一些方法可自动应用于各种模型。</p>
</div>
<div class="section" id="id6">
<h3>8.2.3 变分方法<a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h3>
<p>大多数现代贝叶斯统计都是使用马尔可夫方法（见下一节）完成的，但对于某些问题，马尔可夫方法可能太慢了。变分方法是一种替代方法，特别是对于大数据集和/或计算成本过高的后验而言，变分方法是更好的选择。</p>
<p>变分法的基本思想是用一种更简单的分布来近似后验分布，这类似于拉普拉斯方法，但采用更精细的方式。可以通过解决一个最优化问题来找到这个更简单的分布，这个最优化问题包括在某种度量相似程度的方法下找到与后验最接近的可能分布。衡量分布之间相似程度的常用方法是使用 <code class="docutils literal notranslate"><span class="pre">Kullback-Leibler(KL)</span> <span class="pre">散度</span></code>（第 5 章中讨论过）。使用 <code class="docutils literal notranslate"><span class="pre">KL</span> <span class="pre">散度</span></code> 可得：</p>
<div class="math notranslate nohighlight">
\[
D_{K L}(q(\theta) \| p(\theta \mid y))=\int q(\theta) \log \frac{q(\theta)}{p(\theta \mid y)} d(\theta)  \tag{8.1}
\]</div>
<p>其中 <span class="math notranslate nohighlight">\(q(\theta)\)</span> 是较简单的分布，用于近似后验分布 <span class="math notranslate nohighlight">\(p(\theta)\)</span>， <span class="math notranslate nohighlight">\(q(\theta)\)</span> 通常被称为变分分布。通过使用最优化方法，我们试图找出分布 <span class="math notranslate nohighlight">\(q\)</span> 的参数（通常称为变分参数），使其在 <code class="docutils literal notranslate"><span class="pre">KL</span> <span class="pre">散度</span></code> 的度量上尽可能接近后验分布。注意，我们写了<span class="math notranslate nohighlight">\(D_{K L}(q(\theta) \| p(\theta \mid y))\)</span>，没有写 <span class="math notranslate nohighlight">\(\left.D_{K L}(p(\theta \mid y)) \| q(\theta)\right)\)</span>，因为这让问题表达更方便，此外两者之间不能交换。不过，在另一个方向上写 <code class="docutils literal notranslate"><span class="pre">KL</span> <span class="pre">散度</span></code> 也是有用的，实际上导致了另一组本书不讨论的方法。</p>
<p>表达式 8.1 的问题是我们不知道后验，所以不能直接使用它。需要找到另一种方式来表达问题。以下步骤显示了如何做到这一点。如果您不关心中间步骤，请跳到公式 8.7。</p>
<p>首先，用条件分布的定义做替换：</p>
<div class="math notranslate nohighlight">
\[
D_{K L}(q(\theta) \| p(\theta \mid y))=\int q(\theta) \log \frac{q(\theta)}{\underline{p(\theta, y)}} d(\theta) \tag{8.2}
\]</div>
<p>然后重排公式 8.2：</p>
<div class="math notranslate nohighlight">
\[
=\int q(\theta) \log \frac{q(\theta)}{p(\theta, y)} p(y) d(\theta) \tag{8.3}
\]</div>
<p>根据对数性质，得到方程：</p>
<div class="math notranslate nohighlight">
\[
=\int q(\theta)\left(\log \frac{q(\theta)}{p(\theta, y)}+\log p(y)\right) d(\theta) \tag{8.4}
\]</div>
<p>重新排列：</p>
<div class="math notranslate nohighlight">
\[
=\int q(\theta) \log \frac{q(\theta)}{p(\theta, y)} d(\theta)+\int q(\theta) \log p(y) d(\theta) \tag{8.5}
\]</div>
<p><span class="math notranslate nohighlight">\(q(\theta) 的积分是 1，可以将 \)</span>\text{log} p(\theta)$ 移出积分，然后得到：</p>
<div class="math notranslate nohighlight">
\[
=\int q(\theta) \log \frac{q(\theta)}{p(\theta, y)} d(\theta)+\log p(y) \tag{8.6}
\]</div>
<p>利用对数性质：</p>
<div class="math notranslate nohighlight">
\[
D_{K L}(q(\theta)|| p(\theta \mid y))=\underbrace{-\int q(\theta) \log \frac{p(\theta, y)}{q(\theta)} d(\theta)}_{\text {evidence lower bound (ELBO) }}+\log p(y)) \tag{8.7}
\]</div>
<p>因为 <span class="math notranslate nohighlight">\(D_{K L} \geq 0\)</span> ，然后 <span class="math notranslate nohighlight">\(D_{KL} \geq 0\)</span> ，或者换句话说，证据（或边缘似然）总是等于或大于 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code>，这就是其命名的原因。既然证据是一个常量，我们可以只关注 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 。最大化 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code> 的值相当于最小化 <code class="docutils literal notranslate"><span class="pre">KL</span> <span class="pre">散度</span></code>。因此，使 <code class="docutils literal notranslate"><span class="pre">ELBO</span> <span class="pre">最大化</span></code> 是一种使其尽可能接近后验的方法。</p>
<p>注意此时还没有引入任何近似，只是做了一些代数。实际上在选择 <span class="math notranslate nohighlight">\(q(.)\)</span> 时才开始引入近似。原则上， <span class="math notranslate nohighlight">\(q(.)\)</span> 可以是任何想要的分布形式，但实践中应选择易于处理的分布。一种解决方案是假设高维后验可以用若干独立的一维分布来描述；在数学上，可以表示如下：</p>
<div class="math notranslate nohighlight">
\[
q(\theta)=\prod_{j} q_{j}\left(\theta_{j}\right) \tag{8.8}
\]</div>
<p>这种近似方式被称为<code class="docutils literal notranslate"><span class="pre">平均场近似（Mean-Field</span> <span class="pre">Approximation）</span></code>。平均场近似在物理学常被用来将（具有许多相互作用的）复杂系统建模为（无相互作用的）子系统的集合，或只有在做平均时才考虑相互作用。</p>
<p>我们可为每个参数 <span class="math notranslate nohighlight">\(\theta_j\)</span> 选择不同的分布 <span class="math notranslate nohighlight">\(q_j\)</span>。通常，<span class="math notranslate nohighlight">\(q_j\)</span> 取自指数族分布。指数族包括正态分布、指数分布、贝塔分布、狄利克雷分布、伽马分布、泊松分布、绝对分布和伯努利分布等常见分布。</p>
<p>有了上述有要素，就可以将推断问题转化为优化问题；至少在概念上，问题转换成：“使用一些优化方法最大化 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code>” 。不过在实践中事情可能不会那么简单，会稍微复杂一些。</p>
</div>
<div class="section" id="id7">
<h3>8.2.4 自动微分变分推断方法<a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h3>
<p>平均场变分法的主要缺点是：必须为每个模型提出一个特定算法。我们很难形成一个通用的推断引擎，只能生成需要用户干预的特定模型方法。许多人注意到了该问题，并提出了以 <code class="docutils literal notranslate"><span class="pre">变分方法自动化</span></code> 为重点的解决方案。最近提出的一种方法是 <code class="docutils literal notranslate"><span class="pre">自动微分变分推断法</span> <span class="pre">(ADVI)</span></code>，参见<a class="reference external" href="http:///%E2%80%8B/%E2%80%8Barxiv.org/%E2%80%8Babs/%E2%80%8B1603.%E2%80%8B00788">链接</a> 。在概念层面上，<code class="docutils literal notranslate"><span class="pre">ADVI</span></code> 采取的主要步骤是：</p>
<ul class="simple">
<li><p>将所有有界分布转换到实数轴上，就像之前在拉普拉斯方法中讨论的那样。</p></li>
<li><p>用高斯分布来近似无界参数（公式 8.8 中的 <span class="math notranslate nohighlight">\(q_j\)</span> ）；注意变换后参数空间上的高斯在原参数空间上是非高斯的。</p></li>
<li><p>使用自动微分来最大化 <code class="docutils literal notranslate"><span class="pre">ELBO</span></code>。</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 文档（例如：<a class="reference external" href="https:///%E2%80%8B/%E2%80%8Bdocs.%E2%80%8Bpymc.%E2%80%8Bio/%E2%80%8Bnb_%E2%80%8Bexamples">链接</a>） 提供了许多关于使用变分推断的示例。</p>
</div>
</div>
<div class="section" id="id8">
<h2>8.2 马尔科夫方法<a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h2>
<p>有一系列和随机数相关的方法被统称为 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code>。对于贝叶斯统计，只要能够逐点计算出似然（人工建模与似然假设）和先验值（人工设置先验），<code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code>就能从真实后验分布中获得样本。这个条件与网格法相同，但由于 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code>能够依据取值概率从分布的不同区域采集不同数量的样本，所以其性能远远优于网格法。<code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code>会根据参数空间中每个区域的相对概率来确定从该区域抽取样本的次数。例如：如果区域 A 的概率是区域 B 的两倍，那么从 A 获得的样本将是从 B 获得样本的两倍。因此，即使不能解析计算出整个后验，也可以使用 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code>从中获取样本。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>注：后验公式中分母的边缘似然项很难计算，进而导致很难计算后验的绝对概率值，而 MCMC 方法可以依据相对概率确定采样数量，这是其一大优势。</p>
</div>
<p>我们在统计模型中关心的几乎所有事物，基本都与某些关于 <span class="math notranslate nohighlight">\(\theta\)</span> 的函数 <span class="math notranslate nohighlight">\(f(\theta)\)</span> 的期望值有关，比如：</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[f]=\int_{\theta} p(\theta) f(\theta) \mathrm{d} \theta \tag{8.9}
\]</div>
<p>下面是该表达式的一些特例：</p>
<ul class="simple">
<li><p>求后验（公式 1.14）</p></li>
<li><p>求后验预测性分布（公式 1.17）</p></li>
<li><p>求给定模型的边缘似然（公式 5.13）</p></li>
</ul>
<p>使用 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code>，在采样密度符合相对概率的条件下，可以用有限样本近似公式 8.9：</p>
<div class="math notranslate nohighlight">
\[
\lim _{N \rightarrow \infty} \mathbb{E}_{\pi}[f]=\frac{1}{N} \sum_{n=1}^{N} f\left(\theta_{n}\right) \tag{8.10}
\]</div>
<p>式 8.10 的问题是：等式仅渐近成立。也就是说，需要无限数量的样本才成立！但实践中仅有有限数量样本，因此我们希望 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code> 能够尽可能快地收敛到正确答案，即用尽可能少的样本得到正确答案，这也是 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code> 的难点问题。</p>
<p>一般来说，要确定 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">样本</span></code> 已经收敛并非易事。因此，实践中必须依靠实证检验来确保有一个可靠的 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">近似</span></code> 。我们将在 8.4 节“诊断样本”部分讨论该问题。当然，其他近似（包括非马尔可夫方法）也需要实证检验，但本书不会过多讨论它们。</p>
<p>对 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code> 有一个概念性了解有助于从这些方法中诊断样本。为了理解什么是 <code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code>，后文把该方法分成两个部分：<code class="docutils literal notranslate"><span class="pre">蒙特卡洛部分</span></code> 和 <code class="docutils literal notranslate"><span class="pre">马尔可夫链部分</span></code> 。</p>
<div class="section" id="id9">
<h3>8.3.1 蒙特卡洛<a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h3>
<p>蒙特卡洛部分可以用随机数来解释。蒙特卡洛方法是一个非常广泛的算法家族，它使用随机抽样来计算或模拟给定过程。蒙特卡洛是摩纳哥的一个区，那里有一家非常著名的赌场。蒙特卡洛方法的开发者之一 <code class="docutils literal notranslate"><span class="pre">Stanislaw</span> <span class="pre">Ulam</span></code> 有一个叔叔曾在那里赌博。<code class="docutils literal notranslate"><span class="pre">Stan</span></code> 的关键思想是，虽然许多问题很难解决，甚至很难用确切方式表达出来，但可以通过从这些问题中抽取样本来有效地研究它们。事实上，蒙特卡洛方法的最早动机是回答有关在纸牌游戏中拿到一张特定牌的可能性问题。解决该问题的常见方法是组合数学分析法。但 <code class="docutils literal notranslate"><span class="pre">Stan</span></code> 认为存在另一种方法，进行多次单轮游戏，最后计算其中有多少次是我们感兴趣的。</p>
<p>蒙特卡洛方法的第一个应用是解决核物理问题，这是当时用工具很难解决的问题。如今，即使是个人计算机也足以使用蒙特卡洛方法解决许多有趣的问题；因此，蒙特卡洛方法被应用于科学、工程、工业和艺术中的各种问题。</p>
<p>在使用蒙特卡洛方法计算数值的例子中，教科书上的经典案例是估计 <span class="math notranslate nohighlight">\(π\)</span> 。实际中有更好的方法来计算 <span class="math notranslate nohighlight">\(π\)</span> ，不过该例仍然具有教学意义。我们可以通过以下蒙特卡洛过程估计 <span class="math notranslate nohighlight">\(π\)</span> ：</p>
<p>（1）在边长为 <span class="math notranslate nohighlight">\(2R\)</span> 的正方形内随机撒 <span class="math notranslate nohighlight">\(N\)</span> 个点。
（2）在正方形内画一个半径为 <span class="math notranslate nohighlight">\(R\)</span> 的圆，计算在圆内点的个数。
（3）利用比例估计 <span class="math notranslate nohighlight">\(\bar \pi = 4\frac{inside}{N}\)</span> 。</p>
<p>以下是一些注意事项：</p>
<p>（1）圆和正方形的面积分别与圆内的点数和总点数成正比。
（2）如果关系 <span class="math notranslate nohighlight">\(\sqrt{\left(x^{2}+y^{2}\right)} \leq R\)</span> 成立，则认为该点在圆内。
（3）正方形的面积是 <span class="math notranslate nohighlight">\((2R)^2\)</span> ，圆的面积是 <span class="math notranslate nohighlight">\(πR^2\)</span> ，因此二者面积之比是 <span class="math notranslate nohighlight">\(4/π\)</span></p>
<p>圆和正方形的面积分别正比于圆内点数和总点数 <span class="math notranslate nohighlight">\(N\)</span>。我们可以通过几行简单的代码来模拟该蒙特卡洛过程计算 <span class="math notranslate nohighlight">\(π\)</span> 值，同时计算出估计值与实际值之间的相对误差：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">N</span><span class="p">))</span>
<span class="n">inside</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">1</span>
<span class="n">pi</span> <span class="o">=</span> <span class="n">inside</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">*</span><span class="mi">4</span><span class="o">/</span><span class="n">N</span>
<span class="n">error</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">((</span><span class="n">pi</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span> <span class="o">/</span> <span class="n">pi</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span>
<span class="n">outside</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">invert</span><span class="p">(</span><span class="n">inside</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">8</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">inside</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">inside</span><span class="p">],</span> <span class="s1">&#39;b.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">outside</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">outside</span><span class="p">],</span> <span class="s1">&#39;r.&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;π*= </span><span class="si">{</span><span class="n">pi</span><span class="si">:</span><span class="s1">4.3f</span><span class="si">}</span><span class="se">\n</span><span class="s1">error = </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s1">4.3f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s1">&#39;square&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">frameon</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">framealpha</span><span class="o">=</span><span class="mf">0.9</span><span class="p">);</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021060716361541.webp" /></p>
<p>上面的代码中，<code class="docutils literal notranslate"><span class="pre">outside</span></code> 变量仅用于绘图，在计算 <span class="math notranslate nohighlight">\(\hat \pi\)</span>  过程中没有用到。另外一点需要澄清的是，由于这里用的是单位圆，因此在判断一个点是否在圆内时没有计算平方根。</p>
</div>
<div class="section" id="id10">
<h3>8.3.2 马尔科夫链<a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h3>
<p>马尔科夫链是一个数学对象，包含一系列状态以及状态之间的转移概率，如果每个状态转移到其他状态的概率只与当前状态相关，那么这个状态链就称为马尔科夫链。有这样一个马尔科夫链之后，我们可以任取一个初始点，然后根据状态转移概率进行随机游走。假设能够找到这样一个马尔科夫链，其状态转移概率正比于我们想要采样的分布（如贝叶斯分析中的后验分布），采样过程就变成了简单地在该状态链上移动的过程。</p>
<p>那么，如何在不知道后验分布的情况下找到这样的状态链呢？有一个概念叫做<code class="docutils literal notranslate"><span class="pre">细节平衡条件（Detailed</span> <span class="pre">Balance</span> <span class="pre">Condition）</span></code> ，直观上讲，该条件是说，我们需要采用一种可逆的方式移动。也就是说，从状态 <span class="math notranslate nohighlight">\(i\)</span> 转移到状态 <span class="math notranslate nohighlight">\(j\)</span> 的概率必须和状态 <span class="math notranslate nohighlight">\(j\)</span> 转移到状态 <span class="math notranslate nohighlight">\(i\)</span> 的概率相等。该条件是充分条件而不一定是必要条件，且比较容易证明，所以被用于大多数 <code class="docutils literal notranslate"><span class="pre">MCMC方法</span></code> 。</p>
<p>总的来说就是，如果我们能够找到满足细节平衡条件的马尔科夫链，就可以保证从中采样得到的样本来自正确的分布。保证细节平衡的最流行的算法是 Metropolis-Hasting 算法。</p>
</div>
<div class="section" id="metropolis-hastings">
<h3>8.3.3 Metropolis-Hastings 算法<a class="headerlink" href="#metropolis-hastings" title="Permalink to this headline">¶</a></h3>
<p>某些分布（如高斯分布）有非常有效的样本获取算法，但对于大多数分布，情况并非如此。<code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code> 能够从任何概率分布 <span class="math notranslate nohighlight">\(p(x)\)</span> 中获得与其取值概率成比例的样本，而不用计算归一化因子。这非常有用，因为许多问题（不仅是贝叶斯统计）中计算归一化因子都是非常困难的。</p>
<p>为了更形象地理解这个算法，用下面这个例子来类比。假设我们想知道某个湖的水容量以及这个湖中最深的点，湖水很浑浊以至于没法通过肉眼来估计深度，而且这个湖相当大，网格法显然不是个好办法。为了找到一个采样策略，我们请来了两个好朋友小马和小萌。经过讨论之后想出了如下办法，我们需要一个船和一个很长的棍子：</p>
<ol class="simple">
<li><p>随机选一个点，然后将船开过去。</p></li>
<li><p>用棍子测量湖的深度。</p></li>
<li><p>将船移到另一个地点并重新测量。</p></li>
<li><p>按如下方式比较两点的测量结果。</p>
<ul class="simple">
<li><p>如果新的地点比旧的地点水位深，那么在笔记本上记录下新的测量值并重复过程（2）。</p></li>
<li><p>如果新的地点比旧的地点水位浅，那么我们有两个选择：接受或者拒绝。</p>
<ul>
<li><p>接受意味着记录下新的测量值并重复过程（2）；</p></li>
<li><p>拒绝意味着重新回到上一个点，再次记录下上一个点的测量值。</p></li>
</ul>
</li>
</ul>
</li>
</ol>
<p>如何决定接受还是拒绝新的测量值呢？这里的一个技巧便是使用 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">准则</span></code>，即接受新的测量值的概率正比于新旧两点的测量值之比。</p>
<p>按照以上过程迭代下去，不仅可以得到整个湖的水容量和最深的点，而且可以得到整个湖底的近似曲面。在这个类比中，湖底的曲面其实就是后验分布，而最深的点就是后验的众数。根据小马的说法，迭代的次数越多，近似的效果越好。事实上，理论保证了在这种情形下，如果能采样无数次，最终能得到完整的后验。幸运地是，实际上对于很多问题而言，只需要相对较少地采样就可以得到一个相当准确的近似。</p>
<p>前面的解释足以对 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code> 有一个概念性的理解。接下来几段包含更详细、更正式的解释。</p>
<p><code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code> 算法的步骤如下：</p>
<ol>
<li><p>给参数 <span class="math notranslate nohighlight">\(x_i\)</span> 赋一个初始值，通常是随机初始化或者使用某些经验值。</p></li>
<li><p>从某个提议分布（如高斯分布、均匀分布等简单分布） <span class="math notranslate nohighlight">\(q(x_{i+1}|x_i)\)</span> 中随机抽取一个新参数值。这一步可视为对状态 <span class="math notranslate nohighlight">\(x_i\)</span> 的扰动。</p></li>
<li><p>根据 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">准则</span></code> 计算新参数值的接受概率：</p>
<div class="math notranslate nohighlight">
\[
   p_{a}\left(x_{i+1} \mid x_{i}\right)=\min \left(1, \frac{p\left(x_{i+1}\right) q\left(x_{i} \mid x_{i+1}\right)}{p\left(x_{i}\right) q\left(x_{i+1} \mid x_{i}\right)}\right) \tag{8.11}
   \]</div>
</li>
<li><p>从位于区间 [0,1] 内的均匀分布中随机抽取一个值，如果第（3）步中得到的接受概率比该值大，那么就接受新参数值，否则仍保持原值。</p></li>
<li><p>回到第（2）步重新迭代，直到有足够多的样本（稍后会解释什么叫足够多）。</p></li>
</ol>
<p>有几点需要注意：</p>
<ol class="simple">
<li><p>如果选择的提议分布 <span class="math notranslate nohighlight">\(q(x_{i+1}|x_i)\)</span> 是对称的，那么可以得到式 8.12，通常称为 <code class="docutils literal notranslate"><span class="pre">Metropolis</span> <span class="pre">准则</span></code> 。</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
p_{a}\left(x_{i+1} \mid x_{i}\right)=\min \left(1, \frac{p\left(x_{i+1}\right)}{p\left(x_{i}\right)}\right) \tag{8.12}
\]</div>
<ol class="simple">
<li><p>步骤（3）和步骤（4）表明：我们总是会转移到一个比当前状态（或参数）概率更大的状态（或参数），对于概率更小的，则会以 <span class="math notranslate nohighlight">\(x_{i+1}\)</span> 与 <span class="math notranslate nohighlight">\(x_i\)</span> 之比的概率接受。该准则中的接受步骤使得采样过程相比网格法更高效，同时保证了采样的准确性。</p></li>
<li><p>目标分布（贝叶斯统计中的后验分布）是通过记录下来的采样值来近似的。如果我们接受转移到新的状态 <span class="math notranslate nohighlight">\(x_{i+1}\)</span>，那么我们就记录该采样值 <span class="math notranslate nohighlight">\(x_{i+1}\)</span>。如果拒绝转移到 <span class="math notranslate nohighlight">\(x_{i+1}\)</span>，那么我们就记录 <span class="math notranslate nohighlight">\(x_i\)</span>。</p></li>
</ol>
<p>最后，我们会得到一连串记录值，有时候也称采样链或者迹。如果一切都正常进行，那么这些采样值就是后验的近似。在采样链中出现次数最多的值对应后验中出现概率最大的值（最大后验概率点）。该过程的优点是：<code class="docutils literal notranslate"><span class="pre">后验分析很简单，可以把对后验求积分的过程转化成对采样链求和的过程</span></code> （式 8.10）。</p>
<p>下面的代码展示了 <code class="docutils literal notranslate"><span class="pre">Metropolis</span> <span class="pre">算法</span></code>的一个基本实现。这段代码并不是为了解决什么实际问题，只是用来演示，如果我们知道怎么计算给定点的函数值，就能得到该函数的采样。需要注意代码不包含贝叶斯相关的部分，既没有先验也没有数据。要知道，<code class="docutils literal notranslate"><span class="pre">MCMC</span></code> 是一类能够用于解决很多问题的通用方法。例如，在一个（非贝叶斯的）分子模型中，可能需要一个函数来计算在某个状态 <span class="math notranslate nohighlight">\(x\)</span> 下系统的能量而不是简单地调用 <code class="docutils literal notranslate"><span class="pre">func.pdf(x)</span></code> 函数。<code class="docutils literal notranslate"><span class="pre">metropolis</span></code> 函数的第一个参数是一个 <code class="docutils literal notranslate"><span class="pre">SciPy</span></code> 的分布，假设我们不知道如何从中直接采样。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">metropolis</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">draws</span><span class="o">=</span><span class="mi">10000</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;A very simple Metropolis implementation&quot;&quot;&quot;</span>
    <span class="n">trace</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">draws</span><span class="p">)</span>
    <span class="n">old_x</span> <span class="o">=</span> <span class="mf">0.5</span>  <span class="c1"># func.mean()</span>
    <span class="n">old_prob</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">old_x</span><span class="p">)</span>
    <span class="n">delta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">draws</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">draws</span><span class="p">):</span>
        <span class="n">new_x</span> <span class="o">=</span> <span class="n">old_x</span> <span class="o">+</span> <span class="n">delta</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="n">new_prob</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">new_x</span><span class="p">)</span>
        <span class="n">acceptance</span> <span class="o">=</span> <span class="n">new_prob</span> <span class="o">/</span> <span class="n">old_prob</span>
        <span class="k">if</span> <span class="n">acceptance</span> <span class="o">&gt;=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">():</span>
            <span class="n">trace</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_x</span>
            <span class="n">old_x</span> <span class="o">=</span> <span class="n">new_x</span>
            <span class="n">old_prob</span> <span class="o">=</span> <span class="n">new_prob</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">trace</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">old_x</span>
    <span class="k">return</span> <span class="n">trace</span>
</pre></div>
</div>
<p>在下一个示例中，我们将 <code class="docutils literal notranslate"><span class="pre">func</span></code> 定义为 <code class="docutils literal notranslate"><span class="pre">beta函数</span></code>，原因很简单，因为很容易更改它们的参数获得不同形状。我们将<code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 获得的样本绘制为直方图，并将真实分布绘制为连续(橙色)线：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">func</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">trace</span> <span class="o">=</span> <span class="n">metropolis</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">func</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">.99</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">func</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;C1-&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;True distribution&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">hist</span><span class="p">(</span><span class="n">trace</span><span class="p">[</span><span class="n">trace</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">],</span> <span class="n">bins</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">density</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Estimated</span>
<span class="n">distribution</span><span class="s1">&#39;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;pdf(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">([])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210608114143f5.webp" /></p>
<p>算法效率很大程度上依赖于提议分布；如果建议状态离当前状态很远，拒绝的机会很高；如果建议状态非常接近，则搜索参数空间的速度很慢。在这两种情况下，都需要比不那么极端的情况下更多的样本。通常，提议分布是多变量高斯分布，其协方差矩阵是在调谐阶段确定的。<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 会自适应地调整协方差，基本原则是：一维高斯分布的理想接受度为 50% ， <span class="math notranslate nohighlight">\(n\)</span> 维高斯分布的理想接受度为 23% 左右。</p>
<p><code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code> 通常需要一段时间才能开始从目标分布中获取样本。因此，实践中会进行老化步骤。老化是一种实用的技巧，并不是马尔科夫理论的组成部分；事实上，对于无限样本来说，并不需要老化。考虑到我们只能计算有限样本，删除样本的第一部分只是为了获得更好结果的临时技巧。</p>
<p>现在你应该从概念上掌握了 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">算法</span></code>。也许你需要回过头去重新阅读前面几页才能完全消化。此外，我还强烈建议阅读 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 核心作者写的<a class="reference external" href="https://twiecki.io/blog/2015/11/10/mcmc-sampling/">博文</a> 。他用一个简单的例子实现了 <code class="docutils literal notranslate"><span class="pre">metropolis</span></code> 方法，并将其用于求解后验分布，文中用非常好看的图展示了采样的过程，同时简单讨论了最初选取的步长如何影响了最终结果。</p>
</div>
<div class="section" id="id11">
<h3>8.3.4 汉密尔顿蒙特卡洛方法/不掉向采样<a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code>（包括 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code>）都在理论上保证如果采样次数足够多，最终会得到后验分布的准确近似。不过，实际中想要采样足够多次可能需要相当长的时间，因此，人们提出了一些 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">算法</span></code>的替代方案。这些替代方案最初都是用来解决统计力学的问题。<code class="docutils literal notranslate"><span class="pre">汉密尔顿蒙特卡洛方法</span></code>，又称 <code class="docutils literal notranslate"><span class="pre">混合蒙特卡洛（Hybrid</span> <span class="pre">Monte</span> <span class="pre">Carlo，HMC）</span></code>，是此类改进方案之一。简单来说，汉密尔顿这个词描述的是物理系统的总能量，而另外一个名称中的“混合”是指将 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">算法</span></code> 与 <code class="docutils literal notranslate"><span class="pre">分子力学</span></code> 相结合。<code class="docutils literal notranslate"><span class="pre">HMC</span> <span class="pre">方法</span></code> 本质上和 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code> 是一样的，改进的地方在于：原来是随机放置小船，现在有了一个更聪明的办法，将小船沿着湖底方向放置。为什么这个做法更聪明？因为这样做避免了 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">算法</span></code> 的一个主要问题：探索得太慢而且采样结果存在自相关（因为大多数采样结果都被拒绝了）。</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>统计力学是物理学的一个分支，主要研究原子和分子系统的特性。</p>
</div>
<p>那么，如何才能不必深入其数学细节而理解汉密尔顿蒙特卡洛方法呢？假设我们还是在湖面上坐着船，为了决定下一步将要去哪，我们从当前位置往湖底扔了一个球，受“球状奶牛”的启发 [3]，我们假设球面是理想的，没有摩擦，因而不会被泥巴和水减速。扔下球之后，让它滚一小会儿，然后把船划到球所在的位置。现在利用 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span> <span class="pre">算法</span></code> 中提到的 <code class="docutils literal notranslate"><span class="pre">Metropolis</span> <span class="pre">准则</span></code> 来选择接受或者拒绝，重复整个过程一定次数。改进后的过程对新位置有更高的接受概率，即使它们的位置相比前一位置距离较远。</p>
<p>现在跳出我们的思维实验，回到现实中来。基于汉密尔顿的方法需要计算函数的梯度。梯度是在多个维度上导数的推广。我们可以用梯度信息来模拟球在曲面上移动的过程。因此，我们面临一个权衡；<code class="docutils literal notranslate"><span class="pre">HMC</span></code> 计算过程要比 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code> 更复杂，但被接受概率更高。对于一些复杂的问题，<code class="docutils literal notranslate"><span class="pre">HMC</span> <span class="pre">方法</span></code> 更合适一些。<code class="docutils literal notranslate"><span class="pre">HMC</span> <span class="pre">方法</span></code>的另一个缺点是：想要得到很好的采样需要指定一些参数。当手动完成此调优时，需要进行一些试验和错误，还需要有经验的用户，这使得此过程不像我们所希望的那样是一个通用的推理引擎。幸运的是，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 配备了一个相对较新的采样器，名为不掉头采样器 <code class="docutils literal notranslate"><span class="pre">No-U-Turn</span> <span class="pre">Sampler(NUTS)</span></code>。事实证明，该方法在不需要人工干预(或至少将其最小化)的情况下，为求解贝叶斯模型提供了非常好的效率。NUTS 的问题是，它只适用于连续分布，因为我们无法计算离散分布的梯度。<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 通过将 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 分配给连续参数，将 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 分配给离散参数来解决此问题。</p>
<p>我强烈推荐 <code class="docutils literal notranslate"><span class="pre">Chi</span> <span class="pre">Feng</span></code> 的 <a class="reference external" href="https://%E2%80%8Bchi-%E2%80%8Bfeng.%E2%80%8Bgithub.%E2%80%8Bio/mcmc-demo/">动画</a> 来补充这部分内容的学习。</p>
</div>
<div class="section" id="id12">
<h3>8.3.5 序贯蒙特卡洛<a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code> 和 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code>（以及其他哈密尔顿蒙特卡洛变种）存在的问题是：如果后验有多个峰，并且这些峰被非常低概率的区域分开，则上述方法可能会陷入某个单一众数，而错过其他值。</p>
<p>解决多重极小值问题的主要方法是 <code class="docutils literal notranslate"><span class="pre">回火法（tempering）</span></code> 。回火法借用了统计力学，物理系统可填充的状态数取决于系统的温度：在绝对温度为 <span class="math notranslate nohighlight">\(0K\)</span> 时，每个系统都停留在单一状态；另一方面，对于无限大的温度，所有可能状态都是同等可能的；通常我们对处于某一中间温度的系统感兴趣。</p>
<p>对于贝叶斯模型，有一种直观方式来适应这种调温的想法，那就是用以下方式写出贝叶斯定理：</p>
<div class="math notranslate nohighlight">
\[
p(\theta \mid y)_{\beta}=p(y \mid \theta)^{\beta} p(\theta) \tag{8.13}
\]</div>
<p>公式 1.4 和 8.13 之间的唯一区别是参数 <span class="math notranslate nohighlight">\(\beta\)</span> ，此处被称为 <code class="docutils literal notranslate"><span class="pre">逆温</span></code> 或 <code class="docutils literal notranslate"><span class="pre">回火参数</span></code> 。请注意，对于 <span class="math notranslate nohighlight">\(\beta=0\)</span> 我们得到 <span class="math notranslate nohighlight">\(p(y \mid \theta)^{\beta}=1\)</span> ，因此调温后的后验 <span class="math notranslate nohighlight">\(p(\theta \mid y)_{\beta}\)</span> 就是先验 <span class="math notranslate nohighlight">\(p(\theta)\)</span>； 并且当 <span class="math notranslate nohighlight">\(\beta=1\)</span> 时，调温后的后验是真实的完整后验。由于从先验采样通常比从后验采样容易（通过增加 <span class="math notranslate nohighlight">\(\beta\)</span> 的值），因此我们可以从更容易的分布开始采样，然后慢慢地将其变形为真正关心的更复杂的分布。</p>
<p>利用该思想的方法很多，<code class="docutils literal notranslate"><span class="pre">序贯蒙特卡洛</span> <span class="pre">(Sequential</span> <span class="pre">Monte</span> <span class="pre">Carlo</span> <span class="pre">,</span> <span class="pre">SMC)</span></code> 就是其中之一。在 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 中实施的 <code class="docutils literal notranslate"><span class="pre">SMC</span> <span class="pre">方法</span></code> 可总结如下：</p>
<ol class="simple">
<li><p>以 0 初始化参数 <span class="math notranslate nohighlight">\(\beta\)</span> 。</p></li>
<li><p>从回火后的后验中生成 <span class="math notranslate nohighlight">\(N\)</span> 个样本，形成样本集 <span class="math notranslate nohighlight">\(S_{\beta}\)</span>。</p></li>
<li><p>增加一点 <span class="math notranslate nohighlight">\(\beta\)</span>。</p></li>
<li><p>计算 <span class="math notranslate nohighlight">\(N\)</span> 个权重 <span class="math notranslate nohighlight">\(w\)</span> 构成的集合 <span class="math notranslate nohighlight">\(W\)</span> ，权重根据新的回火后验计算得出。</p></li>
<li><p>根据 <span class="math notranslate nohighlight">\(w\)</span> 对 <span class="math notranslate nohighlight">\(S_{\beta}\)</span> 重采样，得到 <span class="math notranslate nohighlight">\(S_w\)</span>。</p></li>
<li><p>运行 <span class="math notranslate nohighlight">\(N\)</span> 条 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code>链，每条链从 <span class="math notranslate nohighlight">\(S_w\)</span> 中的不同样本开始。</p></li>
<li><p>从步骤 3 开始重复，直到 <span class="math notranslate nohighlight">\(\beta \geq 1\)</span>。</p></li>
</ol>
<p>重采样步骤通过移除概率较低的样本并将其替换为概率较高的样本来实现。<code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 步骤扰乱了这些样本，有助于探索参数空间。回火法的效率在很大程度上取决于 <span class="math notranslate nohighlight">\(\beta\)</span> 的中间值（通常称之为冷却计划）。<span class="math notranslate nohighlight">\(\beta\)</span> 的两个相邻值间的差异越小，两个回火后验的距离就越近，从一个阶段转移到下一个阶段就越容易。但如果步长过小，将需要更多中间阶段，会导致大量计算资源浪费，而不会真正提高结果的准确性。</p>
<p>幸运的是，<code class="docutils literal notranslate"><span class="pre">SMC</span></code> 可以自动计算 <span class="math notranslate nohighlight">\(\beta\)</span> 的中间值。精确的冷却计划将根据问题难度进行调整；较难采样的分布将比简单分布需要更多的中间阶段。</p>
<p><code class="docutils literal notranslate"><span class="pre">SMC</span></code> 如图 8.6 所示，第一个子图显示了特定阶段的五个样本（橙色）点。第二个小图显示了这些样本是如何根据它们调温后的后验密度（蓝色）曲线重新加权的。第三个子图显示了从第二个子图中的重新加权样本开始，运行一定数量 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 步长的结果。请注意，后验密度较低的两个样本（最右侧和最左侧的较小圆圈）如何被丢弃，而不是用作新马尔可夫链的种子：</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021060717005520.webp" /></p>
<p>除了 <span class="math notranslate nohighlight">\(\beta\)</span> 的中间值外，还根据前一阶段的接受率动态计算了另外两个参数：每个马尔可夫链的步数和提议分布的宽度。</p>
<p>对于 <code class="docutils literal notranslate"><span class="pre">SMC</span> <span class="pre">算法</span></code> 的第 6 步，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 使用了 <code class="docutils literal notranslate"><span class="pre">Metropolis</span> <span class="pre">算法</span></code>。这不是唯一选择，但是一个非常合理的选项，并受到理论和实践结果推动。值得注意的是，即使 <code class="docutils literal notranslate"><span class="pre">SMC</span> <span class="pre">方法</span></code>使用了 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code>，它也有几个超出 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 优点：</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">SMC</span></code> 可以从多峰分布中采样。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SMC</span></code> 没有老化期。这是由于重加权过程造成的。权重计算采用了如下方式： <span class="math notranslate nohighlight">\(S_w\)</span> 不是近似 <span class="math notranslate nohighlight">\(p(\theta|y)_{\beta_i}\)</span>，而是 <span class="math notranslate nohighlight">\(p(\theta|y)_{\beta_{i+1}}\)</span> 。因此，在每个阶段中，<code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">链</span></code> 都近似地从正确的后验分布开始。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SMC</span></code> 可以产生低自相关的样本。</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">SMC</span></code> 可以用来近似边缘似然，这是 <code class="docutils literal notranslate"><span class="pre">SMC</span> <span class="pre">方法</span></code> 的一个副产品，几乎不需要额外的计算。</p></li>
</ul>
</div>
</div>
<div class="section" id="id13">
<h2>8.4 诊断样本<a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h2>
<p>本节重点介绍 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 和 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 的样本诊断。因为我们是用有限数量的样本来近似后验，所以检查是否有一个有效样本很重要，否则任何来自它的后续分析都将是有缺陷的。我们可以进行几种直观或定量的检验，来发现样本的问题。但这些检验无法证明样本是否正确；只能提供样本似乎合理的证据。如果发现样本有问题，可以尝试很多解决方案：</p>
<ul class="simple">
<li><p>增加采样数。</p></li>
<li><p>从轨迹的开头删除一些样本。这就是所谓的老化。<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 调优阶段有助于减少老化需求。</p></li>
<li><p>修改采样器参数，例如增加调谐阶段的长度，或增加 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 采样器的 TARGET_ACCEPT 参数。在某些情况下，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 会提供修改建议。</p></li>
<li><p>重新参数化模型，即以不同但等价的方式表达模型。</p></li>
<li><p>转换数据。我们已经看到了一个这样的例子，在第 4 章，推广线性模型和第 5 章，模型比较中，我们展示了将数据居中可以改进线性模型的采样。</p></li>
</ul>
<p>为了使解释更具体，我们将使用具有两个参数的极简分层模型：全局参数 <span class="math notranslate nohighlight">\(a\)</span> 和局部参数 <span class="math notranslate nohighlight">\(b\)</span>（每组参数）。仅此而已，我们在这个模型中甚至没有似然/数据！在这里省略数据是为了强调，我们将讨论的一些属性（特别是在散度一节中）与模型的结构相关，而不是与数据相关。我们将讨论同一模型的两种替代参数化：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">centered_model</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">trace_cm</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="k">with</span> <span class="n">pm</span><span class="o">.</span><span class="n">Model</span><span class="p">()</span> <span class="k">as</span> <span class="n">non_centered_model</span><span class="p">:</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">HalfNormal</span><span class="p">(</span><span class="s1">&#39;a&#39;</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
    <span class="n">b_shift</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Normal</span><span class="p">(</span><span class="s1">&#39;b_offset&#39;</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">sd</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">Deterministic</span><span class="p">(</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="mi">0</span> <span class="o">+</span> <span class="n">b_shift</span> <span class="o">*</span> <span class="n">a</span><span class="p">)</span>
    <span class="n">trace_ncm</span> <span class="o">=</span> <span class="n">pm</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">2000</span><span class="p">,</span> <span class="n">random_seed</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
</pre></div>
</div>
<p>中心模型和非中心模型的不同之处在于，对于前者，我们直接拟合群组尺度的参数，而对于后者，我们将群组尺度的参数建模为平移和缩放的高斯模型。我们将使用几个曲线图和数字摘要来探索其中的差异。</p>
<div class="section" id="id14">
<h3>8.4.1 收敛性<a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">采样</span></code>器（如 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 或 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code>) 需要一段时间才能收敛。正如之前解释过的，<code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code> 在一般条件和无限数量样本下都有收敛的理论保证。不幸的是，在实践中只能获得有限样本，因此必须转而依赖经验的检验，这些检验充其量只能提供一些提示或警告，表明当其失败时，可能会发生糟糕的事情，但不能保证当其没有失败时，一切都是正常的。</p>
<p>直观检查收敛性的一种方法是运行 <code class="docutils literal notranslate"><span class="pre">arviz.plot_trace</span></code> 函数并检查结果。为更好地理解我们在检查这些曲线图时应该查看什么，此处比较一下前面定义的两个模型（参见图 8.6 和 8.7)：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_cm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">],</span> <span class="n">divergences</span><span class="o">=</span><span class="s1">&#39;top&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021060717041601.webp" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_trace</span><span class="p">(</span><span class="n">trace_ncm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210607170440ad.webp" /></p>
<p>图 8.8 中的 <code class="docutils literal notranslate"><span class="pre">KDE</span></code> 比 8.7 中的 <code class="docutils literal notranslate"><span class="pre">KDE</span></code> 更平滑；平滑的 <code class="docutils literal notranslate"><span class="pre">KDE</span></code> 是一个好迹象，而不均匀的 KDE 可能表示存在问题，例如需要更多样本或更严重的问题。迹本身（右侧的图）应该看起来像白噪声，这意味着应该看不到任何可识别的模式；我们希望看到一条自由漫游的曲线，如图 8.8 中的迹。当这种情况发生时，我们说有很好的混合。相反，图 8.6 是一个病态行为的例子；如果您仔细地将它与图 8.8 进行比较，会注意到两条链的重叠在 8.8 比 8.7 大，您还会注意到 8.7 中沿着迹的几个区域发生了一些可疑的事情；最清楚的一个是在 500-1000 画之间的区域：您会看到其中一条链（蓝色的）卡住了（基本上是一条水平线）。</p>
<p>图 8.9 显示了一些混合良好（右侧）和混合不良（左侧）的迹的示例。如果有多个区域，例如离散变量或多模态分布，我们希望迹不会在一个值或区域上花费太多时间才移动到其他区域，而是很容易从一个区域跳到另一个区域：</p>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_202106071705182a.webp" /></p>
<p>好 <code class="docutils literal notranslate"><span class="pre">MCMC</span></code> 样本的另一个特征是自相似的迹。例如，前 10%（大约）应该与迹中的其他部分相似，例如最后 50% 或 10%。再说一次，我们不希望迹中存在某些模式；相反，想要一些嘈杂的东西。使用 <code class="docutils literal notranslate"><span class="pre">az.plot_trace</span></code> 也可以看到这一点。如果迹的第一部分看起来与其他部分不同，这表明需要老化，或者需要更多的样本。如果我们发现其他部分缺乏自相似性，或者看到了某种模式，这可能意味着需要更多的诊断，但通常情况下，我们应该尝试使用不同的参数化。对于困难的模型，我们甚至可能需要应用所有这些策略的组合。</p>
<p>默认情况下，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 将尝试并行运行独立的链（确切数量取决于可用处理器的数量）。这是使用 <code class="docutils literal notranslate"><span class="pre">pm.sample</span></code> 函数中的 <code class="docutils literal notranslate"><span class="pre">chains</span></code> 参数指定的。我们可以使用 <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 的  <code class="docutils literal notranslate"><span class="pre">plot_trace</span></code> 或 <code class="docutils literal notranslate"><span class="pre">plot_forest</span></code> 函数直观地检查并行链是否彼此相似。然后，可以将并行链合并为一个单独链进行推断，因此请注意，并行运行链并不浪费资源。</p>
<p>比较独立链的一种定量方法是使用统计量 <code class="docutils literal notranslate"><span class="pre">R</span> <span class="pre">帽</span></code> ，其思想是用链内方差计算链间方差。理想情况下，我们应该期望其值为 1。作为经验规则，值低于 1.1 也没问题；值越高越表示不收敛。我们可以使用 <code class="docutils literal notranslate"><span class="pre">az.r_hat</span></code> 函数计算它；我们只需要传递一个 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 迹对象。默认情况下，还会使用 <code class="docutils literal notranslate"><span class="pre">az.summary</span></code> 函数以及可选的 <code class="docutils literal notranslate"><span class="pre">az.plot_forest</span></code> 来计算 <code class="docutils literal notranslate"><span class="pre">r_hat</span> <span class="pre">诊断</span></code>，如我们在以下示例中所看到的：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_forest</span><span class="p">(</span><span class="n">trace_cm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">],</span> <span class="n">r_hat</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">eff_n</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210607170617f5.webp" /></p>
<p>对于 <code class="docutils literal notranslate"><span class="pre">az.summary</span></code> 也是如此：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">summaries</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_cm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">]),</span>
                      <span class="n">az</span><span class="o">.</span><span class="n">summary</span><span class="p">(</span><span class="n">trace_ncm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])])</span>
<span class="n">summaries</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;centered&#39;</span><span class="p">,</span> <span class="s1">&#39;non_centered&#39;</span><span class="p">]</span>
<span class="n">summaries</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210607170651ca.webp" /></p>
</div>
<div class="section" id="id15">
<h3>8.4.2 蒙特卡洛误差<a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>数字摘要返回的数值之一是 <code class="docutils literal notranslate"><span class="pre">mc_error</span></code>，这是对采样引入误差的估计。该估计考虑到样本并不是真正彼此独立的。<code class="docutils literal notranslate"><span class="pre">mc_error</span></code> 是 <span class="math notranslate nohighlight">\(n\)</span> 个数据块的平均值 <span class="math notranslate nohighlight">\(x\)</span> 的标准误差，每个数据块只是迹的一部分：</p>
<div class="math notranslate nohighlight">
\[
\mathrm{mc}_{\mathrm{error}}=\frac{\sigma(x)}{\sqrt{n}} \tag{8.14}
\]</div>
<p>此误差应低于我们希望在结果中看到的精度。</p>
</div>
<div class="section" id="id16">
<h3>8.4.3 自相关<a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h3>
<p>分布（包括后验分布）的理想样本应该具有等于 0 的自相关。当给定迭代的值不独立于其他迭代的采样值时，样本是自相关的。在实践中，<code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code> 产生的样本是自相关的，特别是 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code>，在较小程度上是 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 和 <code class="docutils literal notranslate"><span class="pre">SMC</span></code> 。<code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 提供了一个方便的函数来绘制自相关曲线：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_autocorr</span><span class="p">(</span><span class="n">trace_cm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_202106071708494c.webp" /></p>
<p><code class="docutils literal notranslate"><span class="pre">az.plot_autocorr</span></code> 显示采样值与连续点(最多100个点)的平均相关性。理想情况下，应该看不到自相关。在实践中，我们希望样本迅速下降到较低的自相关值。让我们绘制非中心模型的自相关图：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_autocorr</span><span class="p">(</span><span class="n">trace_ncm</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;a&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210608130900c0.webp" /></p>
<p>通过比较图 8.11 和图 8.12，可以很容易地看到，非中心模型的样本几乎没有自相关，而中心模型的样本显示出更大的自相关值。</p>
</div>
<div class="section" id="id17">
<h3>8.4.4 有效样本量<a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h3>
<p>有自相关性的采样要比没有自相关性的采样包含的信息量更少，给定采样大小和采样的自相关性之后，可以尝试估计出该采样在采样次数为多少时，没有自相关性且包含信息量不变，该值称为有效采样次数。</p>
<p>参数的自相关程度越高，获得给定精度所需的样本数量就越大，换句话说，自相关会减少有效样本的数量。我们可以在 <code class="docutils literal notranslate"><span class="pre">ArviZ</span></code> 中使用 <code class="docutils literal notranslate"><span class="pre">az.effect_n</span></code> 函数计算有效样本量。通过传递 <code class="docutils literal notranslate"><span class="pre">effn=True</span></code> 参数，还可以通过 <code class="docutils literal notranslate"><span class="pre">az.summary</span></code> 和 <code class="docutils literal notranslate"><span class="pre">az.plot_forest</span></code> 函数计算有效样本大小（参见图 8.9)。</p>
<p>理想情况下，有效样本量应该接近实际样本量。与 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 相比，<code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 的有效样本量通常比 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 高得多，因此，一般来说，如果您使用 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code>，通常需要的样本比使用 <code class="docutils literal notranslate"><span class="pre">Metropolis</span></code> 时要少。</p>
<p>如果任何参数的有效样本量低于 200，<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 都会发出警告。作为一般指南，100 个有效样本应该可以很好地估计分布的平均值，但是拥有更多样本将提供每次重新运行模型时变化较小的估计值，这也是使用 200 作为有效样本大小临界值的部分原因。对于大多数问题值，1,000 到 2,000 个有效样本将绰绰有余。</p>
</div>
<div class="section" id="id18">
<h3>8.4.5 发散性<a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<p>我们现在将探索不包含 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 的检验，因为它们基于方法的内部工作，而不是生成的样本的属性。这些检验基于发散性，是诊断样本的一种强大而灵敏的方法。</p>
<p>当我试图设置本书中的模型以避免散度时，您可能已经看到指示出现散度的 <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 消息。散度可能表明 <code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 在后验遇到了无法正确探索的高曲率区域；这告诉我们采样器可能缺少参数空间的一个区域，因此结果将是有偏差的。散度通常比这里讨论的测试敏感得多，因此，即使其余测试通过，它们也可以发出问题的信号。散度的一个很好的特点是，它们往往看起来靠近有问题的参数空间区域，因此我们可以使用它们来识别问题所在。可视化散度的一种方法是使用带有 DISGENCES=True 参数的 az.lot_air：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">_</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">sharey</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span>
<span class="n">constrained_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">tr</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">trace_cm</span><span class="p">,</span> <span class="n">trace_ncm</span><span class="p">]):</span>
    <span class="n">az</span><span class="o">.</span><span class="n">plot_pair</span><span class="p">(</span><span class="n">tr</span><span class="p">,</span> <span class="n">var_names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">],</span> <span class="n">coords</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;b_dim_0&#39;</span><span class="p">:[</span><span class="mi">0</span><span class="p">]},</span>
<span class="n">kind</span><span class="o">=</span><span class="s1">&#39;scatter&#39;</span><span class="p">,</span>
                 <span class="n">divergences</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">contour</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="n">divergences_kwargs</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;color&#39;</span><span class="p">:</span><span class="s1">&#39;C1&#39;</span><span class="p">},</span>
                 <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>
    <span class="n">ax</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">([</span><span class="s1">&#39;centered&#39;</span><span class="p">,</span> <span class="s1">&#39;non-centered&#39;</span><span class="p">][</span><span class="n">idx</span><span class="p">])</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_2021060717112515.webp" /></p>
<p>在图 8.13 中，小（蓝色）点是规则样本，较大（黑色和橙色）点表示散度。我们可以看到，中心模型的散度主要集中在漏斗的尖端。我们还可以看到，非中心模型没有发散，尖端更尖锐。采样器通过分叉告诉我们，它很难从漏斗尖端附近的区域取样。我们确实可以在图 8.13 中检查到，居中的模型在尖端附近没有样本，靠近散度集中的地方。这真是太棒了！</p>
<p>散度也用黑色、“|\”标记表示在 ArviZ 的轨迹图中，如图 8.7 所示。请注意，散度是如何集中在轨迹的病理平坦部分周围的。</p>
<p>可视化散度的另一种有用的方法是用平行的曲线图：</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">az</span><span class="o">.</span><span class="n">plot_parallel</span><span class="p">(</span><span class="n">trace_cm</span><span class="p">)</span>
</pre></div>
</div>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210607171240da.webp" /></p>
<p>在这里，我们可以看到 b 和 a 的散度都集中在 0 附近。如图 8.13 和 8.14 所示的曲线图非常重要，因为它们让我们知道参数空间的哪一部分可能有问题，还因为它们帮助我们发现误报。让我解释最后一点。<code class="docutils literal notranslate"><span class="pre">PyMC3</span></code> 使用启发式方法来标记散度，有时，这种启发式方法可能会说我们有散度，但实际上并非如此。一般来说，如果散度分散在参数空间中，我们可能会有误报；如果散度集中，那么我们可能会有问题。当产生散度时，通常有三种方法可以摆脱它们，或者至少减少它们的数量：</p>
<p>增加调优步骤的数量，类似于 pm.sample（调优=1000)。将 TARGET_ACCEPT 参数的值从默认值 0.8 增加。最大值为 1，因此您可以尝试使用诸如 0.85 或 0.9 之类的值。重新参数化模型。正如我们刚刚看到的，非中心模型是中心模型的重新参数化，这导致了更好的样本，并且没有分歧。</p>
</div>
<div class="section" id="id19">
<h3>8.4.6 非居中参数化<a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h3>
<p>从图 8.13 中，可以看到 <span class="math notranslate nohighlight">\(a\)</span> 和 <span class="math notranslate nohighlight">\(b\)</span> 参数是相关的。因为 <span class="math notranslate nohighlight">\(b\)</span> 是形状 10 的向量，所以我们选择绘制 <span class="math notranslate nohighlight">\(b(0)\)</span> ，但是 <span class="math notranslate nohighlight">\(b\)</span> 的任何其他元素都应该显示相同的模式，事实上，这在图 8.14 中表现得非常清楚。这种相关性和这种特殊的漏斗形状是模型定义和模型部分汇集数据的能力的结果。当 <span class="math notranslate nohighlight">\(a\)</span> 的值减小时，<span class="math notranslate nohighlight">\(b\)</span> 的单个值变得越来越接近全局平均值。换句话说，收缩级别会越来越高，因此数据会越来越集中（直到完全集中）。允许部分池化的相同结构还引入了影响采样器方法性能的相关性。</p>
<p>在第 3 章“线性回归模型”中，我们看到线性模型也会导致相关性（性质不同）；对于这些模型，一个简单的解决办法是将数据居中。我们可能想在这里做同样的事情，但不幸的是，这不会摆脱漏斗形状带来的采样问题。漏斗形状的微妙特征是相关性随参数空间中的位置而变化，因此将数据居中无助于降低这种相关性。<code class="docutils literal notranslate"><span class="pre">MCMC</span> <span class="pre">方法</span></code>，如 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code>，在探索高度相关的空间时遇到了问题；这些方法找到合适样本的唯一途径是在前一步的邻域中提出一个新的步骤。结果，探索变得高度自相关且缓慢得令人痛苦。缓慢的混合可能会非常剧烈，以至于简单地增加样本（绘制）的数量并不是一个合理或可行的解决方案。</p>
<p><code class="docutils literal notranslate"><span class="pre">NUTS</span></code> 等采样器更适合这项工作，因为它们根据参数空间的曲率提出步骤，但正如我们已经讨论过的，采样过程的效率高度依赖于调优阶段。对于后验的一些几何形状，例如那些由分层模型诱导的几何形状，调整阶段过度调整到链开始的局部邻域，使得对其他区域的探索效率低下，因为新的建议更具随机性，类似于 <code class="docutils literal notranslate"><span class="pre">Metropolis-Hastings</span></code> 的行为。</p>
</div>
</div>
<div class="section" id="id20">
<h2>8.5 小结<a class="headerlink" href="#id20" title="Permalink to this headline">¶</a></h2>
<p>在本章中，我们概念性地介绍了一些最常用的计算后验分布的方法，包括变分法和马尔可夫链蒙特卡洛方法。我们特别强调通用推断引擎，即设计用于任何给定模型（或至少广泛的模型）的方法。这些方法是任何概率编程语言的核心，因为它们允许自动推断，让用户专注于迭代模型设计和结果解释。我们还讨论了诊断样本的数值测试和视觉测试。如果不能很好地逼近后验分布，贝叶斯框架的所有优点和灵活性就会消失，因此评估推断过程的质量对于我们对推断过程本身的质量是至关重要的。</p>
</div>
<div class="section" id="id21">
<h2>练习<a class="headerlink" href="#id21" title="Permalink to this headline">¶</a></h2>
<p><img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_20210607171503fc.webp" />
<img alt="" src="https://gitee.com/XiShanSnow/imagebed/raw/master/images/articles/bayesian_stat_202106071715161a.webp" /></p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            
    <a class='left-prev' id="prev-link" href="chapter07-GaussianProcesses.html" title="previous page">第 7 章 高斯过程</a>
    <a class='right-next' id="next-link" href="chapter09-WheretoGoNext.html" title="next page">第 9 章 下一步去哪儿？</a>

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Osvaldo Martin<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>